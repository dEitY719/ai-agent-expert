{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepEval을 활용한 Agent 평가 가이드\n",
    "\n",
    "이 노트북에서는 DeepEval을 사용하여 AI Agent를 체계적으로 평가하는 방법을 단계별로 설명합니다.\n",
    "\n",
    "## 목차\n",
    "\n",
    "## 학습 목표\n",
    "- DeepEval의 핵심 기능 이해\n",
    "- 체계적인 평가 메트릭 설계\n",
    "- 자동화된 평가 파이프라인 구축\n",
    "- 평가 결과 분석 및 개선점 도출\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 설치\n",
    "\n",
    "먼저 필요한 라이브러리들을 설치하고 환경을 설정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "# %pip install deepeval langgraph langchain-openai tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 import\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# DeepEval import\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "from deepeval.synthesizer import Synthesizer\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "# OpenAI 및 Tavily import\n",
    "from openai import OpenAI\n",
    "from tavily import TavilyClient\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = (\n",
    "    \"your-api-key\"\n",
    ")\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"your-api-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 기본 Agent 구현\n",
    "\n",
    "간단한 Agent를 구현합니다. 이 Agent는 다음과 같은 도구들을 사용할 수 있습니다:\n",
    "- 웹 검색 (Tavily API)\n",
    "- 현재 시간 조회\n",
    "- 간단한 수학 계산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicAgent 클래스 정의 완료!\n"
     ]
    }
   ],
   "source": [
    "class BasicAgent:\n",
    "    \"\"\"기본적인 Agent 구현\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # OpenAI 클라이언트 초기화\n",
    "        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "        # Tavily 클라이언트 초기화\n",
    "        self.tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "\n",
    "        # 도구들 정의\n",
    "        self.tools = [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"search_web\",\n",
    "                    \"description\": \"웹에서 정보를 검색합니다\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\"query\": {\"type\": \"string\", \"description\": \"검색할 쿼리\"}},\n",
    "                        \"required\": [\"query\"],\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_current_time\",\n",
    "                    \"description\": \"현재 시간을 반환합니다\",\n",
    "                    \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []},\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"calculate\",\n",
    "                    \"description\": \"간단한 수학 계산을 수행합니다\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\"expression\": {\"type\": \"string\", \"description\": \"계산할 수식 (예: 2 + 3 * 4)\"}},\n",
    "                        \"required\": [\"expression\"],\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    def search_web(self, query: str) -> str:\n",
    "        \"\"\"웹 검색 도구\"\"\"\n",
    "        try:\n",
    "            results = self.tavily_client.search(\n",
    "                query=query, max_results=3, include_domains=[\"wikipedia.org\", \"news.ycombinator.com\", \"github.com\"]\n",
    "            )\n",
    "\n",
    "            search_results = []\n",
    "            for result in results.get(\"results\", []):\n",
    "                search_results.append(\n",
    "                    {\n",
    "                        \"title\": result.get(\"title\", \"\"),\n",
    "                        \"content\": result.get(\"content\", \"\"),\n",
    "                        \"url\": result.get(\"url\", \"\"),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            return json.dumps(search_results, ensure_ascii=False, indent=2)\n",
    "        except Exception as e:\n",
    "            return f\"검색 중 오류 발생: {str(e)}\"\n",
    "\n",
    "    def get_current_time(self) -> str:\n",
    "        \"\"\"현재 시간 도구\"\"\"\n",
    "        from datetime import datetime\n",
    "\n",
    "        now = datetime.now()\n",
    "        return f\"현재 시간: {now.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "    def calculate(self, expression: str) -> str:\n",
    "        \"\"\"계산 도구\"\"\"\n",
    "        try:\n",
    "            # 안전한 계산을 위해 제한된 연산만 허용\n",
    "            allowed_chars = set(\"0123456789+-*/.() \")\n",
    "            if not all(c in allowed_chars for c in expression):\n",
    "                return \"안전하지 않은 수식입니다. 숫자와 기본 연산자만 사용하세요.\"\n",
    "\n",
    "            result = eval(expression)\n",
    "            return f\"{expression} = {result}\"\n",
    "        except Exception as e:\n",
    "            return f\"계산 오류: {str(e)}\"\n",
    "\n",
    "    def run(self, user_input: str) -> Dict[str, Any]:\n",
    "        \"\"\"Agent 실행\"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"당신은 도움이 되는 AI 어시스턴트입니다. \n",
    "사용자의 질문에 정확하고 유용한 답변을 제공하세요.\n",
    "\n",
    "사용 가능한 도구들:\n",
    "1. search_web: 웹에서 정보를 검색합니다\n",
    "2. get_current_time: 현재 시간을 알려줍니다\n",
    "3. calculate: 간단한 수학 계산을 수행합니다\n",
    "\n",
    "도구를 사용할 때는 정확한 매개변수를 제공하세요.\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "        ]\n",
    "\n",
    "        tools_called = []\n",
    "        max_iterations = 5\n",
    "        iteration = 0\n",
    "\n",
    "        while iteration < max_iterations:\n",
    "            # OpenAI API 호출\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\", messages=messages, tools=self.tools, tool_choice=\"auto\", temperature=0.1\n",
    "            )\n",
    "\n",
    "            message = response.choices[0].message\n",
    "            messages.append(message)\n",
    "\n",
    "            # 도구 호출이 있는지 확인\n",
    "            if hasattr(message, \"tool_calls\") and message.tool_calls:\n",
    "                for tool_call in message.tool_calls:\n",
    "                    tool_name = tool_call.function.name\n",
    "                    tool_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "                    # 도구 호출 기록\n",
    "                    tools_called.append({\"tool_name\": tool_name, \"parameters\": tool_args, \"call_id\": tool_call.id})\n",
    "\n",
    "                    # 도구 실행\n",
    "                    if tool_name == \"search_web\":\n",
    "                        result = self.search_web(tool_args[\"query\"])\n",
    "                    elif tool_name == \"get_current_time\":\n",
    "                        result = self.get_current_time()\n",
    "                    elif tool_name == \"calculate\":\n",
    "                        result = self.calculate(tool_args[\"expression\"])\n",
    "                    else:\n",
    "                        result = f\"알 수 없는 도구: {tool_name}\"\n",
    "\n",
    "                    # 도구 결과를 메시지에 추가\n",
    "                    messages.append({\"role\": \"tool\", \"tool_call_id\": tool_call.id, \"content\": result})\n",
    "\n",
    "                iteration += 1\n",
    "            else:\n",
    "                # 도구 호출이 없으면 최종 답변\n",
    "                break\n",
    "\n",
    "        # 최종 답변 추출\n",
    "        final_answer = \"\"\n",
    "        for message in reversed(messages):\n",
    "            if hasattr(message, \"role\") and message.role == \"assistant\" and not hasattr(message, \"tool_calls\"):\n",
    "                final_answer = message.content\n",
    "                break\n",
    "            elif isinstance(message, dict) and message.get(\"role\") == \"assistant\" and not message.get(\"tool_calls\"):\n",
    "                final_answer = message[\"content\"]\n",
    "                break\n",
    "\n",
    "        return {\"input\": user_input, \"output\": final_answer, \"tools_called\": tools_called, \"iterations\": iteration}\n",
    "\n",
    "\n",
    "print(\"BasicAgent 클래스 정의 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력: 현재 시간을 알려주세요\n",
      "출력: \n",
      "도구 호출: 1개\n",
      "  - get_current_time: {}\n"
     ]
    }
   ],
   "source": [
    "# Agent 테스트\n",
    "agent = BasicAgent()\n",
    "\n",
    "# 간단한 테스트\n",
    "test_input = \"현재 시간을 알려주세요\"\n",
    "result = agent.run(test_input)\n",
    "\n",
    "print(f\"입력: {result['input']}\")\n",
    "print(f\"출력: {result['output']}\")\n",
    "print(f\"도구 호출: {len(result['tools_called'])}개\")\n",
    "for tool_call in result[\"tools_called\"]:\n",
    "    print(f\"  - {tool_call['tool_name']}: {tool_call['parameters']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DeepEval 기본 사용법\n",
    "\n",
    "DeepEval의 핵심 구성 요소들을 살펴보겠습니다:\n",
    "- **LLMTestCase**: 개별 테스트 케이스\n",
    "- **EvaluationDataset**: 테스트 케이스들의 컬렉션\n",
    "- **GEval**: 커스텀 평가 메트릭\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMTestCase 생성 완료!\n",
      "입력: 현재 시간을 알려주세요\n",
      "실제 출력: 현재 시간: 2024-01-15 14:30:25\n",
      "예상 출력: 현재 시간 정보를 제공하는 응답\n",
      "컨텍스트: ['사용자가 현재 시간을 요청함']\n",
      "설명: 시간 조회 기능 테스트\n"
     ]
    }
   ],
   "source": [
    "# LLMTestCase 생성 예시\n",
    "test_case = LLMTestCase(\n",
    "    input=\"현재 시간을 알려주세요\",\n",
    "    actual_output=\"현재 시간: 2024-01-15 14:30:25\",\n",
    "    expected_output=\"현재 시간 정보를 제공하는 응답\",\n",
    "    context=[\"사용자가 현재 시간을 요청함\"],\n",
    "    comments=\"시간 조회 기능 테스트\",\n",
    ")\n",
    "\n",
    "print(\"LLMTestCase 생성 완료!\")\n",
    "print(f\"입력: {test_case.input}\")\n",
    "print(f\"실제 출력: {test_case.actual_output}\")\n",
    "print(f\"예상 출력: {test_case.expected_output}\")\n",
    "print(f\"컨텍스트: {test_case.context}\")\n",
    "print(f\"설명: {test_case.comments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋에 3개 테스트 케이스 추가됨\n",
      "데이터셋 통계:\n",
      "  - 총 테스트 케이스: 3\n",
      "  - 총 골든 데이터: 0\n"
     ]
    }
   ],
   "source": [
    "# EvaluationDataset 생성 및 관리\n",
    "dataset = EvaluationDataset()\n",
    "\n",
    "# 여러 테스트 케이스 추가\n",
    "test_cases = [\n",
    "    LLMTestCase(\n",
    "        input=\"현재 시간을 알려주세요\",\n",
    "        actual_output=\"현재 시간: 2024-01-15 14:30:25\",\n",
    "        expected_output=\"현재 시간 정보를 제공하는 응답\",\n",
    "        context=[\"사용자가 현재 시간을 요청함\"],\n",
    "        comments=\"시간 조회 기능 테스트\",\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"2 + 3 * 4를 계산해주세요\",\n",
    "        actual_output=\"2 + 3 * 4 = 14\",\n",
    "        expected_output=\"14\",\n",
    "        context=[\"사용자가 수학 계산을 요청함\"],\n",
    "        comments=\"계산 기능 테스트\",\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"Python 프로그래밍의 장점을 알려주세요\",\n",
    "        actual_output=\"Python은 간결하고 읽기 쉬운 문법을 가진 프로그래밍 언어입니다...\",\n",
    "        expected_output=\"Python의 주요 장점들을 설명하는 응답\",\n",
    "        context=[\"사용자가 Python에 대한 정보를 요청함\"],\n",
    "        comments=\"일반적인 질문 답변 테스트\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# 데이터셋에 테스트 케이스들 추가\n",
    "for test_case in test_cases:\n",
    "    dataset.add_test_case(test_case)\n",
    "\n",
    "print(f\"데이터셋에 {len(dataset.test_cases)}개 테스트 케이스 추가됨\")\n",
    "print(f\"데이터셋 통계:\")\n",
    "print(f\"  - 총 테스트 케이스: {len(dataset.test_cases)}\")\n",
    "print(f\"  - 총 골든 데이터: {len(dataset.goldens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 합성 데이터 생성\n",
    "\n",
    "DeepEval의 Synthesizer를 사용하여 문서에서 자동으로 평가 데이터를 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 문서 생성 완료!\n",
      "생성된 문서: ['ai_guide.txt', 'evaluation_guide.txt']\n"
     ]
    }
   ],
   "source": [
    "# 샘플 문서 생성\n",
    "sample_documents = {\n",
    "    \"ai_guide.txt\": \"\"\"\n",
    "AI Agent는 인공지능 기반의 자율적인 소프트웨어 프로그램입니다.\n",
    "\n",
    "Agent의 주요 특징:\n",
    "1. 자율성: 인간의 개입 없이 독립적으로 작업 수행\n",
    "2. 반응성: 환경 변화에 적절히 반응\n",
    "3. 능동성: 목표 달성을 위해 능동적으로 행동\n",
    "4. 사회성: 다른 Agent나 시스템과 상호작용\n",
    "\n",
    "Agent의 핵심 구성 요소:\n",
    "- 인식 모듈: 환경 정보 수집 및 해석\n",
    "- 추론 모듈: 정보 분석 및 의사결정\n",
    "- 행동 모듈: 결정된 행동 실행\n",
    "- 학습 모듈: 경험을 통한 성능 개선\n",
    "\n",
    "현대 AI Agent는 자연어 처리, 컴퓨터 비전, 강화학습 등의 기술을 활용합니다.\n",
    "ChatGPT, Claude, Gemini 등의 대화형 AI도 Agent의 한 형태입니다.\n",
    "\"\"\",\n",
    "    \"evaluation_guide.txt\": \"\"\"\n",
    "AI Agent 평가는 Agent의 성능과 품질을 측정하는 중요한 과정입니다.\n",
    "\n",
    "평가의 주요 목적:\n",
    "1. 성능 측정: Agent가 목표를 얼마나 잘 달성하는지 측정\n",
    "2. 품질 보증: 출력의 정확성과 일관성 확인\n",
    "3. 개선점 식별: 약점과 개선이 필요한 영역 파악\n",
    "4. 비교 분석: 다른 Agent나 버전과의 성능 비교\n",
    "\n",
    "평가 방법론:\n",
    "- 정량적 평가: 수치적 지표를 통한 객관적 측정\n",
    "- 정성적 평가: 전문가 판단을 통한 주관적 평가\n",
    "- 사용자 평가: 실제 사용자의 만족도와 피드백\n",
    "- 자동 평가: AI 모델을 활용한 자동화된 평가\n",
    "\n",
    "주요 평가 메트릭:\n",
    "- 정확도: 올바른 답변의 비율\n",
    "- 완성도: 요청사항 충족 정도\n",
    "- 일관성: 동일한 입력에 대한 일관된 출력\n",
    "- 효율성: 리소스 사용량과 처리 시간\n",
    "\"\"\",\n",
    "}\n",
    "\n",
    "# 문서들을 파일로 저장\n",
    "for filename, content in sample_documents.items():\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"샘플 문서 생성 완료!\")\n",
    "print(f\"생성된 문서: {list(sample_documents.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Synthesizer' object has no attribute 'generate_goldens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 문서에서 골든 데이터 생성\u001b[39;00m\n\u001b[1;32m      5\u001b[0m document_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(sample_documents\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m----> 6\u001b[0m goldens \u001b[38;5;241m=\u001b[39m \u001b[43msynthesizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_goldens\u001b[49m(document_files)   \u001b[38;5;66;03m# 다른 메서드일 가능성 큼\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m생성된 골든 데이터: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(goldens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m개\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 생성된 골든 데이터 출력\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Synthesizer' object has no attribute 'generate_goldens'"
     ]
    }
   ],
   "source": [
    "# Synthesizer를 사용한 합성 데이터 생성\n",
    "synthesizer = Synthesizer()\n",
    "\n",
    "# 문서에서 골든 데이터 생성\n",
    "document_files = list(sample_documents.keys())\n",
    "goldens = synthesizer.generate_goldens(document_files)  # 다른 메서드일 가능성 큼\n",
    "\n",
    "print(f\"생성된 골든 데이터: {len(goldens)}개\")\n",
    "\n",
    "# 생성된 골든 데이터 출력\n",
    "for i, golden in enumerate(goldens, 1):\n",
    "    print(f\"\\n골든 데이터 {i}:\")\n",
    "    print(f\"  입력: {golden.input}\")\n",
    "    print(f\"  예상 출력: {golden.expected_output[:100]}...\")\n",
    "    print(f\"  컨텍스트: {len(golden.context)}개 청크\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 평가 메트릭 구현\n",
    "\n",
    "Agent 평가를 위한 커스텀 메트릭들을 구현합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 평가 메트릭 생성\n",
    "task_completion_metric = GEval(\n",
    "    name=\"Task Completion\",\n",
    "    criteria=\"\"\"\n",
    "Agent가 사용자의 요청을 얼마나 완전하게 수행했는지 평가하세요.\n",
    "\n",
    "평가 기준:\n",
    "1. 사용자 요청의 핵심 의도를 파악했는가?\n",
    "2. 요청된 작업을 적절히 수행했는가?\n",
    "3. 결과가 사용자의 기대에 부합하는가?\n",
    "\n",
    "점수는 1-10 척도로 평가하며, 10점은 완벽한 작업 완수를 의미합니다.\n",
    "\"\"\",\n",
    "    evaluation_params=[\n",
    "        # YOUR_CODE\n",
    "    ],\n",
    "    evaluation_steps=[\n",
    "        \"사용자 요청의 핵심 요구사항을 식별하세요.\",\n",
    "        \"Agent의 출력에서 각 요구사항의 충족 여부를 확인하세요.\",\n",
    "        \"전체적인 작업 완성도를 종합적으로 평가하세요.\",\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "response_quality_metric = GEval(\n",
    "    name=\"Response Quality\",\n",
    "    criteria=\"\"\"\n",
    "Agent 응답의 품질을 평가하세요.\n",
    "\n",
    "평가 기준:\n",
    "1. 응답이 정확하고 유용한가?\n",
    "2. 응답이 명확하고 이해하기 쉬운가?\n",
    "3. 응답이 완전하고 포괄적인가?\n",
    "\n",
    "점수는 1-10 척도로 평가하며, 10점은 완벽한 응답을 의미합니다.\n",
    "\"\"\",\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    evaluation_steps=[\"응답의 정확성을 평가하세요.\", \"응답의 명확성을 평가하세요.\", \"응답의 완성도를 평가하세요.\"],\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "print(\"커스텀 평가 메트릭 생성 완료!\")\n",
    "print(f\"생성된 메트릭:\")\n",
    "print(f\"  - {task_completion_metric.name}\")\n",
    "print(f\"  - {response_quality_metric.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 전체 평가 파이프라인\n",
    "\n",
    "이제 모든 구성 요소를 연결하여 완전한 평가 파이프라인을 구축합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1단계: 골든 데이터를 테스트 케이스로 변환\n",
    "print(\"=== 1단계: 테스트 케이스 변환 ===\")\n",
    "test_cases = []\n",
    "for golden in goldens:\n",
    "    test_case = LLMTestCase(\n",
    "        # YOUR_CODE\n",
    "    )\n",
    "    test_cases.append(test_case)\n",
    "\n",
    "print(f\"변환된 테스트 케이스: {len(test_cases)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2단계: Agent 실행\n",
    "print(\"\\n=== 2단계: Agent 실행 ===\")\n",
    "for i, test_case in enumerate(test_cases):\n",
    "    try:\n",
    "        result = agent.run(test_case.input)\n",
    "        test_case.actual_output = result[\"output\"]\n",
    "\n",
    "        # 도구 호출 정보를 추가 메타데이터에 저장\n",
    "        test_case.additional_metadata = {\"tools_called\": result[\"tools_called\"], \"iterations\": result[\"iterations\"]}\n",
    "\n",
    "        print(f\"테스트 케이스 {i+1} 완료\")\n",
    "\n",
    "    except Exception as e:\n",
    "        test_case.actual_output = f\"오류 발생: {str(e)}\"\n",
    "        test_case.additional_metadata = {\"error\": str(e)}\n",
    "        print(f\"테스트 케이스 {i+1} 실패: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3단계: 메트릭 평가\n",
    "print(\"\\n=== 3단계: 메트릭 평가 ===\")\n",
    "metrics = [task_completion_metric, response_quality_metric]\n",
    "evaluation_results = {\n",
    "    \"test_cases\": [],\n",
    "    \"overall_scores\": {}\n",
    "}\n",
    "\n",
    "for i, test_case in enumerate(test_cases):\n",
    "    print(f\"테스트 케이스 {i+1} 평가 중...\")\n",
    "    \n",
    "    case_results = {\n",
    "        \"input\": test_case.input,\n",
    "        \"actual_output\": test_case.actual_output,\n",
    "        \"expected_output\": test_case.expected_output,\n",
    "        \"scores\": {},\n",
    "        \"details\": {}\n",
    "    }\n",
    "    \n",
    "    # 각 메트릭으로 평가\n",
    "    for metric in metrics:\n",
    "        try:\n",
    "            metric_name = metric.name\n",
    "            metric.# YOUR_CODE\n",
    "            score = # YOUR_CODE\n",
    "            reason = getattr(metric, 'reason', 'N/A')\n",
    "            \n",
    "            case_results[\"scores\"][metric_name] = score\n",
    "            case_results[\"details\"][metric_name] = {\n",
    "                \"score\": score,\n",
    "                \"reason\": reason\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            metric_name = metric.name\n",
    "            case_results[\"scores\"][metric_name] = 0.0\n",
    "            case_results[\"details\"][metric_name] = {\n",
    "                \"score\": 0.0,\n",
    "                \"reason\": f\"평가 실패: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    evaluation_results[\"test_cases\"].append(case_results)\n",
    "\n",
    "# 전체 점수 계산\n",
    "for metric in metrics:\n",
    "    metric_name = metric.name\n",
    "    scores = [case[\"scores\"].get(metric_name, 0.0) for case in evaluation_results[\"test_cases\"]]\n",
    "    if scores:\n",
    "        evaluation_results[\"overall_scores\"][metric_name] = {\n",
    "            \"average\": sum(scores) / len(scores),\n",
    "            \"min\": min(scores),\n",
    "            \"max\": max(scores),\n",
    "            \"std\": (sum((x - sum(scores)/len(scores))**2 for x in scores) / len(scores))**0.5\n",
    "        }\n",
    "\n",
    "print(\"메트릭 평가 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 결과 분석 및 시각화\n",
    "\n",
    "평가 결과를 분석하고 시각화합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 평가 결과 출력\n",
    "print(\"=== 전체 평가 결과 ===\")\n",
    "for metric_name, scores in evaluation_results[\"overall_scores\"].items():\n",
    "    print(f\"\\n{metric_name}:\")\n",
    "    print(f\"  평균: {scores['average']:.2f}\")\n",
    "    print(f\"  최소: {scores['min']:.2f}\")\n",
    "    print(f\"  최대: {scores['max']:.2f}\")\n",
    "    print(f\"  표준편차: {scores['std']:.2f}\")\n",
    "\n",
    "# 성능 등급 분류\n",
    "print(f\"\\n=== 성능 등급 분류 ===\")\n",
    "for metric_name, scores in evaluation_results[\"overall_scores\"].items():\n",
    "    avg_score = scores[\"average\"]\n",
    "    if avg_score >= 8.0:\n",
    "        grade = \"우수\"\n",
    "    elif avg_score >= 6.0:\n",
    "        grade = \"양호\"\n",
    "    elif avg_score >= 4.0:\n",
    "        grade = \"보통\"\n",
    "    else:\n",
    "        grade = \"개선 필요\"\n",
    "\n",
    "    print(f\"  {metric_name}: {grade} ({avg_score:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개별 테스트 케이스 분석\n",
    "print(\"\\n=== 개별 테스트 케이스 분석 ===\")\n",
    "for i, case in enumerate(evaluation_results[\"test_cases\"]):\n",
    "    print(f\"\\n테스트 케이스 {i+1}:\")\n",
    "    print(f\"  입력: {case['input'][:100]}...\")\n",
    "    print(f\"  점수:\")\n",
    "    for metric_name, score in case[\"scores\"].items():\n",
    "        print(f\"    {metric_name}: {score:.2f}\")\n",
    "\n",
    "    # 도구 호출 정보 출력\n",
    "    if \"tools_called\" in case.get(\"additional_metadata\", {}):\n",
    "        tools = case[\"additional_metadata\"][\"tools_called\"]\n",
    "        print(f\"  도구 호출: {len(tools)}개\")\n",
    "        for tool in tools:\n",
    "            print(f\"    - {tool['tool_name']}: {tool['parameters']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 JSON 파일로 저장\n",
    "with open(\"evaluation_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(evaluation_results, f, ensure_ascii=False, indent=2, default=str)\n",
    "\n",
    "print(\"\\n=== 평가 완료 ===\")\n",
    "print(\"결과가 evaluation_results.json에 저장되었습니다.\")\n",
    "\n",
    "# 임시 파일 정리\n",
    "import glob\n",
    "\n",
    "for file in glob.glob(\"*.txt\"):\n",
    "    os.remove(file)\n",
    "print(\"임시 파일들이 정리되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
