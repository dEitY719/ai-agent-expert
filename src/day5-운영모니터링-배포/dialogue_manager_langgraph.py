import os

from langchain_google_genai import ChatGoogleGenerativeAI
from langfuse import observe
from langfuse.langchain import CallbackHandler


async def handle_dialogue_logic(user_input: str):
    """LangGraph Agentì˜ ì—­í• ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” ëŒ€í™” ê´€ë¦¬ ë¡œì§ (Langfuse ì¶”ì  ê¸°ëŠ¥ ì¶”ê°€)"""
    print("[LangGraph Service] ğŸ§  ëŒ€í™” ê´€ë¦¬ì ì‹¤í–‰ë¨...")

    # ê° ì„œë¹„ìŠ¤ í˜¸ì¶œì„ ìœ„í•œ ê³ ìœ í•œ Langfuse í•¸ë“¤ëŸ¬ ìƒì„±
    handler = CallbackHandler()

    try:
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite", temperature=0)
        prompt = f"ë‹¤ìŒ ì‚¬ìš©ì ìš”ì²­ì˜ í•µì‹¬ ì£¼ì œë¥¼ 20ë‹¨ì–´ ì´ë‚´ì˜ ê°„ê²°í•œ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ê³ , ì‚¬ìš©ìì˜ ìˆ¨ê²¨ì§„ ìš”êµ¬ì‚¬í•­(ìŠ¤íƒ€ì¼, í†¤ì•¤ë§¤ë„ˆ ë“±)ì„ ì¶”ë¡ í•´ì¤˜. ê²°ê³¼ëŠ” 'ì£¼ì œ: [ìš”ì•½ëœ ì£¼ì œ]\nìš”êµ¬ì‚¬í•­: [ì¶”ë¡ ëœ ìš”êµ¬ì‚¬í•­]' í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì¤˜. ë‹¤ë¥¸ ë§ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆ.\n\nì‚¬ìš©ì ìš”ì²­: '{user_input}'"

        # LLM í˜¸ì¶œ ì‹œ configì— ì½œë°± í•¸ë“¤ëŸ¬ ì „ë‹¬
        response_text = llm.invoke(prompt, config={"callbacks": [handler]}).content

        topic = response_text.split("ì£¼ì œ:")[1].split("ìš”êµ¬ì‚¬í•­:")[0].strip()
        preferences = response_text.split("ìš”êµ¬ì‚¬í•­:")[1].strip()
    except Exception as e:
        print(f"[LangGraph Service] ğŸ”´ LLM í˜¸ì¶œ ì˜¤ë¥˜, Fallback ë¡œì§ ì‚¬ìš©: {e}")
        topic = user_input
        preferences = "ì „ë¬¸ì ì´ë©´ì„œë„ ì‰¬ìš´ ì–´ì¡°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."

    response = {
        "agent_response": f"ì•Œê² ìŠµë‹ˆë‹¤. '{topic}'ì— ëŒ€í•œ ë¸”ë¡œê·¸ ê¸€ ìƒì„±ì„ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤. '{preferences}' ìš”êµ¬ì‚¬í•­ì„ ë°˜ì˜í•˜ê² ìŠµë‹ˆë‹¤.",
        "next_action": {"action": "CREATE_CONTENT", "topic": topic, "user_preferences": preferences},
    }
    print(f"[LangGraph Service] âœ… ë‹¤ìŒ í–‰ë™ ê²°ì •: {response['next_action']}")
    return response


print("âœ… dialogue_manager_langgraph.py íŒŒì¼ì´ Langfuse ì¶”ì  ê¸°ëŠ¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
