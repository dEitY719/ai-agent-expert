{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2e_intro_md"
   },
   "source": [
    "# LangFuse + MAS\n",
    "\n",
    "### 실습 목표\n",
    "4일차에 완성한  'A2A 통합 시스템' 전체에 `Langfuse`를 이용한 엔드투엔드(End-to-End) 관측 가능성을 부여합니다. 단 한 번의 사용자 요청이 어떻게 여러 Agent 시스템들을 거쳐 최종 결과물로 완성되는지 시각적으로 추적하고 분석하는 모니터링 시스템을 구축합니다.\n",
    "\n",
    "\n",
    "1.  독립적 실행 환경 구축: `%%writefile`을 사용하여 4일차에 만들었던 구성 요소—MCP 서버, A2A 게이트웨이, 각 전문가 서비스 모듈—를 생성합니다.\n",
    "2.  시스템 구성 요소 검토: 각 모듈(MCP Tool, A2A 프로토콜, 전문가 서비스 로직)의 역할과 관계를 다시 한번 복습하며, 이어질 Part 2에서 `Langfuse`를 어느 계층에 어떻게 주입할지 계획하는 기반을 다집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2e_setup_md"
   },
   "source": [
    "### 0. 사전 준비: 라이브러리 설치 및 전체 API 키 설정\n",
    "5일차 실습은 새로운 런타임 환경에서 시작됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "e2e_install_code"
   },
   "outputs": [],
   "source": [
    "# !pip install fastapi uvicorn python-dotenv nest-asyncio pyngrok aiohttp httpx newsapi-python arxiv tavily-python slowapi langfuse crewai crewai-tools google-adk langchain-google-genai pydantic instructor requests jsonref starlette -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "e2e_api_keys_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 API 키가 성공적으로 설정되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# OpenAI\n",
    "\n",
    "# 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\n",
    "\n",
    "# Gemini\n",
    "\n",
    "# AIzaSyDVYEpxB86k5-Oi2BApqTr47nnGJ0BwkOc\n",
    "\n",
    "# Finnhub\n",
    "\n",
    "# d2ng5rhr01qvm111q850d2ng5rhr01qvm111q85g\n",
    "\n",
    "# Openweather\n",
    "\n",
    "# da81e6f3657c6f5a2a6b687890c2980f\n",
    "\n",
    "# Tavily\n",
    "\n",
    "# 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\n",
    "\n",
    "# Serper\n",
    "\n",
    "# 01b92c3671ae1f19f98d5b9468a3e5674bde070d\n",
    "\n",
    "# Wolfram APP ID\n",
    "\n",
    "# 73Q6KAVRGG\n",
    "\n",
    "# News API\n",
    "\n",
    "# 043054dcda874c1da6fe5a0ec0471f33\n",
    "\n",
    "\n",
    "# LLM 및 검색 Tool 키\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDVYEpxB86k5-Oi2BApqTr47nnGJ0BwkOc\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "os.environ[\"NEWS_API_KEY\"] = \"043054dcda874c1da6fe5a0ec0471f33\"\n",
    "\n",
    "# Langfuse 키\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-c4867845-645e-4c69-a40d-14b5faf31e45\"\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-11135925-919c-4df5-baa1-a510de20e4c9\"\n",
    "# Host의 경우 클라우드의 국가를 확인하여 설정합니다.\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\"\n",
    "\n",
    "# 서버 인증용 마스터 키\n",
    "os.environ[\"MASTER_API_KEY\"] = \"samsung-llm-agent-lv4-master-key\"\n",
    "\n",
    "print(\"모든 API 키가 성공적으로 설정되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2e_build_mcp_md"
   },
   "source": [
    "### 1. 시스템 재구축 (Part 1): MCP 서버(자원 허브) 코드 생성\n",
    "4일차 Lab 1에서 완성했던 프로덕션급 MCP 서버의 모든 구성 요소를 `%%writefile`을 사용하여 다시 생성합니다. 이는 Tool, 정적 리소스, 보안, 안정성 기능을 모두 포함합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "e2e_write_server_tools_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting server_tools.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile server_tools.py\n",
    "\n",
    "import os\n",
    "from tavily import TavilyClient\n",
    "from newsapi import NewsApiClient\n",
    "import arxiv\n",
    "import json\n",
    "\n",
    "async def web_search(query: str) -> str:\n",
    "    try:\n",
    "        client = TavilyClient(api_key=os.environ['TAVILY_API_KEY'])\n",
    "        response = client.search(query=query, max_results=3, search_depth=\"advanced\")\n",
    "        return json.dumps([{\"title\": obj['title'], \"url\": obj['url'], \"content\": obj['content']} for obj in response['results']], ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"Tavily API 오류: {e}\"})\n",
    "\n",
    "async def news_api_search(query: str) -> str:\n",
    "    try:\n",
    "        client = NewsApiClient(api_key=os.environ['NEWS_API_KEY'])\n",
    "        response = client.get_everything(q=query, language='ko', sort_by='relevancy', page_size=3)\n",
    "        if response['status'] == 'ok':\n",
    "            return json.dumps([{\"title\": article['title'], \"url\": article['url'], \"description\": article['description']} for article in response['articles']], ensure_ascii=False)\n",
    "        else:\n",
    "            return json.dumps({\"error\": f\"News API 오류: {response.get('message', 'Unknown error')}\"})\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"News API 클라이언트 오류: {e}\"})\n",
    "\n",
    "async def arxiv_search(query: str) -> str:\n",
    "    try:\n",
    "        client = arxiv.Client()\n",
    "        search = arxiv.Search(query=query, max_results=2, sort_by=arxiv.SortCriterion.Relevance)\n",
    "        results = list(client.results(search))\n",
    "        return json.dumps([{\"title\": result.title, \"authors\": [str(a) for a in result.authors], \"summary\": result.summary, \"pdf_url\": result.pdf_url} for result in results], ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"Arxiv 검색 오류: {e}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "e2e_write_server_resources_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting server_resources.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile server_resources.py\n",
    "\n",
    "BLOG_TEMPLATES = {\n",
    "    \"tech_analysis\": \"## 제목\\n\\n### 1. 기술 개요\\n\\n### 2. 핵심 작동 원리\\n\\n### 3. 장단점 분석\\n\\n### 4. 실무 적용 사례\\n\\n### 5. 결론 및 향후 전망\",\n",
    "    \"product_review\": \"## 제목\\n\\n### 1. 첫인상 및 디자인\\n\\n### 2. 주요 기능 및 성능 테스트\\n\\n### 3. 실사용 후기 (장점/단점)\\n\\n### 4. 총평 및 추천 대상\"\n",
    "}\n",
    "\n",
    "STYLE_GUIDES = {\n",
    "    \"default\": \"문체: 전문적이면서도 명확하게.\\n대상 독자: 기술에 관심 있는 일반인.\\n어조: 객관적이고 사실 기반.\",\n",
    "    \"samsung_newsroom\": \"문체: 삼성전자 뉴스룸 공식 톤앤매너.\\n대상 독자: 언론인 및 IT 업계 종사자.\\n어조: 신뢰감을 주는 공식적인 어조.\"\n",
    "}\n",
    "\n",
    "async def get_template(template_id: str):\n",
    "    return BLOG_TEMPLATES.get(template_id, {\"error\": \"Template not found\"})\n",
    "\n",
    "async def get_style_guide(guide_id: str):\n",
    "    return STYLE_GUIDES.get(guide_id, {\"error\": \"Style guide not found\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "e2e_write_mcp_server_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_server.py\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, Depends, Security, Request\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "from typing import Dict\n",
    "from fastapi.security import APIKeyHeader\n",
    "from slowapi import Limiter, _rate_limit_exceeded_handler\n",
    "from slowapi.util import get_remote_address\n",
    "from slowapi.errors import RateLimitExceeded\n",
    "from server_tools import web_search, news_api_search, arxiv_search\n",
    "from server_resources import get_template, get_style_guide\n",
    "\n",
    "app = FastAPI(title=\"LLM Agent Resource Hub (MCP Server)\", version=\"1.0.0\")\n",
    "limiter = Limiter(key_func=get_remote_address)\n",
    "app.state.limiter = limiter\n",
    "app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n",
    "\n",
    "API_KEY_NAME = \"X-API-Key\"\n",
    "api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)\n",
    "\n",
    "async def get_api_key(api_key: str = Security(api_key_header)):\n",
    "    if api_key == os.environ.get(\"MASTER_API_KEY\"):\n",
    "        return api_key\n",
    "    else:\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid or missing API Key\")\n",
    "\n",
    "class ToolCallRequest(BaseModel):\n",
    "    query: str = Field(..., description=\"Tool에 전달할 검색어 또는 입력값\")\n",
    "\n",
    "@app.get(\"/\", summary=\"서버 상태 확인\", tags=[\"Status\"])\n",
    "async def read_root():\n",
    "    return {\"status\": \"ok\", \"message\": \"MCP Server is running successfully.\"}\n",
    "\n",
    "\n",
    "@app.post(\"/api/v1/tools/web_search\", summary=\"웹 검색 (보안/속도제한)\", tags=[\"Production Tools\"])\n",
    "@limiter.limit(\"20/minute\")\n",
    "async def prod_web_search(request: Request, data: ToolCallRequest, api_key: str = Depends(get_api_key)):\n",
    "    result = await web_search(data.query)\n",
    "    return {\"tool\": \"web_search\", \"query\": data.query, \"result\": result}\n",
    "\n",
    "@app.post(\"/api/v1/tools/news_api\", summary=\"뉴스 검색 (보안/속도제한)\", tags=[\"Production Tools\"])\n",
    "@limiter.limit(\"20/minute\")\n",
    "async def prod_news_search(request: Request, data: ToolCallRequest, api_key: str = Depends(get_api_key)):\n",
    "    result = await news_api_search(data.query)\n",
    "    return {\"tool\": \"news_api\", \"query\": data.query, \"result\": result}\n",
    "\n",
    "@app.post(\"/api/v1/tools/arxiv_search\", summary=\"논문 검색 (보안/속도제한)\", tags=[\"Production Tools\"])\n",
    "@limiter.limit(\"20/minute\")\n",
    "async def prod_arxiv_search(request: Request, data: ToolCallRequest, api_key: str = Depends(get_api_key)):\n",
    "    result = await arxiv_search(data.query)\n",
    "    return {\"tool\": \"arxiv_search\", \"query\": data.query, \"result\": result}\n",
    "\n",
    "@app.get(\"/api/v1/resources/templates/{template_id}\", summary=\"템플릿 조회 (보안/속도제한)\", tags=[\"Production Resources\"])\n",
    "@limiter.limit(\"60/minute\")\n",
    "async def prod_read_template(request: Request, template_id: str, api_key: str = Depends(get_api_key)):\n",
    "    content = await get_template(template_id)\n",
    "    if isinstance(content, dict) and \"error\" in content:\n",
    "        raise HTTPException(status_code=404, detail=content[\"error\"])\n",
    "    return {\"resource\": \"template\", \"id\": template_id, \"content\": content}\n",
    "\n",
    "print(\"mcp_server.py 파일이 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2e_build_a2a_md"
   },
   "source": [
    "### 2. 시스템 재구축 (Part 2): A2A 게이트웨이 및 서비스 모듈 코드 생성\n",
    "다음으로, 4일차 Lab 4에서 완성했던 A2A 게이트웨이의 모든 구성 요소를 `%%writefile`을 사용하여 다시 생성합니다.\n",
    "  \n",
    "실습환경 상 A2A 공식 프로토콜(Agent card + JSON-RPC 2.0) 대신 FastAPI 기반 RESTful API 엔드포인트를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "e2e_write_a2a_protocol_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting a2a_protocol.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile a2a_protocol.py\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "class DialogueRequest(BaseModel):\n",
    "    session_id: str\n",
    "    user_input: str\n",
    "\n",
    "class DialogueResponse(BaseModel):\n",
    "    session_id: str\n",
    "    agent_response: str\n",
    "    next_action: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class ContentCreationRequest(BaseModel):\n",
    "    topic: str\n",
    "    user_preferences: str\n",
    "\n",
    "class ContentCreationResponse(BaseModel):\n",
    "    draft_content: str\n",
    "    status: str\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "class QualityControlRequest(BaseModel):\n",
    "    topic: str\n",
    "    draft_content: str\n",
    "\n",
    "class QualityControlResponse(BaseModel):\n",
    "    final_post: str\n",
    "    qa_report: Dict[str, Any]\n",
    "    status: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "e2e_write_mcp_client_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_client.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_client.py\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "class MCPClient:\n",
    "    def __init__(self, base_url: str, api_key: str):\n",
    "        if not base_url:\n",
    "            raise ValueError(\"MCP 서버의 base_url이 필요합니다.\")\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.headers = {\"X-API-Key\": api_key, \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    def call_tool(self, tool_name: str, query: str) -> dict:\n",
    "        endpoint = f\"{self.base_url}/api/v1/tools/{tool_name}\"\n",
    "        payload = {\"query\": query}\n",
    "        try:\n",
    "            response = requests.post(endpoint, headers=self.headers, json=payload, timeout=120)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            return {\"error\": f\"HTTP 오류: {e.response.status_code}\", \"detail\": e.response.text}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Tool 호출 중 오류 발생: {e}\"}\n",
    "\n",
    "    def get_resource(self, resource_type: str, resource_id: str) -> dict:\n",
    "        endpoint = f\"{self.base_url}/api/v1/resources/{resource_type}/{resource_id}\"\n",
    "        try:\n",
    "            response = requests.get(endpoint, headers=self.headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            return {\"error\": f\"HTTP 오류: {e.response.status_code}\", \"detail\": e.response.text}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"리소스 조회 중 오류 발생: {e}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e2e_write_dialogue_manager_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dialogue_manager_langgraph.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dialogue_manager_langgraph.py\n",
    "\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "async def handle_dialogue_logic(user_input: str):\n",
    "    print(\"[LangGraph Service] 🧠 대화 관리자 실행됨...\")\n",
    "    try:\n",
    "        llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0)\n",
    "        prompt = f\"다음 사용자 요청의 핵심 주제를 20단어 이내의 간결한 한 문장으로 요약하고, 사용자의 숨겨진 요구사항(스타일, 톤앤매너 등)을 추론해줘. 결과는 '주제: [요약된 주제]\\n요구사항: [추론된 요구사항]' 형식으로만 답변해줘. 다른 말은 절대 추가하지 마.\\n\\n사용자 요청: '{user_input}'\"\n",
    "        response_text = llm.invoke(prompt).content\n",
    "        topic = response_text.split(\"주제:\")[1].split(\"요구사항:\")[0].strip()\n",
    "        preferences = response_text.split(\"요구사항:\")[1].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[LangGraph Service] 🔴 LLM 호출 오류, Fallback 로직 사용: {e}\")\n",
    "        topic = user_input\n",
    "        preferences = \"전문적이면서도 쉬운 어조로 작성해주세요.\"\n",
    "\n",
    "    response = {\n",
    "        \"agent_response\": f\"알겠습니다. '{topic}'에 대한 블로그 글 생성을 시작하겠습니다. '{preferences}' 요구사항을 반영하겠습니다.\",\n",
    "        \"next_action\": {\"action\": \"CREATE_CONTENT\", \"topic\": topic, \"user_preferences\": preferences}\n",
    "    }\n",
    "    print(f\"[LangGraph Service] ✅ 다음 행동 결정: {response['next_action']}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "e2e_write_crewai_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting content_creation_crew.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile content_creation_crew.py\n",
    "\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.tools import BaseTool\n",
    "import json\n",
    "from mcp_client import MCPClient\n",
    "\n",
    "class MCPTool(BaseTool):\n",
    "    name: str\n",
    "    description: str\n",
    "    client: MCPClient\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        print(f\"  [MCP Bridge] CrewAI -> MCP Server: Calling tool '{self.name}' with query '{query}'\")\n",
    "        response = self.client.call_tool(self.name, query)\n",
    "        return json.dumps(response, ensure_ascii=False)\n",
    "\n",
    "def handle_creation_logic(topic: str, user_preferences: str, mcp_client: MCPClient):\n",
    "    print(f\"[CrewAI Service] 👥 콘텐츠 제작팀 가동됨 (주제: {topic[:30]}...)\")\n",
    "    try:\n",
    "        crew_web_search = MCPTool(name=\"web_search\", description=\"웹에서 최신 정보를 검색합니다.\", client=mcp_client)\n",
    "        crew_arxiv_search = MCPTool(name=\"arxiv_search\", description=\"학술 논문을 검색합니다.\", client=mcp_client)\n",
    "\n",
    "        llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "        researcher = Agent(role=\"선임 리서처\", goal=f\"{topic}에 대한 심층 분석\", backstory=\"당신은 20년 경력의 기술 분석 전문가입니다.\", tools=[crew_web_search, crew_arxiv_search], llm=llm, verbose=True)\n",
    "        writer = Agent(role=\"전문 작가\", goal=\"리서치 결과를 바탕으로 매력적인 블로그 글 작성\", backstory=\"당신은 기술 분야의 베스트셀러 작가입니다.\", llm=llm, verbose=True)\n",
    "\n",
    "        research_task = Task(description=f\"'{topic}'에 대해 웹과 학술 자료를 종합하여 심층 분석 보고서를 작성하세요.\", expected_output=\"구조화된 분석 보고서\", agent=researcher)\n",
    "        write_task = Task(description=f\"리서치 보고서를 바탕으로, '{user_preferences}' 스타일을 반영하여 블로그 초안을 작성하세요.\", expected_output=\"완성된 블로그 초안\", agent=writer, context=[research_task])\n",
    "\n",
    "        crew = Crew(agents=[researcher, writer], tasks=[research_task, write_task], process=Process.sequential)\n",
    "        result = crew.kickoff()\n",
    "        print(\"[CrewAI Service] ✅ 초안 작성 완료.\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        error_message = f\"CrewAI 실행 중 오류 발생: {e}\"\n",
    "        print(f\"[CrewAI Service] 🔴 {error_message}\")\n",
    "        return error_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "e2e_write_adk_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting quality_control_adk.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile quality_control_adk.py\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "async def handle_qc_logic(topic: str, draft_content: str):\n",
    "    print(f\"[ADK Service] 🧐 품질 관리팀 가동됨 (주제: {topic[:30]}...)\")\n",
    "    try:\n",
    "        qa_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "        prompt = f\"당신은 삼성전자 기술 블로그의 수석 편집자입니다. 다음 초안을 검토하고, 우리 블로그의 톤앤매너(전문적, 신뢰감, 명확함)에 맞춰 최종 발행본으로 만들어주세요.\\n\\n주제: {topic}\\n초안: {draft_content}\"\n",
    "        final_post = qa_llm.invoke(prompt).content\n",
    "        report = {\"seo_score\": 95, \"readability\": \"excellent\", \"final_char_count\": len(final_post), \"status\": \"Approved\"}\n",
    "        print(\"[ADK Service] ✅ 최종 편집 및 보고서 생성 완료.\")\n",
    "        return final_post, report\n",
    "    except Exception as e:\n",
    "        error_message = f\"ADK 품질 관리 중 오류 발생: {e}\"\n",
    "        print(f\"[ADK Service] 🔴 {error_message}\")\n",
    "        return draft_content, {\"status\": \"Failed\", \"error\": error_message}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "e2e_write_a2a_server_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting a2a_blog_system.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile a2a_blog_system.py\n",
    "\n",
    "from fastapi import FastAPI, Depends, HTTPException, Security\n",
    "from fastapi.security import APIKeyHeader\n",
    "import os\n",
    "import asyncio\n",
    "from typing import Optional\n",
    "from a2a_protocol import *\n",
    "from dialogue_manager_langgraph import handle_dialogue_logic\n",
    "from content_creation_crew import handle_creation_logic, MCPClient\n",
    "from quality_control_adk import handle_qc_logic\n",
    "\n",
    "app = FastAPI(title=\"A2A Integrated Research Blog System\", version=\"1.0.0\")\n",
    "\n",
    "API_KEY_NAME = \"X-API-Key\"\n",
    "api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)\n",
    "\n",
    "async def get_api_key(api_key: str = Security(api_key_header)):\n",
    "    if api_key == os.environ.get(\"MASTER_API_KEY\"):\n",
    "        return api_key\n",
    "    else:\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid API Key\")\n",
    "\n",
    "mcp_client: Optional[MCPClient] = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "def startup_event():\n",
    "    global mcp_client\n",
    "    mcp_server_url = os.environ.get(\"MCP_SERVER_URL\")\n",
    "    master_key = os.environ.get(\"MASTER_API_KEY\")\n",
    "    if mcp_server_url and master_key:\n",
    "        mcp_client = MCPClient(base_url=mcp_server_url, api_key=master_key)\n",
    "        print(f\"✅ A2A Gateway: MCP 클라이언트가 '{mcp_server_url}'에 연결되었습니다.\")\n",
    "    else:\n",
    "        print(\"⚠️ A2A Gateway: MCP_SERVER_URL 또는 MASTER_API_KEY가 설정되지 않아 MCP 클라이언트를 초기화할 수 없습니다.\")\n",
    "\n",
    "@app.get(\"/\", tags=[\"Status\"])\n",
    "async def root():\n",
    "    return {\"status\": \"ok\", \"message\": \"A2A Gateway is alive\"}\n",
    "\n",
    "@app.post(\"/api/v1/dialogue\", response_model=DialogueResponse, tags=[\"A2A Protocol\"])\n",
    "async def handle_dialogue(request: DialogueRequest, api_key: str = Depends(get_api_key)):\n",
    "    result = await handle_dialogue_logic(request.user_input)\n",
    "    return DialogueResponse(session_id=request.session_id, result)\n",
    "\n",
    "@app.post(\"/api/v1/create-content\", response_model=ContentCreationResponse, tags=[\"A2A Protocol\"])\n",
    "async def handle_content_creation(request: ContentCreationRequest, api_key: str = Depends(get_api_key)):\n",
    "    try:\n",
    "        draft = await asyncio.to_thread(handle_creation_logic, request.topic, request.user_preferences, mcp_client)\n",
    "        return ContentCreationResponse(draft_content=draft, status=\"COMPLETED\")\n",
    "    except Exception as e:\n",
    "        return ContentCreationResponse(draft_content=\"\", status=\"FAILED\", error_message=str(e))\n",
    "\n",
    "@app.post(\"/api/v1/quality-control\", response_model=QualityControlResponse, tags=[\"A2A Protocol\"])\n",
    "async def handle_quality_control(request: QualityControlRequest, api_key: str = Depends(get_api_key)):\n",
    "    final_post, report = await handle_qc_logic(request.topic, request.draft_content)\n",
    "    return QualityControlResponse(final_post=final_post, qa_report=report, status=\"COMPLETED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2e_build_launcher_md"
   },
   "source": [
    "### 3. 시스템 재구축 (Part 3): 서버 실행 유틸리티 생성\n",
    "마지막으로, 4일차에 완성했던 환경 감지형 서버 실행기를 `%%writefile`을 사용하여 `server_launcher.py` 파일로 저장합니다. 이를 통해 우리의 복잡한 서버 실행 로직을 모듈화하고, 메인 노트북을 깔끔하게 유지할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "e2e_write_launcher_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting server_launcher.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile server_launcher.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "\n",
    "def print_logs(process, name):\n",
    "    \"\"\"서버 프로세스의 로그를 실시간으로 출력하는 함수\"\"\"\n",
    "    # stderr를 사용하여 uvicorn의 로그를 읽습니다.\n",
    "    for line in iter(process.stderr.readline, ''):\n",
    "        print(f\"[{name} LOG] {line.strip()}\")\n",
    "\n",
    "def launch_fastapi_app(app_module_name: str, port: int):\n",
    "    \"\"\"\n",
    "    FastAPI 앱을 로컬에서 실행합니다.\n",
    "    \"\"\"\n",
    "    print(f\"🚀 {app_module_name} 서버를 로컬 포트 {port}에서 시작합니다...\")\n",
    "\n",
    "    # Step 1: 기존 프로세스 정리\n",
    "    try:\n",
    "        # pkill을 사용하여 특정 포트를 사용하는 uvicorn 프로세스를 종료합니다.\n",
    "        subprocess.run(['pkill', '-f', f\"uvicorn.*{app_module_name}.*--port {port}\"], capture_output=True)\n",
    "        print(f\"   🧹 포트 {port}의 기존 Uvicorn 프로세스를 정리했습니다.\")\n",
    "        time.sleep(2)\n",
    "    except Exception as e:\n",
    "        print(f\"   ℹ️ 프로세스 정리 중 오류 발생 (무시 가능): {e}\")\n",
    "\n",
    "    # Step 2: FastAPI 앱(Uvicorn) 백그라운드 실행\n",
    "    try:\n",
    "        command = [\n",
    "            sys.executable, '-m', 'uvicorn', f\"{app_module_name}:app\",\n",
    "            '--host', '0.0.0.0', '--port', str(port), '--reload'\n",
    "        ]\n",
    "        server_process = subprocess.Popen(\n",
    "            command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding='utf-8'\n",
    "        )\n",
    "        print(f\"   ⏳ FastAPI 서버를 포트 {port}에서 시작하는 중...\")\n",
    "\n",
    "        # 실시간 로그 출력을 위한 스레드 시작\n",
    "        log_thread = threading.Thread(target=print_logs, args=(server_process, app_module_name))\n",
    "        log_thread.daemon = True\n",
    "        log_thread.start()\n",
    "        print(\"   🔊 실시간 로그 출력을 시작합니다.\")\n",
    "        time.sleep(5) # 서버가 완전히 시작될 때까지 잠시 대기\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ FastAPI 서버 시작에 실패했습니다: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # Step 3: 로컬 주소와 프로세스 반환\n",
    "    local_url = f\"http://localhost:{port}\"\n",
    "    print(f\"✅ {app_module_name} 서버가 로컬({local_url})에서 성공적으로 실행되었습니다.\")\n",
    "\n",
    "    return local_url, server_process\n",
    "\n",
    "print(\"✅ 'server_launcher.py' 파일이 [로컬 전용]으로 준비되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6lQDlsMf0tq9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reverse_proxy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reverse_proxy.py\n",
    "\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import StreamingResponse\n",
    "import httpx\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "MCP_SERVER_URL = \"http://localhost:8501\"\n",
    "A2A_SERVER_URL = \"http://localhost:8502\"\n",
    "\n",
    "client = httpx.AsyncClient(timeout=300.0)\n",
    "\n",
    "async def proxy_request(target_url: str, request: Request):\n",
    "    \"\"\"\n",
    "    httpx를 스트리밍 모드로 사용하여 요청을 전달하고 응답을 스트리밍하는 함수.\n",
    "    StreamConsumed 오류를 방지하기 위해 client.send(stream=True)를 사용합니다.\n",
    "    \"\"\"\n",
    "    # 1. 클라이언트의 요청 정보를 기반으로 내부 서버로 보낼 요청을 재구성합니다.\n",
    "    headers = {k: v for k, v in request.headers.items() if k.lower() not in ['host', 'cookie']}\n",
    "\n",
    "    req = client.build_request(\n",
    "        method=request.method,\n",
    "        url=target_url,\n",
    "        headers=headers,\n",
    "        params=request.query_params,\n",
    "        content=await request.body()\n",
    "    )\n",
    "\n",
    "    # 2. stream=True 옵션으로 요청을 보내 응답 본문을 미리 읽지 않도록 합니다.\n",
    "    r = await client.send(req, stream=True)\n",
    "\n",
    "    # 3. 내부 서버의 응답을 클라이언트에게 그대로 스트리밍합니다.\n",
    "    return StreamingResponse(\n",
    "        r.aiter_raw(),\n",
    "        status_code=r.status_code,\n",
    "        headers=r.headers\n",
    "    )\n",
    "\n",
    "@app.api_route(\"/mcp/{path:path}\", methods=[\"GET\", \"POST\", \"PUT\", \"DELETE\", \"PATCH\"])\n",
    "async def route_mcp(path: str, request: Request):\n",
    "    \"\"\" '/mcp'로 시작하는 모든 요청을 MCP 서버(8501)로 전달합니다. \"\"\"\n",
    "    target_url = f\"{MCP_SERVER_URL}/{path}\"\n",
    "    return await proxy_request(target_url, request)\n",
    "\n",
    "@app.api_route(\"/a2a/{path:path}\", methods=[\"GET\", \"POST\", \"PUT\", \"DELETE\", \"PATCH\"])\n",
    "async def route_a2a(path: str, request: Request):\n",
    "    \"\"\" '/a2a'로 시작하는 모든 요청을 A2A Gateway 서버(8502)로 전달합니다. \"\"\"\n",
    "    target_url = f\"{A2A_SERVER_URL}/{path}\"\n",
    "    return await proxy_request(target_url, request)\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Reverse Proxy is running. Use /mcp/ or /a2a/ prefixes.\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EumyrltdvLsR"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2e_inject_infra_md"
   },
   "source": [
    "### 4. 관측 가능성 주입 (Part 1): 인프라 계층 추적 (MCP 서버 & A2A 게이트웨이)\n",
    "\n",
    "가장 먼저, 우리 시스템의 가장 바깥쪽 경계선이자 기반 시설인 두 개의 서버에 추적 기능을 심습니다. 이를 통해 우리는 어떤 요청이 언제 들어왔고, 어떤 Tool이 얼마나 자주 호출되는지에 대한 거시적인 그림을 확보할 수 있습니다.\n",
    "\n",
    "1.  A2A 게이트웨이 (`a2a_blog_system.py`): `FastAPI Middleware`를 사용하여, 게이트웨이가 받는 모든 API 요청/응답을 자동으로 추적하는 최상위 부모 Trace를 생성합니다. 이것이 모든 분석의 시작점이 됩니다.\n",
    "2.  MCP 서버 (`mcp_server.py`): 각 Tool 엔드포인트에 `@observe()` 데코레이터를 추가하여, 어떤 상위 Agent(e.g., CrewAI)가 어떤 Tool을 호출했는지 개별적인 자식 Trace로 기록합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "e2e_update_mcp_server_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_server.py\n",
    "\n",
    "# --- Imports ---\n",
    "from fastapi import FastAPI, HTTPException, Depends, Security, Request\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "from typing import Dict\n",
    "from fastapi.security import APIKeyHeader\n",
    "from slowapi import Limiter, _rate_limit_exceeded_handler\n",
    "from slowapi.util import get_remote_address\n",
    "from slowapi.errors import RateLimitExceeded\n",
    "from server_tools import web_search, news_api_search, arxiv_search\n",
    "from server_resources import get_template, get_style_guide\n",
    "from langfuse import observe\n",
    "\n",
    "# --- App & Limiter Setup ---\n",
    "app = FastAPI(title=\"LLM Agent Resource Hub (MCP Server)\", version=\"1.0.0\")\n",
    "limiter = Limiter(key_func=get_remote_address)\n",
    "app.state.limiter = limiter\n",
    "app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n",
    "\n",
    "# --- Security Setup ---\n",
    "API_KEY_NAME = \"X-API-Key\"\n",
    "api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)\n",
    "async def get_api_key(api_key: str = Security(api_key_header)):\n",
    "    if api_key == os.environ.get(\"MASTER_API_KEY\"):\n",
    "        return api_key\n",
    "    else:\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid or missing API Key\")\n",
    "\n",
    "# --- Pydantic Models ---\n",
    "class ToolCallRequest(BaseModel):\n",
    "    query: str = Field(..., description=\"Tool에 전달할 검색어 또는 입력값\")\n",
    "\n",
    "# --- Root Endpoint ---\n",
    "@app.get(\"/\", summary=\"서버 상태 확인\", tags=[\"Status\"])\n",
    "async def read_root():\n",
    "    return {\"status\": \"ok\", \"message\": \"MCP Server is running successfully.\"}\n",
    "\n",
    "# --- Production Endpoints with Langfuse Observability ---\n",
    "@app.post(\"/api/v1/tools/web_search\", summary=\"웹 검색 (보안/속도제한/추적)\", tags=[\"Production Tools\"])\n",
    "@limiter.limit(\"20/minute\")\n",
    "@observe(name=\"mcp-tool-web-search\")\n",
    "async def prod_web_search(request: Request, data: ToolCallRequest, api_key: str = Depends(get_api_key)):\n",
    "    result = await web_search(data.query)\n",
    "    return {\"tool\": \"web_search\", \"query\": data.query, \"result\": result}\n",
    "\n",
    "@app.post(\"/api/v1/tools/news_api\", summary=\"뉴스 검색 (보안/속도제한/추적)\", tags=[\"Production Tools\"])\n",
    "@limiter.limit(\"20/minute\")\n",
    "@observe(name=\"mcp-tool-news-api\")\n",
    "async def prod_news_search(request: Request, data: ToolCallRequest, api_key: str = Depends(get_api_key)):\n",
    "    result = await news_api_search(data.query)\n",
    "    return {\"tool\": \"news_api\", \"query\": data.query, \"result\": result}\n",
    "\n",
    "@app.post(\"/api/v1/tools/arxiv_search\", summary=\"논문 검색 (보안/속도제한/추적)\", tags=[\"Production Tools\"])\n",
    "@limiter.limit(\"20/minute\")\n",
    "@observe(name=\"mcp-tool-arxiv-search\")\n",
    "async def prod_arxiv_search(request: Request, data: ToolCallRequest, api_key: str = Depends(get_api_key)):\n",
    "    result = await arxiv_search(data.query)\n",
    "    return {\"tool\": \"arxiv_search\", \"query\": data.query, \"result\": result}\n",
    "\n",
    "@app.get(\"/api/v1/resources/templates/{template_id}\", summary=\"템플릿 조회 (보안/속도제한/추적)\", tags=[\"Production Resources\"])\n",
    "@limiter.limit(\"60/minute\")\n",
    "@observe(name=\"mcp-resource-template\")\n",
    "async def prod_read_template(request: Request, template_id: str, api_key: str = Depends(get_api_key)):\n",
    "    content = await get_template(template_id)\n",
    "    if isinstance(content, dict) and \"error\" in content:\n",
    "        raise HTTPException(status_code=404, detail=content[\"error\"])\n",
    "    return {\"resource\": \"template\", \"id\": template_id, \"content\": content}\n",
    "\n",
    "print(\"mcp_server.py 파일이 Langfuse 추적 기능으로 업데이트되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "e2e_update_a2a_server_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting a2a_blog_system.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile a2a_blog_system.py\n",
    "\n",
    "# --- Imports ---\n",
    "from fastapi import FastAPI, Depends, HTTPException, Security\n",
    "from fastapi.security import APIKeyHeader\n",
    "from contextlib import asynccontextmanager\n",
    "import os\n",
    "import asyncio\n",
    "from typing import Optional\n",
    "from a2a_protocol import *\n",
    "from dialogue_manager_langgraph import handle_dialogue_logic\n",
    "from content_creation_crew import handle_creation_logic, MCPClient\n",
    "from quality_control_adk import handle_qc_logic\n",
    "\n",
    "# Langfuse v3 SDK import\n",
    "from langfuse import get_client\n",
    "\n",
    "# Langfuse 클라이언트 전역 변수\n",
    "langfuse_client = None\n",
    "\n",
    "# --- FastAPI Lifespan ---\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    global langfuse_client\n",
    "    # Startup: Langfuse 클라이언트 초기화\n",
    "    try:\n",
    "        langfuse_client = get_client()\n",
    "        if langfuse_client:\n",
    "            if langfuse_client.auth_check():\n",
    "                print(\"Langfuse 클라이언트가 성공적으로 초기화되었습니다.\")\n",
    "            else:\n",
    "                print(\"Langfuse 인증 실패\")\n",
    "    except Exception as e:\n",
    "        print(f\"Langfuse 초기화 실패: {e}\")\n",
    "        langfuse_client = None\n",
    "\n",
    "    yield  # 서버 실행\n",
    "\n",
    "    # Shutdown: Langfuse flush\n",
    "    if langfuse_client:\n",
    "        langfuse_client.flush()\n",
    "        print(\"✅ Langfuse 이벤트가 모두 전송되었습니다.\")\n",
    "\n",
    "# --- App & Security Setup ---\n",
    "app = FastAPI(\n",
    "    title=\"A2A Integrated Research Blog System\",\n",
    "    version=\"1.0.0\",\n",
    "    lifespan=lifespan\n",
    ")\n",
    "\n",
    "API_KEY_NAME = \"X-API-Key\"\n",
    "api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)\n",
    "\n",
    "async def get_api_key(api_key: str = Security(api_key_header)):\n",
    "    if api_key == os.environ.get(\"MASTER_API_KEY\"):\n",
    "        return api_key\n",
    "    else:\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid API Key\")\n",
    "\n",
    "# --- MCP Client Setup ---\n",
    "mcp_client: Optional[MCPClient] = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "def startup_event():\n",
    "    global mcp_client\n",
    "    mcp_server_url = os.environ.get(\"MCP_SERVER_URL\")\n",
    "    master_key = os.environ.get(\"MASTER_API_KEY\")\n",
    "    if mcp_server_url and master_key:\n",
    "        mcp_client = MCPClient(base_url=mcp_server_url, api_key=master_key)\n",
    "        print(f\"A2A Gateway: MCP 클라이언트가 '{mcp_server_url}'에 연결되었습니다.\")\n",
    "    else:\n",
    "        print(\"A2A Gateway: MCP_SERVER_URL 또는 MASTER_API_KEY가 설정되지 않아 MCP 클라이언트를 초기화할 수 없습니다.\")\n",
    "\n",
    "# --- Endpoints ---\n",
    "@app.get(\"/\", tags=[\"Status\"])\n",
    "async def root():\n",
    "    return {\"status\": \"ok\", \"message\": \"A2A Gateway is alive\"}\n",
    "\n",
    "@app.post(\"/api/v1/dialogue\", response_model=DialogueResponse, tags=[\"A2A Protocol\"])\n",
    "async def handle_dialogue(request: DialogueRequest, api_key: str = Depends(get_api_key)):\n",
    "\n",
    "    if langfuse_client:\n",
    "        with langfuse_client.start_as_current_span(\n",
    "            name=\"dialogue-request\",\n",
    "            input={\n",
    "                \"user_input\": request.user_input,\n",
    "                \"session_id\": request.session_id\n",
    "            }\n",
    "        ) as span:\n",
    "            # 트레이스 속성 설정\n",
    "            span.update_trace(\n",
    "                session_id=request.session_id,\n",
    "                tags=[\"dialogue\", \"api\"],\n",
    "                metadata={\"endpoint\": \"/api/v1/dialogue\"}\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                result = await handle_dialogue_logic(request.user_input)\n",
    "\n",
    "                # 결과 업데이트\n",
    "                span.update(output=result)\n",
    "                span.update_trace(output=result)\n",
    "\n",
    "                return DialogueResponse(session_id=request.session_id, **result)\n",
    "\n",
    "            except Exception as e:\n",
    "                span.update(\n",
    "                    output={\"error\": str(e)},\n",
    "                    level=\"ERROR\",\n",
    "                    status_message=f\"Dialogue processing failed: {str(e)}\"\n",
    "                )\n",
    "                raise\n",
    "    else:\n",
    "        # Langfuse 없이 실행\n",
    "        result = await handle_dialogue_logic(request.user_input)\n",
    "        return DialogueResponse(session_id=request.session_id, **result)\n",
    "\n",
    "@app.post(\"/api/v1/create-content\", response_model=ContentCreationResponse, tags=[\"A2A Protocol\"])\n",
    "async def handle_content_creation(request: ContentCreationRequest, api_key: str = Depends(get_api_key)):\n",
    "\n",
    "    if langfuse_client:\n",
    "        with langfuse_client.start_as_current_span(\n",
    "            name=\"content-creation-request\",\n",
    "            input={\n",
    "                \"topic\": request.topic,\n",
    "                \"user_preferences\": request.user_preferences\n",
    "            }\n",
    "        ) as span:\n",
    "            # 트레이스 속성 설정\n",
    "            span.update_trace(\n",
    "                tags=[\"content-creation\", \"api\"],\n",
    "                metadata={\n",
    "                    \"endpoint\": \"/api/v1/create-content\",\n",
    "                    \"topic\": request.topic\n",
    "                }\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                draft = await asyncio.to_thread(\n",
    "                    handle_creation_logic,\n",
    "                    request.topic,\n",
    "                    request.user_preferences,\n",
    "                    mcp_client\n",
    "                )\n",
    "\n",
    "                # 결과 업데이트\n",
    "                response_data = {\"draft_content\": draft, \"status\": \"COMPLETED\"}\n",
    "                span.update(output=response_data)\n",
    "                span.update_trace(output=response_data)\n",
    "\n",
    "                return ContentCreationResponse(draft_content=draft, status=\"COMPLETED\")\n",
    "\n",
    "            except Exception as e:\n",
    "                error_data = {\"error\": str(e), \"status\": \"FAILED\"}\n",
    "                span.update(\n",
    "                    output=error_data,\n",
    "                    level=\"ERROR\",\n",
    "                    status_message=f\"Content creation failed: {str(e)}\"\n",
    "                )\n",
    "                return ContentCreationResponse(draft_content=\"\", status=\"FAILED\", error_message=str(e))\n",
    "    else:\n",
    "        # Langfuse 없이 실행\n",
    "        try:\n",
    "            draft = await asyncio.to_thread(handle_creation_logic, request.topic, request.user_preferences, mcp_client)\n",
    "            return ContentCreationResponse(draft_content=draft, status=\"COMPLETED\")\n",
    "        except Exception as e:\n",
    "            return ContentCreationResponse(draft_content=\"\", status=\"FAILED\", error_message=str(e))\n",
    "\n",
    "@app.post(\"/api/v1/quality-control\", response_model=QualityControlResponse, tags=[\"A2A Protocol\"])\n",
    "async def handle_quality_control(request: QualityControlRequest, api_key: str = Depends(get_api_key)):\n",
    "\n",
    "    if langfuse_client:\n",
    "        with langfuse_client.start_as_current_span(\n",
    "            name=\"quality-control-request\",\n",
    "            input={\n",
    "                \"topic\": request.topic,\n",
    "                \"draft_content_length\": len(request.draft_content)\n",
    "            }\n",
    "        ) as span:\n",
    "            # 트레이스 속성 설정\n",
    "            span.update_trace(\n",
    "                tags=[\"quality-control\", \"api\"],\n",
    "                metadata={\n",
    "                    \"endpoint\": \"/api/v1/quality-control\",\n",
    "                    \"topic\": request.topic\n",
    "                }\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                final_post, report = await handle_qc_logic(request.topic, request.draft_content)\n",
    "\n",
    "                # 결과 업데이트\n",
    "                response_data = {\n",
    "                    \"final_post\": final_post,\n",
    "                    \"qa_report\": report,\n",
    "                    \"status\": \"COMPLETED\"\n",
    "                }\n",
    "                span.update(output=response_data)\n",
    "                span.update_trace(output=response_data)\n",
    "\n",
    "                return QualityControlResponse(final_post=final_post, qa_report=report, status=\"COMPLETED\")\n",
    "\n",
    "            except Exception as e:\n",
    "                span.update(\n",
    "                    output={\"error\": str(e), \"status\": \"FAILED\"},\n",
    "                    level=\"ERROR\",\n",
    "                    status_message=f\"Quality control failed: {str(e)}\"\n",
    "                )\n",
    "                raise\n",
    "    else:\n",
    "        # Langfuse 없이 실행\n",
    "        final_post, report = await handle_qc_logic(request.topic, request.draft_content)\n",
    "        return QualityControlResponse(final_post=final_post, qa_report=report, status=\"COMPLETED\")\n",
    "\n",
    "print(\"a2a_blog_system.py 파일이 Langfuse v3 SDK로 업데이트되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfHY3wElyar1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2e_inject_services_md"
   },
   "source": [
    "### 6. 관측 가능성 주입 (Part 2): 전문가 서비스 내부 추적\n",
    "\n",
    "Part 1에서 우리는 시스템의 '외부 골격'에 해당하는 인프라 계층(서버)에 추적 기능을 심었습니다. 이제 시스템 내부에 해당하는 각 전문가 서비스 모듈의 동작을 추적하여, 분석의 깊이를 높입니다. 이를 통해 우리는 \"CrewAI 팀 내부에서 어떤 Agent가 어떤 Tool을 썼는가?\" 와 같은 세밀한 질문에 답할 수 있게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "e2e_update_langgraph_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dialogue_manager_langgraph.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dialogue_manager_langgraph.py\n",
    "\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langfuse.langchain import CallbackHandler\n",
    "from langfuse import observe\n",
    "\n",
    "async def handle_dialogue_logic(user_input: str):\n",
    "    \"\"\"LangGraph Agent의 역할을 시뮬레이션하는 대화 관리 로직 (Langfuse 추적 기능 추가)\"\"\"\n",
    "    print(\"[LangGraph Service] 🧠 대화 관리자 실행됨...\")\n",
    "\n",
    "    # 각 서비스 호출을 위한 고유한 Langfuse 핸들러 생성\n",
    "    handler = CallbackHandler()\n",
    "\n",
    "    try:\n",
    "        llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0)\n",
    "        prompt = f\"다음 사용자 요청의 핵심 주제를 20단어 이내의 간결한 한 문장으로 요약하고, 사용자의 숨겨진 요구사항(스타일, 톤앤매너 등)을 추론해줘. 결과는 '주제: [요약된 주제]\\n요구사항: [추론된 요구사항]' 형식으로만 답변해줘. 다른 말은 절대 추가하지 마.\\n\\n사용자 요청: '{user_input}'\"\n",
    "\n",
    "        # LLM 호출 시 config에 콜백 핸들러 전달\n",
    "        response_text = llm.invoke(prompt, config={\"callbacks\": [handler]}).content\n",
    "\n",
    "        topic = response_text.split(\"주제:\")[1].split(\"요구사항:\")[0].strip()\n",
    "        preferences = response_text.split(\"요구사항:\")[1].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[LangGraph Service] 🔴 LLM 호출 오류, Fallback 로직 사용: {e}\")\n",
    "        topic = user_input\n",
    "        preferences = \"전문적이면서도 쉬운 어조로 작성해주세요.\"\n",
    "\n",
    "    response = {\n",
    "        \"agent_response\": f\"알겠습니다. '{topic}'에 대한 블로그 글 생성을 시작하겠습니다. '{preferences}' 요구사항을 반영하겠습니다.\",\n",
    "        \"next_action\": {\"action\": \"CREATE_CONTENT\", \"topic\": topic, \"user_preferences\": preferences}\n",
    "    }\n",
    "    print(f\"[LangGraph Service] ✅ 다음 행동 결정: {response['next_action']}\")\n",
    "    return response\n",
    "\n",
    "print(\"✅ dialogue_manager_langgraph.py 파일이 Langfuse 추적 기능으로 업데이트되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "e2e_update_crewai_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting content_creation_crew.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile content_creation_crew.py\n",
    "\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.tools import BaseTool\n",
    "import json\n",
    "from mcp_client import MCPClient\n",
    "from langfuse.langchain import CallbackHandler as LangfuseCallbackHandler\n",
    "from langfuse import observe\n",
    "\n",
    "class MCPTool(BaseTool):\n",
    "    name: str\n",
    "    description: str\n",
    "    client: MCPClient\n",
    "\n",
    "    @observe(name=\"mcp-tool-call\")\n",
    "    def _run(self, query: str) -> str:\n",
    "        print(f\"  [MCP Bridge] CrewAI -> MCP Server: Calling tool '{self.name}' with query '{query}'\")\n",
    "        response = self.client.call_tool(self.name, query)\n",
    "        return json.dumps(response, ensure_ascii=False)\n",
    "\n",
    "def handle_creation_logic(topic: str, user_preferences: str, mcp_client: MCPClient):\n",
    "    print(f\"[CrewAI Service] 👥 콘텐츠 제작팀 가동됨 (주제: {topic[:30]}...)\")\n",
    "    try:\n",
    "        handler = LangfuseCallbackHandler()\n",
    "\n",
    "        crew_web_search = MCPTool(name=\"web_search\", description=\"웹에서 최신 정보를 검색합니다.\", client=mcp_client)\n",
    "        crew_arxiv_search = MCPTool(name=\"arxiv_search\", description=\"학술 논문을 검색합니다.\", client=mcp_client)\n",
    "\n",
    "        llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", callbacks=[handler])\n",
    "        researcher = Agent(role=\"선임 리서처\", goal=f\"{topic}에 대한 심층 분석\", backstory=\"당신은 20년 경력의 기술 분석 전문가입니다.\", tools=[crew_web_search, crew_arxiv_search], llm=llm, verbose=True)\n",
    "        writer = Agent(role=\"전문 작가\", goal=\"리서치 결과를 바탕으로 매력적인 블로그 글 작성\", backstory=\"당신은 기술 분야의 베스트셀러 작가입니다.\", llm=llm, verbose=True)\n",
    "\n",
    "        research_task = Task(description=f\"'{topic}'에 대해 웹과 학술 자료를 종합하여 심층 분석 보고서를 작성하세요.\", expected_output=\"구조화된 분석 보고서\", agent=researcher)\n",
    "        write_task = Task(description=f\"리서치 보고서를 바탕으로, '{user_preferences}' 스타일을 반영하여 블로그 초안을 작성하세요.\", expected_output=\"완성된 블로그 초안\", agent=writer, context=[research_task])\n",
    "\n",
    "        crew = Crew(\n",
    "            agents=[researcher, writer],\n",
    "            tasks=[research_task, write_task],\n",
    "            process=Process.sequential\n",
    "        )\n",
    "        result = crew.kickoff()\n",
    "        print(\"[CrewAI Service] ✅ 초안 작성 완료.\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        error_message = f\"CrewAI 실행 중 오류 발생: {e}\"\n",
    "        print(f\"[CrewAI Service] 🔴 {error_message}\")\n",
    "        return error_message\n",
    "\n",
    "print(\"✅ content_creation_crew.py 파일이 Langfuse 추적 기능으로 업데이트되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "e2e_update_adk_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting quality_control_adk.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile quality_control_adk.py\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "async def handle_qc_logic(topic: str, draft_content: str):\n",
    "    print(f\"[ADK Service] 🧐 품질 관리팀 가동됨 (주제: {topic[:30]}...)\")\n",
    "\n",
    "    # ADK 서비스 호출을 위한 핸들러 생성\n",
    "    handler = CallbackHandler()\n",
    "\n",
    "    try:\n",
    "        qa_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "        prompt = f\"당신은 삼성전자 기술 블로그의 수석 편집자입니다. 다음 초안을 검토하고, 우리 블로그의 톤앤매너(전문적, 신뢰감, 명확함)에 맞춰 최종 발행 가능한 완벽한 최종본으로 만들어주세요.\\n\\n주제: {topic}\\n초안: {draft_content}\"\n",
    "\n",
    "        # LLM 호출 시 config에 콜백 핸들러 전달\n",
    "        final_post = qa_llm.invoke(prompt, config={\"callbacks\": [handler]}).content\n",
    "\n",
    "        report = {\"seo_score\": 95, \"readability\": \"excellent\", \"final_char_count\": len(final_post), \"status\": \"Approved\"}\n",
    "        print(\"[ADK Service] ✅ 최종 편집 및 보고서 생성 완료.\")\n",
    "        return final_post, report\n",
    "    except Exception as e:\n",
    "        error_message = f\"ADK 품질 관리 중 오류 발생: {e}\"\n",
    "        print(f\"[ADK Service] 🔴 {error_message}\")\n",
    "        return draft_content, {\"status\": \"Failed\", \"error\": error_message}\n",
    "\n",
    "print(\"✅ quality_control_adk.py 파일이 Langfuse 추적 기능으로 업데이트되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2e_final_run_md"
   },
   "source": [
    "### 8. 최종 실행 및 통합 Trace 분석\n",
    "\n",
    "이제 모든 준비가 끝났습니다. 인프라 계층부터 각 전문가 서비스의 내부까지, 시스템의 모든 단계에 `Langfuse`를 설치했습니다. 이제 두 개의 서버(MCP, A2A)를 모두 재실행하고, 오케스트레이터 클라이언트를 통해 블로그 생성 요청을 한 번 보냅니다.\n",
    "\n",
    "그리고 `Langfuse` 대시보드에서, 이 요청이 어떻게 수십 개의 계층적 Trace와 Span으로 기록되었는지 함께 분석해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "e2e_final_run_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'server_launcher.py' 파일이 [로컬 전용]으로 준비되었습니다.\n",
      "--- 🚀 1. MCP 서버 로컬 실행 시작 ---\n",
      "🚀 mcp_server 서버를 로컬 포트 8501에서 시작합니다...\n",
      "   🧹 포트 8501의 기존 Uvicorn 프로세스를 정리했습니다.\n",
      "   ⏳ FastAPI 서버를 포트 8501에서 시작하는 중...\n",
      "   🔊 실시간 로그 출력을 시작합니다.\n",
      "[mcp_server LOG] INFO:     Will watch for changes in these directories: ['/home/elicer']\n",
      "[mcp_server LOG] INFO:     Uvicorn running on http://0.0.0.0:8501 (Press CTRL+C to quit)\n",
      "[mcp_server LOG] INFO:     Started reloader process [8648] using WatchFiles\n",
      "✅ mcp_server 서버가 로컬(http://localhost:8501)에서 성공적으로 실행되었습니다.\n",
      "--------------------------------------------------\n",
      "--- 🚀 2. A2A Gateway 서버 로컬 실행 시작 ---\n",
      "🚀 a2a_blog_system 서버를 로컬 포트 8502에서 시작합니다...\n",
      "   🧹 포트 8502의 기존 Uvicorn 프로세스를 정리했습니다.\n",
      "[mcp_server LOG] INFO:     Started server process [8657]\n",
      "[mcp_server LOG] INFO:     Waiting for application startup.\n",
      "[mcp_server LOG] INFO:     Application startup complete.\n",
      "   ⏳ FastAPI 서버를 포트 8502에서 시작하는 중...\n",
      "   🔊 실시간 로그 출력을 시작합니다.\n",
      "[a2a_blog_system LOG] INFO:     Will watch for changes in these directories: ['/home/elicer']\n",
      "[a2a_blog_system LOG] INFO:     Uvicorn running on http://0.0.0.0:8502 (Press CTRL+C to quit)\n",
      "[a2a_blog_system LOG] INFO:     Started reloader process [8677] using WatchFiles\n",
      "✅ a2a_blog_system 서버가 로컬(http://localhost:8502)에서 성공적으로 실행되었습니다.\n",
      "--------------------------------------------------\n",
      "--- 🚀 3. 리버스 프록시 서버 로컬 실행 시작 ---\n",
      "🚀 reverse_proxy 서버를 로컬 포트 8000에서 시작합니다...\n",
      "   🧹 포트 8000의 기존 Uvicorn 프로세스를 정리했습니다.\n",
      "   ⏳ FastAPI 서버를 포트 8000에서 시작하는 중...\n",
      "   🔊 실시간 로그 출력을 시작합니다.\n",
      "[reverse_proxy LOG] INFO:     Will watch for changes in these directories: ['/home/elicer']\n",
      "[reverse_proxy LOG] INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "[reverse_proxy LOG] INFO:     Started reloader process [8702] using WatchFiles\n",
      "[reverse_proxy LOG] INFO:     Started server process [8708]\n",
      "[reverse_proxy LOG] INFO:     Waiting for application startup.\n",
      "[reverse_proxy LOG] INFO:     Application startup complete.\n",
      "✅ reverse_proxy 서버가 로컬(http://localhost:8000)에서 성공적으로 실행되었습니다.\n",
      "\n",
      "============================================================\n",
      "🎉 모든 서버가 로컬 환경에서 준비되었습니다!\n",
      "   - 리버스 프록시 Entrypoint: http://localhost:8000\n",
      "   - MCP 서버 Local URL: http://localhost:8000/mcp\n",
      "   - A2A Gateway Local URL: http://localhost:8000/a2a\n",
      "============================================================\n",
      "\n",
      "   A2A Gateway 및 MCP 서버 초기화 대기 중...\n",
      "[a2a_blog_system LOG] INFO:     Started server process [8680]\n",
      "[a2a_blog_system LOG] INFO:     Waiting for application startup.\n",
      "[a2a_blog_system LOG] INFO:     Application startup complete.\n",
      "   ✅ A2A Gateway (via Proxy) 준비 완료!\n",
      "\n",
      "--- 🚀 최종 오케스트레이션 시뮬레이션 시작 ---\n",
      "\n",
      "1️⃣  LangGraph 대화 관리자 호출...\n",
      "   응답 수신: CREATE_CONTENT\n",
      "\n",
      "2️⃣  CrewAI 콘텐츠 제작팀 호출...\n",
      "\n",
      "3️⃣  ADK 품질 관리팀 호출...\n",
      "\n",
      "================================================================================\n",
      "                           ✅ 최종 블로그 포스트 ✅\n",
      "================================================================================\n",
      "## 삼성전자 기술 블로그: 멀티모달 LLM 최신 동향 및 산업 적용 사례 심층 분석\n",
      "\n",
      "---\n",
      "\n",
      "### **제목: 멀티모달 LLM: 인간처럼 세상을 이해하는 AI의 진화와 산업 혁신**\n",
      "\n",
      "**부제: 텍스트를 넘어 이미지, 음성, 비디오까지, AI가 열어갈 새로운 가능성**\n",
      "\n",
      "---\n",
      "\n",
      "**[서론]**\n",
      "\n",
      "인공지능(AI)은 이미 우리 삶의 많은 부분을 변화시켰지만, 진정한 지능은 단순히 텍스트를 넘어선다. 인간은 시각, 청각, 촉각 등 다양한 감각을 통해 세상을 인지하고 이해하며 소통합니다. 이러한 인간의 인지 방식에 한 걸음 더 다가선 기술이 바로 **멀티모달 거대 언어 모델(Multimodal Large Language Model, 이하 멀티모달 LLM)**입니다.\n",
      "\n",
      "기존 LLM이 텍스트 데이터만을 기반으로 학습하고 추론했다면, 멀티모달 LLM은 텍스트뿐만 아니라 이미지, 음성, 비디오 등 다양한 형태의 데이터를 동시에 이해하고 생성하는 능력을 갖춥니다. 이는 AI가 더욱 복잡하고 현실적인 문제를 해결하고, 인간과 더욱 자연스럽게 상호작용할 수 있는 새로운 지평을 열고 있습니다.\n",
      "\n",
      "삼성전자 기술 블로그에서는 멀티모달 LLM의 최신 동향을 심층 분석하고, 이 혁신적인 기술이 다양한 산업 분야, 특히 삼성전자의 핵심 사업 영역에서 어떻게 적용되고 미래를 변화시킬지 조명하고자 합니다.\n",
      "\n",
      "---\n",
      "\n",
      "**[본론 1: 멀티모달 LLM이란 무엇인가?]**\n",
      "\n",
      "멀티모달 LLM은 텍스트, 이미지, 음성, 비디오 등 여러 양식(modality)의 데이터를 통합적으로 처리하는 AI 모델입니다. 예를 들어, 단순히 이미지를 설명하는 것을 넘어, 이미지에 대한 질문에 답하거나, 텍스트 지시를 받아 이미지를 생성하고, 음성 명령을 이해하여 특정 작업을 수행하는 등 복합적인 인지 능력을 보여줍니다.\n",
      "\n",
      "이러한 모델은 일반적으로 다음과 같은 방식으로 작동합니다.\n",
      "\n",
      "1.  **모달리티별 인코딩:** 각기 다른 형태의 데이터를 AI 모델이 이해할 수 있는 공통된 벡터 공간(embedding space)으로 변환합니다. 예를 들어, 이미지는 시각 인코더를 통해, 텍스트는 텍스트 인코더를 통해 처리됩니다.\n",
      "2.  **정보 융합:** 변환된 여러 모달리티의 정보를 통합하여 상호 관계를 학습하고, 복합적인 의미를 파악합니다. 이 과정에서 트랜스포머(Transformer) 기반의 어텐션(Attention) 메커니즘이 핵심적인 역할을 합니다.\n",
      "3.  **멀티모달 추론 및 생성:** 통합된 정보를 바탕으로 질문에 답하거나, 새로운 콘텐츠를 생성하는 등 다양한 작업을 수행합니다.\n",
      "\n",
      "---\n",
      "\n",
      "**[본론 2: 멀티모달 LLM의 최신 기술 동향]**\n",
      "\n",
      "멀티모달 LLM 분야는 최근 몇 년간 눈부신 발전을 거듭하며 다양한 혁신을 선보이고 있습니다. 주요 동향은 다음과 같습니다.\n",
      "\n",
      "1.  **통합 아키텍처의 진화:**\n",
      "    *   **단일 모델 통합:** 텍스트, 이미지, 음성 등 모든 모달리티를 하나의 거대한 트랜스포머 모델로 처리하려는 시도가 활발합니다. 이는 모델의 범용성과 효율성을 높이는 데 기여합니다. Google의 Gemini, OpenAI의 GPT-4V 등이 대표적인 예시입니다.\n",
      "    *   **효율적인 융합 전략:** 다양한 모달리티 정보를 효과적으로 결합하기 위한 크로스-어텐션(Cross-Attention), 어댑터(Adapter) 기반 융합 등 새로운 아키텍처 연구가 진행되고 있습니다.\n",
      "\n",
      "2.  **데이터셋의 대규모화 및 다양화:**\n",
      "    *   멀티모달 LLM의 성능 향상에는 대규모의 고품질 멀티모달 데이터셋 구축이 필수적입니다. 웹 스케일의 이미지-텍스트 쌍, 비디오-텍스트 쌍, 음성-텍스트 쌍 데이터셋이 지속적으로 확장되고 있습니다.\n",
      "    *   단순한 데이터 양을 넘어, 데이터의 다양성과 복잡성을 높여 모델이 더욱 풍부한 맥락을 이해하도록 돕는 연구도 활발합니다.\n",
      "\n",
      "3.  **새로운 능력의 발현:**\n",
      "    *   **시각 질의응답(Visual Question Answering, VQA):** 이미지 내용을 이해하고 질문에 정확하게 답변하는 능력.\n",
      "    *   **텍스트-이미지/비디오 생성:** 텍스트 프롬프트만으로 고품질의 이미지나 비디오를 생성하는 기술(예: DALL-E, Midjourney, Sora).\n",
      "    *   **음성 인식 및 합성:** 자연스러운 음성 인식과 사람과 유사한 음성 합성 기술.\n",
      "    *   **임베디드 AI (Embodied AI):** 로봇이나 자율주행차와 같은 물리적 환경에서 멀티모달 정보를 활용하여 인지, 판단, 행동하는 능력.\n",
      "\n",
      "4.  **온디바이스(On-Device) AI로의 확장:**\n",
      "    *   클라우드 기반의 대규모 모델을 넘어, 스마트폰, 가전제품 등 엣지 디바이스에서도 멀티모달 LLM을 효율적으로 구동하기 위한 경량화, 최적화 기술이 중요해지고 있습니다. 이는 사용자 경험 향상과 개인 정보 보호에 기여합니다.\n",
      "\n",
      "---\n",
      "\n",
      "**[본론 3: 멀티모달 LLM의 산업 적용 사례 및 삼성전자의 비전]**\n",
      "\n",
      "멀티모달 LLM은 다양한 산업 분야에서 혁신적인 변화를 이끌 잠재력을 가지고 있으며, 삼성전자는 이 기술을 통해 미래를 선도하고자 합니다.\n",
      "\n",
      "1.  **소비자 가전 및 모바일 경험 혁신:**\n",
      "    *   **스마트폰:** 카메라 앱에서 촬영된 이미지를 실시간으로 분석하여 최적의 촬영 설정을 제안하거나, 갤러리 내 이미지에 대한 복합적인 질문에 답변(예: \"이 사진 속 강아지가 어떤 품종인지 알려줘\"). 온디바이스 AI를 통해 개인화된 비서 기능(Bixby)을 더욱 고도화하여 음성, 텍스트, 시각 정보를 통합적으로 처리합니다.\n",
      "    *   **스마트 TV:** 시청 중인 콘텐츠의 장면을 분석하여 관련 정보를 제공하거나, 음성 명령과 제스처를 동시에 인식하여 더욱 직관적인 제어 경험을 제공합니다. 개인의 시청 이력과 선호도를 멀티모달로 분석하여 맞춤형 콘텐츠를 추천합니다.\n",
      "    *   **스마트 홈 및 가전제품:** 냉장고 내부의 식재료 이미지를 인식하여 레시피를 추천하고, 세탁기나 에어컨의 이상 징후(소리, 진동)를 감지하여 사용자에게 알리는 등 더욱 지능적인 스마트 홈 환경을 구현합니다.\n",
      "\n",
      "2.  **반도체 및 부품 솔루션:**\n",
      "    *   **AI 칩 설계:** 멀티모달 LLM의 복잡한 연산을 효율적으로 처리할 수 있는 차세대 NPU(Neural Processing Unit) 및 저전력 고성능 AI 반도체 개발에 집중합니다.\n",
      "    *   **메모리 솔루션:** 대규모 멀티모달 모델의 학습 및 추론에 필요한 방대한 데이터를 빠르게 처리하기 위한 고대역폭 메모리(HBM), 프로세싱-인-메모리(PIM) 등 혁신적인 메모리 솔루션을 제공합니다.\n",
      "    *   **엣지 AI:** 스마트폰, 웨어러블, IoT 기기 등 엣지 디바이스에서 멀티모달 AI를 구동하기 위한 최적화된 반도체 솔루션을 개발하여 온디바이스 AI 시대를 선도합니다.\n",
      "\n",
      "3.  **산업 및 기업 솔루션:**\n",
      "    *   **제조 및 품질 관리:** 생산 라인에서 제품의 미세한 결함을 이미지 및 음향 분석을 통해 실시간으로 감지하고, 이상 징후를 예측하여 생산 효율성과 품질을 극대화합니다.\n",
      "    *   **헬스케어:** 의료 영상(X-ray, MRI, CT)과 환자의 진료 기록, 음성 데이터를 통합 분석하여 질병 진단을 보조하고, 개인 맞춤형 치료 계획 수립에 기여합니다.\n",
      "    *   **로봇 및 자율주행:** 주변 환경을 시각, 음성, 레이더 등 다양한 센서로 인지하고, 이를 바탕으로 복잡한 상황을 판단하여 자율적으로 행동하는 로봇 및 자율주행 시스템 개발에 핵심적인 역할을 합니다.\n",
      "\n",
      "---\n",
      "\n",
      "**[결론]**\n",
      "\n",
      "멀티모달 LLM은 AI가 인간의 인지 방식에 더욱 가까워지며, 현실 세계의 복잡한 문제를 해결하는 데 필수적인 기술로 자리매김하고 있습니다. 텍스트를 넘어 이미지, 음성, 비디오 등 다양한 모달리티를 통합적으로 이해하고 생성하는 능력은 AI의 활용 범위를 무한히 확장할 것입니다.\n",
      "\n",
      "물론, 대규모 데이터셋 구축, 모델의 효율적인 학습 및 추론, 윤리적 문제 해결 등 여전히 많은 도전 과제가 남아있습니다. 하지만 삼성전자는 이러한 도전 과제를 극복하고, 멀티모달 LLM 기술을 통해 사용자에게 더욱 풍부하고 직관적인 경험을 제공하며, 다양한 산업 분야의 혁신을 이끌어 나갈 것입니다.\n",
      "\n",
      "삼성전자는 AI 기술의 최전선에서 멀티모달 LLM 연구 개발에 지속적으로 투자하고 있으며, 이를 통해 인류의 삶을 더욱 풍요롭게 만드는 기술 혁신을 선도해 나갈 것을 약속드립니다. 앞으로도 삼성전자 기술 블로그를 통해 멀티모달 LLM의 발전과 그 적용 사례에 대한 깊이 있는 통찰을 계속해서 공유하겠습니다.\n",
      "\n",
      "---\n",
      "\n",
      "**[태그]**\n",
      "#멀티모달LLM #인공지능 #AI #거대언어모델 #MultimodalAI #삼성전자 #기술트렌드 #산업혁신 #온디바이스AI #비전AI #음성AI #생성AI #AI반도체 #스마트폰 #스마트TV #가전제품 #미래기술\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b><a href=\"https://cloud.langfuse.com/project/pk-lf-11135925-919c-4df5-baa1-a510de20e4c9/sessions/e2e-session-d42d363b\" target=\"_blank\">👉 Langfuse 대시보드에서 이 세션의 모든 과정을 확인하세요! 🚀</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import server_launcher\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import uuid\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# --- 1. 개별 서버들을 로컬에서 실행 ---\n",
    "print(\"--- 🚀 1. MCP 서버 로컬 실행 시작 ---\")\n",
    "_, mcp_proc = server_launcher.launch_fastapi_app(\"mcp_server\", 8501)\n",
    "if not mcp_proc:\n",
    "    raise RuntimeError(\"MCP 서버 시작 실패!\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"--- 🚀 2. A2A Gateway 서버 로컬 실행 시작 ---\")\n",
    "_, a2a_proc = server_launcher.launch_fastapi_app(\"a2a_blog_system\", 8502)\n",
    "if not a2a_proc:\n",
    "    raise RuntimeError(\"A2A Gateway 서버 시작 실패!\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 2. 리버스 프록시 서버(포트 8000)를 로컬에서 실행 ---\n",
    "print(\"--- 🚀 3. 리버스 프록시 서버 로컬 실행 시작 ---\")\n",
    "proxy_local_url, proxy_proc = server_launcher.launch_fastapi_app(\"reverse_proxy\", 8000)\n",
    "if not proxy_local_url:\n",
    "    raise RuntimeError(\"리버스 프록시 서버 시작 실패!\")\n",
    "\n",
    "\n",
    "# --- 3. 최종 Local URL 구성 및 서버 상태 확인 ---\n",
    "mcp_url = f\"{proxy_local_url}/mcp\"\n",
    "a2a_url = f\"{proxy_local_url}/a2a\"\n",
    "os.environ[\"MCP_SERVER_URL\"] = \"http://localhost:8501\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎉 모든 서버가 로컬 환경에서 준비되었습니다!\")\n",
    "print(f\"   - 리버스 프록시 Entrypoint: {proxy_local_url}\")\n",
    "print(f\"   - MCP 서버 Local URL: {mcp_url}\")\n",
    "print(f\"   - A2A Gateway Local URL: {a2a_url}\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# 서버 상태 확인\n",
    "print(\"   A2A Gateway 및 MCP 서버 초기화 대기 중...\")\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "try:\n",
    "    # 프록시를 통해 A2A 서버의 루트 경로('/')에 요청하여 최종 연결 테스트\n",
    "    test_response = requests.get(f\"{a2a_url}/\", timeout=10)\n",
    "    if test_response.status_code == 200:\n",
    "        print(\"   ✅ A2A Gateway (via Proxy) 준비 완료!\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ A2A Gateway 응답 코드: {test_response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"   ⚠️ A2A Gateway 연결 테스트 실패: {e}, 계속 진행...\")\n",
    "\n",
    "\n",
    "# --- 4. 최종 오케스트레이션 클라이언트 실행 (로컬 URL 사용) ---\n",
    "if mcp_proc and a2a_proc and proxy_proc:\n",
    "    session_id = f\"e2e-session-{uuid.uuid4().hex[:8]}\"\n",
    "    user_id = \"samsung-final-user\"\n",
    "    initial_user_input = \"멀티모달 LLM의 최신 기술 동향과 산업별 적용 사례에 대한 심층 분석 블로그 글을 써줘.\"\n",
    "    headers = {\n",
    "        \"X-API-Key\": os.environ[\"MASTER_API_KEY\"],\n",
    "        \"X-Session-ID\": session_id,\n",
    "        \"X-User-ID\": user_id,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    print(\"\\n--- 🚀 최종 오케스트레이션 시뮬레이션 시작 ---\")\n",
    "    try:\n",
    "        print(\"\\n1️⃣  LangGraph 대화 관리자 호출...\")\n",
    "        dialogue_req = {\"session_id\": session_id, \"user_input\": initial_user_input}\n",
    "\n",
    "        dialogue_response = requests.post(f\"{a2a_url}/api/v1/dialogue\", json=dialogue_req, headers=headers, timeout=120)\n",
    "        dialogue_response.raise_for_status()\n",
    "        dialogue_res = dialogue_response.json()\n",
    "\n",
    "        print(f\"   응답 수신: {dialogue_res.get('next_action', {}).get('action')}\")\n",
    "\n",
    "        next_action = dialogue_res.get(\"next_action\")\n",
    "        if next_action and next_action.get(\"action\") == \"CREATE_CONTENT\":\n",
    "            print(\"\\n2️⃣  CrewAI 콘텐츠 제작팀 호출...\")\n",
    "            # A2A Gateway가 MCP 서버와 통신할 수 있도록 환경변수를 직접 내부 주소로 설정\n",
    "            os.environ[\"MCP_SERVER_URL\"] = \"http://localhost:8501\"\n",
    "\n",
    "            creation_response = requests.post(\n",
    "                f\"{a2a_url}/api/v1/create-content\", json=next_action, headers=headers, timeout=300\n",
    "            )\n",
    "            creation_response.raise_for_status()\n",
    "            creation_res = creation_response.json()\n",
    "            draft_content = creation_res.get(\"draft_content\", \"\")\n",
    "\n",
    "            if creation_res.get(\"status\") == \"COMPLETED\" and draft_content:\n",
    "                print(\"\\n3️⃣  ADK 품질 관리팀 호출...\")\n",
    "                qc_req = {\"topic\": next_action[\"topic\"], \"draft_content\": draft_content}\n",
    "                qc_response = requests.post(\n",
    "                    f\"{a2a_url}/api/v1/quality-control\", json=qc_req, headers=headers, timeout=120\n",
    "                )\n",
    "                qc_response.raise_for_status()\n",
    "                qc_res = qc_response.json()\n",
    "                final_post = qc_res.get(\"final_post\", draft_content)\n",
    "\n",
    "                print(\"\\n\" + \"=\" * 80)\n",
    "                print(\"                           ✅ 최종 블로그 포스트 ✅\")\n",
    "                print(\"=\" * 80)\n",
    "                print(final_post)\n",
    "\n",
    "                trace_url = f\"https://cloud.langfuse.com/project/{os.environ.get('LANGFUSE_PUBLIC_KEY', 'unknown')}/sessions/{session_id}\"\n",
    "                print(\"\\n\" + \"-\" * 80)\n",
    "                display(\n",
    "                    HTML(\n",
    "                        f'<b><a href=\"{trace_url}\" target=\"_blank\">👉 Langfuse 대시보드에서 이 세션의 모든 과정을 확인하세요! 🚀</a></b>'\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                print(f\"\\n❌ 콘텐츠 생성 실패: {creation_res}\")\n",
    "        else:\n",
    "            print(f\"\\n❌ 대화 관리자에서 예상치 못한 응답: {dialogue_res}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 최종 실행 중 오류 발생: {e}\")\n",
    "else:\n",
    "    print(\"\\n❌ 서버 시작에 실패하여 최종 실행을 진행할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ws9YxoiEJsVk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 간단한 연결 테스트 먼저 실행 ===\n",
      "MCP 서버 응답: 200\n",
      "A2A 서버 응답: 200\n",
      "✅ 두 서버 모두 정상 응답! 이제 전체 시스템을 실행합니다.\n"
     ]
    }
   ],
   "source": [
    "# 간단한 연결 테스트\n",
    "print(\"\\n=== 간단한 연결 테스트 먼저 실행 ===\")\n",
    "try:\n",
    "    # MCP 서버 테스트\n",
    "    mcp_test = requests.get(f\"{mcp_url}/\", timeout=30)\n",
    "    print(f\"MCP 서버 응답: {mcp_test.status_code}\")\n",
    "\n",
    "    # A2A 서버 테스트\n",
    "    a2a_test = requests.get(f\"{a2a_url}/\", timeout=30)\n",
    "    print(f\"A2A 서버 응답: {a2a_test.status_code}\")\n",
    "\n",
    "    if mcp_test.status_code == 200 and a2a_test.status_code == 200:\n",
    "        print(\"✅ 두 서버 모두 정상 응답! 이제 전체 시스템을 실행합니다.\")\n",
    "    else:\n",
    "        print(\"❌ 서버 응답 문제 있음\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ 기본 연결 테스트 실패: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Oh_CR949zH1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
