{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "langfuse_dataset_intro_md"
   },
   "source": [
    "# Langfuse 기초 2\n",
    "\n",
    "### 실습 목표\n",
    "두 번째 실습에서는, `Langfuse`를 단순한 모니터링 도구를 넘어 Agent의 성능을 실험하고 평가하는 MLOps/LLMOps 플랫폼으로 활용하는 방법을 살펴봅니다. 프롬프트나 모델의 작은 변화가 시스템 전체에 미치는 영향을 정량적으로 측정하고, 평가의 기준점이 될 데이터셋(Dataset)을 생성해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "langfuse_dataset_setup_md"
   },
   "source": [
    "### 0. 사전 준비: 라이브러리 설치 및 API 키 설정\n",
    "이전 실습과 마찬가지로, `Langfuse`와 `LangChain` 관련 라이브러리들을 모두 설치하고, Google API 키와 Langfuse 프로젝트 키를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "langfuse_dataset_install_code"
   },
   "outputs": [],
   "source": [
    "# !pip install langfuse langchain-google-genai pydantic instructor jsonref -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "langfuse_dataset_api_keys_code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1. Google API 키 설정\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDVYEpxB86k5-Oi2BApqTr47nnGJ0BwkOc\"\n",
    "\n",
    "# 2. Langfuse API 키 설정\n",
    "# https://cloud.langfuse.com/ 에서 생성한 프로젝트의 API 키\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-c4867845-645e-4c69-a40d-14b5faf31e45\"\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-11135925-919c-4df5-baa1-a510de20e4c9\"\n",
    "# Host의 경우 클라우드의 국가를 확인하여 설정합니다.\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "langfuse_dataset_design_md"
   },
   "source": [
    "### 1. 평가 데이터셋(Benchmark) 설계\n",
    "\n",
    "Agent의 성능을 종합적으로 평가하기 위해, 질문과 답변으로 구성된 데이터셋을 설계합니다. 좋은 데이터셋은 다음과 같은 특징을 가집니다.\n",
    "- 다양성: 사실 확인, 요약, 추론, 창의성 등 다양한 능력을 요구하는 질문 포함\n",
    "- 적정 수준의 난이도: 쉽고 기본적인 질문부터, 여러 단계를 거쳐야 해결할 수 있는 복잡한 질문까지 포함\n",
    "- 명확성: `expected_output`이 Agent가 도달해야 할 명확한 목표를 제시\n",
    "\n",
    "간단하게 아래와 같이 네 문항을 만들어보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "langfuse_dataset_design_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 4개의 평가 문항 설계 완료\n"
     ]
    }
   ],
   "source": [
    "evaluation_data = [\n",
    "    {\n",
    "        \"input\": \"LangChain Expression Language (LCEL)가 무엇인지 한 문장으로 설명해줘.\",\n",
    "        \"expected_output\": \"LangChain Expression Language (LCEL)는 파이프(|) 기호를 사용하여 LangChain의 구성 요소들을 매끄럽게 연결하고, 비동기, 스트리밍, 병렬 실행과 같은 고급 기능을 쉽게 구현할 수 있도록 돕는 선언적인 방식의 프로그래밍 인터페이스입니다.\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"AI 에이전트가 내 삶의 비서가 된다면 어떤 점이 좋을지 창의적인 시나리오를 제시해줘.\",\n",
    "        \"expected_output\": \"AI 에이전트가 개인 비서가 되면, 단순히 일정을 관리하는 것을 넘어 나의 건강 상태, 재정 상황, 경력 목표를 총체적으로 분석하여 '오늘 저녁은 스트레스 해소를 위해 예산 내에서 평점이 좋은 이탈리안 레스토랑을 예약하고, 내일 아침엔 중요한 미팅 준비를 위해 관련 자료를 미리 요약해 둘게요'와 같이 능동적으로 삶의 질을 최적화하는 '인생 조력자' 역할을 할 수 있습니다.\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"'자기 성찰(Self-reflection)' 능력을 가진 AI 에이전트의 작동 원리를 설명하는 블로그 글의 서론을 작성해줘.\",\n",
    "        \"expected_output\": \"## 스스로를 돌아보는 AI, 자기 성찰 에이전트의 등장\\n\\n인공지능은 주어진 문제를 해결하는 데 탁월한 능력을 보여왔습니다. 하지만 만약 AI가 자신의 결과물을 스스로 평가하고, 부족한 점을 찾아내며, 더 나은 답을 위해 끊임없이 자신을 개선할 수 있다면 어떨까요? 최근 AI 연구의 최전선에서는 바로 이 '자기 성찰(Self-reflection)' 능력을 갖춘 AI 에이전트가 주목받고 있습니다. 이 글에서는 AI가 어떻게 스스로를 돌아보며 성장하는지, 그 놀라운 작동 원리와 미래 가능성에 대해 깊이 탐구해 보겠습니다.\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"사용자가 'CrewAI와 LangGraph 중 무엇을 써야 할까?'라고 물었을 때, 두 프레임워크의 장단점을 비교하여 어떤 상황에 무엇을 추천할지 답변해줘.\",\n",
    "        \"expected_output\": \"두 프레임워크는 각기 다른 목적을 가진 훌륭한 도구입니다. 빠르게 아이디어를 검증하고 역할 기반의 Multi-Agent 팀을 쉽게 구성하고 싶다면 `CrewAI`를 추천합니다. 반면, '결과가 A이면 B를 하고, C이면 D를 하라'와 같이 복잡한 조건부 로직이나 동적인 제어 흐름이 필요하고, 모든 과정을 완벽하게 제어하고 싶다면 `LangGraph`가 더 적합합니다. 즉, '빠른 개발'이 중요하다면 CrewAI, '높은 자유도와 정교한 제어'가 중요하다면 LangGraph를 선택하는 것이 좋습니다.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"총 {len(evaluation_data)}개의 평가 문항 설계 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "langfuse_dataset_upload_md"
   },
   "source": [
    "### 2. Langfuse에 데이터셋 생성 및 업로드\n",
    "\n",
    "이제 설계한 평가 문항들을 `Langfuse` 서버에 데이터셋으로 업로드합니다. `langfuse.create_dataset()`으로 데이터셋의 '틀'을 먼저 만들고, `langfuse.create_dataset_item()`을 사용하여 각 문항을 하나씩 추가합니다. 만약 동일한 이름의 데이터셋이 이미 존재하면, 기존 데이터셋을 그대로 사용하도록 하여 중복 생성을 방지합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "langfuse_dataset_upload_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새로운 데이터셋 'Agent_Performance_Benchmark_v1'을 생성했습니다.\n"
     ]
    }
   ],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "# Langfuse 클라이언트 초기화\n",
    "langfuse = Langfuse()\n",
    "\n",
    "DATASET_NAME = \"Agent_Performance_Benchmark_v1\"\n",
    "\n",
    "try:\n",
    "    # 1. 데이터셋 생성 (이미 존재하면 예외 발생)\n",
    "    langfuse.create_dataset(name=DATASET_NAME)\n",
    "    print(f\"새로운 데이터셋 '{DATASET_NAME}'을 생성했습니다.\")\n",
    "except Exception as e:\n",
    "    # 이미 존재하는 경우, 예외 메시지를 확인하여 정상 처리\n",
    "    if \"already exists\" in str(e):\n",
    "        print(f\"ℹ데이터셋 '{DATASET_NAME}'은(는) 이미 존재합니다. 기존 데이터셋을 사용합니다.\")\n",
    "    else:\n",
    "        raise e  # 다른 종류의 에러라면 다시 발생시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 데이터셋 아이템 업로드 시작 ---\n",
      "  - Item 'LangChain Expression Language ...' 업로드 성공\n",
      "  - Item 'AI 에이전트가 내 삶의 비서가 된다면 어떤 점이 좋을...' 업로드 성공\n",
      "  - Item ''자기 성찰(Self-reflection)' 능력을 가...' 업로드 성공\n",
      "  - Item '사용자가 'CrewAI와 LangGraph 중 무엇을 ...' 업로드 성공\n"
     ]
    }
   ],
   "source": [
    "# 2. 데이터셋 아이템 업로드\n",
    "print(\"\\n--- 데이터셋 아이템 업로드 시작 ---\")\n",
    "for item in evaluation_data:\n",
    "    try:\n",
    "        langfuse.create_dataset_item(\n",
    "            dataset_name=DATASET_NAME,  # 업로드할 데이터셋 이름\n",
    "            input=item[\"input\"],  # 평가 입력\n",
    "            expected_output=item.get(\"expected_output\"),  # 정답(있으면)\n",
    "            metadata=item.get(\"metadata\"),  # 메타데이터(선택)\n",
    "        )\n",
    "        print(f\"  - Item '{item['input'][:30]}...' 업로드 성공\")\n",
    "    except Exception as e:\n",
    "        # 아이템이 이미 존재하는 경우에 대한 처리도 추가할 수 있습니다.\n",
    "        print(f\"  - Item '{item['input'][:30]}...' 업로드 실패 또는 중복: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "langfuse_dataset_verify_md"
   },
   "source": [
    "#### Langfuse 대시보드 확인 (Action Item)\n",
    "\n",
    "1.  Langfuse Cloud로 이동하여 여러분의 프로젝트에 접속하세요.\n",
    "2.  왼쪽 메뉴에서 'Datasets'를 클릭합니다.\n",
    "3.  `Agent_Performance_Benchmark_v1`이라는 이름의 새로운 데이터셋이 생성된 것을 확인합니다.\n",
    "4.  데이터셋 이름을 클릭하여 내부로 들어가면, 우리가 방금 업로드한 4개의 평가 문항(`input`과 `expected_output`)이 각각의 아이템으로 저장된 것을 볼 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "langfuse_exp_design_md"
   },
   "source": [
    "### 4. 비교 실험 설계: 두 가지 버전의 Agent\n",
    "\n",
    "Agent의 성능을 개선하기 위한 가장 일반적인 방법 중 하나는 프롬프트 엔지니어링입니다. AI 어시스턴트라는 동일한 역할을 가진 두 Agent를 준비하되, 각각에게 다른 가이드라인(프롬프트)을 제공하여 어떤 가이드라인이 더 좋은 결과를 내는지 비교 실험합니다.\n",
    "\n",
    "- Agent V1 (Baseline): 기본적인 역할을 지시하는 간단한 시스템 프롬프트를 사용합니다.\n",
    "- Agent V2 (Improved): V1의 프롬프트에 더하여, '단계별로 생각하라(Think step-by-step)'는 구체적인 지시와, '전문적이면서도 명확한 톤'을 유지하라는 제약 조건을 추가하여, 더 깊이 있는 추론을 유도합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "langfuse_exp_agents_code"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# --- Agent V1: 기본 프롬프트 ---\n",
    "prompt_v1 = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"당신은 사용자의 질문에 대해 답변하는 AI 어시스턴트입니다.\"), (\"user\", \"{input}\")]\n",
    ")\n",
    "\n",
    "# --- Agent V2: 개선된 프롬프트 ---\n",
    "prompt_v2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"당신은 AI 기술 전문가입니다. 다음 지침에 따라 사용자의 질문에 답변해주세요:\\n1. 항상 단계별로 생각하여 논리적인 답변을 구성하세요.\\n2. 전문적인 용어를 사용하되, 누구나 이해할 수 있도록 명확하게 설명해야 합니다.\\n3. 답변은 항상 완성된 문장으로 제공하세요.\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 두 Agent가 공유할 LLM 및 체인 구조\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.2)\n",
    "chain_v1 = prompt_v1 | llm | StrOutputParser()\n",
    "chain_v2 = prompt_v2 | llm | StrOutputParser()\n",
    "\n",
    "# 비교 실험을 위한 설정 딕셔너리\n",
    "models_to_test = {\"Prompt_V1_Baseline\": chain_v1, \"Prompt_V2_Improved\": chain_v2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "langfuse_exp_judge_md"
   },
   "source": [
    "### 5. LLM-as-a-Judge\n",
    "\n",
    "이전 실습에서 구현했던 LLM-as-a-Judge를 재사용합니다. 이 평가자 Agent는 입력 질문(`input`), Agent의 실제 답변(`output`), 그리고 우리가 데이터셋에 정의한 이상적인 정답(`expected_output`)을 모두 고려하여, Agent의 답변이 얼마나 정답에 가까운지를 0.0에서 1.0 사이의 점수로 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "langfuse_exp_judge_code"
   },
   "outputs": [],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 평가자 LLM 초기화\n",
    "evaluator_llm_client = instructor.from_provider(\"google/gemini-2.5-flash\", api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "\n",
    "# 평가 결과를 담을 Pydantic 모델\n",
    "class EvaluationResult(BaseModel):\n",
    "    score: float = Field(description=\"답변이 정답과 얼마나 유사하고 정확한지에 대한 점수 (0.0 ~ 1.0)\")\n",
    "    reasoning: str = Field(\n",
    "        description=\"점수를 매긴 이유에 대한 상세한 설명. 정답과 비교하여 어떤 부분이 좋았고 어떤 부분이 부족했는지 구체적으로 서술.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM-as-a-Judge 함수\n",
    "def evaluate_agent_answer(input: str, output: str, expected_output: str) -> EvaluationResult:\n",
    "    \"\"\"LLM을 사용하여 Agent의 답변을 평가합니다.\"\"\"\n",
    "    prompt = f\"\"\"당신은 엄격하고 객관적인 AI 답변 평가자입니다.\n",
    "    다음은 사용자의 질문, AI의 답변, 그리고 이상적인 정답 예시입니다.\n",
    "    AI의 답변이 이상적인 정답과 비교했을 때 얼마나 정확하고, 완전하며, 질문의 의도를 잘 파악했는지 평가해주세요.\n",
    "    점수는 0.0(전혀 다름)부터 1.0(완벽하게 동일)까지 매겨주세요.\n",
    "\n",
    "    [사용자 질문]\n",
    "    {input}\n",
    "\n",
    "    [AI의 실제 답변]\n",
    "    {output}\n",
    "\n",
    "    [이상적인 정답 예시]\n",
    "    {expected_output}\n",
    "    \"\"\"\n",
    "\n",
    "    evaluation = evaluator_llm_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt.format(input=input, output=output, expected_output=expected_output)}\n",
    "        ],\n",
    "        response_model=EvaluationResult,\n",
    "    )\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "langfuse_exp_pipeline_md"
   },
   "source": [
    "### 6. 평가 파이프라인 실행\n",
    "\n",
    "다음 코드는 전체 평가 과정을 자동화하는 파이프라인입니다.\n",
    "\n",
    "작동 흐름:\n",
    "1.  `langfuse.get_dataset()`을 통해 Part 1에서 만든 평가 데이터셋을 불러옵니다.\n",
    "2.  우리가 테스트할 두 가지 버전의 Agent(`models_to_test`)를 순회합니다.\n",
    "3.  각 Agent에 대해, 데이터셋의 모든 항목(`item`)을 순회합니다.\n",
    "4.  `@observe()` 데코레이터를 사용하여 각 실행을 추적합니다. 데코레이터 내부에서 `update_current_trace()`를 사용하여 데이터셋 항목 정보를 현재 trace에 연결합니다.\n",
    "5.  `run()` 내부에서 Agent(`chain.invoke()`)를 실행하여 답변을 얻습니다.\n",
    "6.  `evaluate_agent_answer()`를 호출하여 자동 평가를 수행합니다.\n",
    "7.  `run.score()`를 사용하여 방금 계산된 점수를 현재 실행(Trace)에 기록합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "langfuse_exp_pipeline_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 'Agent_Performance_Benchmark_v1' 데이터셋에 대한 평가를 시작합니다. ---\n",
      "\n",
      "--- 모델 'Prompt_V1_Baseline' 평가 중... ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664cf154c78648518f861cd288bf3a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Prompt_V1_Baseline:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Item ca416bbd... 처리 완료 (Score: 1.00)\n",
      "  ✅ Item 4164580d... 처리 완료 (Score: 1.00)\n",
      "  ✅ Item 80bd6153... 처리 완료 (Score: 1.00)\n",
      "  ✅ Item 4ae496a2... 처리 완료 (Score: 0.80)\n",
      "\n",
      "--- 모델 'Prompt_V2_Improved' 평가 중... ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96c65b804d34ce38df507638e58514d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Prompt_V2_Improved:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Item ca416bbd... 처리 완료 (Score: 1.00)\n",
      "  ✅ Item 4164580d... 처리 완료 (Score: 0.95)\n",
      "  ✅ Item 80bd6153... 처리 완료 (Score: 0.95)\n",
      "  ✅ Item 4ae496a2... 처리 완료 (Score: 0.85)\n",
      "\n",
      "--- 모든 평가가 완료되었습니다. Langfuse 대시보드에서 결과를 확인하세요. ---\n"
     ]
    }
   ],
   "source": [
    "from langfuse import get_client\n",
    "from langfuse.langchain import CallbackHandler\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "langfuse = get_client()\n",
    "\n",
    "# 1. 데이터셋 가져오기\n",
    "DATASET_NAME = \"Agent_Performance_Benchmark_v1\"\n",
    "dataset = langfuse.get_dataset(name=DATASET_NAME)\n",
    "\n",
    "print(f\"--- '{DATASET_NAME}' 데이터셋에 대한 평가를 시작합니다. ---\")\n",
    "\n",
    "# 2. 각 모델(프롬프트 버전)에 대해 평가 실행\n",
    "for model_name, chain in models_to_test.items():\n",
    "    print(f\"\\n--- 모델 '{model_name}' 평가 중... ---\")\n",
    "\n",
    "    # 3. 데이터셋의 각 아이템에 대해 실행\n",
    "    for item in tqdm(dataset.items, desc=f\"Processing {model_name}\"):\n",
    "\n",
    "        # item.run() 컨텍스트 매니저 사용\n",
    "        with item.run(\n",
    "            run_name=f\"{model_name}_evaluation_run\",  # 동일한 run_name으로 그룹화\n",
    "            run_description=f\"Evaluation of {model_name} on benchmark dataset\",\n",
    "            run_metadata={\"model_name\": model_name, \"prompt_version\": model_name},\n",
    "        ) as root_span:\n",
    "\n",
    "            try:\n",
    "                # Agent 실행\n",
    "                handler = CallbackHandler()\n",
    "                output = chain.invoke({\"input\": item.input}, config={\"callbacks\": [handler]})\n",
    "\n",
    "                # LLM-as-a-Judge로 자동 평가\n",
    "                evaluation = evaluate_agent_answer(\n",
    "                    input=item.input,\n",
    "                    output=output,\n",
    "                    expected_output=item.expected_output,  # 없으면 None 처리하도록 함수 내에서 핸들\n",
    "                )\n",
    "\n",
    "                # trace 업데이트 (출력 기록)\n",
    "                root_span.update_trace(input=item.input, output=output)\n",
    "\n",
    "                # 평가 점수를 현재 trace에 기록\n",
    "                root_span.score_trace(\n",
    "                    name=\"llm-judge-quality-score\",\n",
    "                    value=float(evaluation.score),\n",
    "                    comment=str(getattr(evaluation, \"comment\", \"\")),\n",
    "                    data_type=\"NUMERIC\",\n",
    "                )\n",
    "\n",
    "                print(f\"  ✅ Item {item.id[:8]}... 처리 완료 (Score: {evaluation.score:.2f})\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  - ❌ Item {item.id[:8]}... 처리 중 오류 발생: {e}\")\n",
    "\n",
    "                # 에러 시에도 trace 업데이트\n",
    "                root_span.update_trace(input=item.input, output=f\"Error: {e}\")\n",
    "\n",
    "                root_span.score_trace(name=\"llm-judge-quality-score\", value=0, comment=f\"Execution Error: {e}\")\n",
    "\n",
    "        # 각 항목 사이에 잠시 대기 (API 레이트 리밋 방지)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "langfuse.flush()  # 모든 데이터를 서버로 즉시 전송\n",
    "print(\"\\n--- 모든 평가가 완료되었습니다. Langfuse 대시보드에서 결과를 확인하세요. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "langfuse_analysis_intro_md"
   },
   "source": [
    "### 7. 평가 결과 분석: 데이터 기반 의사결정\n",
    "\n",
    "평가 파이프라인이 `Langfuse`에 데이터를 생성했습니다. 이제 이 데이터를 분석하여, 어떤 프롬프트 버전이 더 우수한지, 그리고 어떤 부분에서 개선이 필요한지에 대한 데이터 기반의 결론을 도출할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "langfuse_analysis_ui_md"
   },
   "source": [
    "#### Langfuse 대시보드 심층 분석 (Action Item)\n",
    "\n",
    "1.  Langfuse Cloud ([https://cloud.langfuse.com/](https://cloud.langfuse.com/)) 로 이동하여 여러분의 프로젝트에 접속하세요.\n",
    "2.  왼쪽 메뉴에서 'Datasets'를 클릭하고, Part 1에서 생성한 `Agent_Performance_Benchmark_v1` 데이터셋을 선택합니다.\n",
    "3.  데이터셋 상세 페이지에서 'Runs' 탭을 클릭합니다. 이곳에서 우리는 방금 실행한 평가 결과를 종합적으로 분석할 수 있습니다.\n",
    "4.  종합 성능 비교:\n",
    "    - 화면 상단에서 각 실행(`evaluation-run-...`)에 대한 평균 점수(Average Score)를 확인합니다. `Prompt_V2_Improved` 버전의 평균 점수가 `Prompt_V1_Baseline`보다 높게 나왔는지 확인합니다.\n",
    "    - 각 실행의 'Metadata' 열을 통해 어떤 프롬프트 버전(`model_name`)에 해당하는 실행인지 명확히 구분할 수 있습니다.\n",
    "5.  개별 항목 심층 분석:\n",
    "    - 점수가 가장 낮게 나온 항목이나, V1보다 V2의 점수가 오히려 더 낮게 나온 항목을 찾아보세요. 이것이 바로 개선해야 할 '실패 케이스(Failure Case)'입니다.\n",
    "    - 해당 항목의 실행(Run)을 클릭하여 Trace 상세 페이지로 이동합니다. 여기서 LLM-as-a-Judge가 남긴 `Score`의 'Comment'를 읽어보면, 왜 낮은 점수를 주었는지에 대한 구체적인 이유를 파악할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "langfuse_playground_intro_md"
   },
   "source": [
    "### 8. 신속한 개선을 위한 Langfuse 플레이그라운드 활용\n",
    "\n",
    "실패 케이스를 분석했다면, 이제 문제를 해결할 차례입니다. `Langfuse`의 플레이그라운드(Playground) 기능은 코드를 수정하고 다시 전체 평가를 실행하는 번거로운 과정 없이, UI 상에서 직접 프롬프트를 수정하고 모델을 바꿔보며 신속하게 개선안을 테스트할 수 있는 실험 도구입니다.\n",
    "\n",
    "시나리오: '창의적인 시나리오 제시' 문항에서 V2 프롬프트가 너무 딱딱한 답변을 생성하여 낮은 점수를 받았다고 가정해 봅시다. 우리는 플레이그라운드에서 이 문제점을 해결해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "langfuse_playground_workflow_md"
   },
   "source": [
    "#### Langfuse 플레이그라운드 워크플로우 (Action Item)\n",
    "\n",
    "1.  실패 케이스 열기: 위 '개별 항목 심층 분석'에서 찾은 낮은 점수의 Trace 상세 페이지로 이동합니다.\n",
    "2.  플레이그라운드 실행: 페이지 오른쪽 상단에 있는 'Open in Playground' 버튼을 클릭합니다.\n",
    "3.  환경 복제 확인: 플레이그라운드 화면이 열리면, 왼쪽 패널에 해당 Trace에서 사용했던 모델(`gemini-2.5-pro`), 온도(`0.2`), 그리고 시스템/사용자 프롬프트가 완벽하게 복제되어 있는 것을 확인합니다.\n",
    "4.  프롬프트 수정 및 실험:\n",
    "    - 'System Prompt' 텍스트 박스에서, 너무 딱딱한 지시사항이었던 `\"당신은 AI 기술 전문가입니다...\"` 부분을 좀 더 창의성을 장려하는 문구, 예를 들어 `\"당신은 상상력이 풍부한 미래학자입니다. 사용자의 질문에 대해 가장 창의적이고 흥미로운 답변을 해주세요.\"` 와 같이 수정해 봅니다.\n",
    "5.  실행 및 비교:\n",
    "    - 하단의 'Run' 버튼을 클릭합니다.\n",
    "    - 잠시 후, 오른쪽 패널에 새로운 결과가 생성됩니다. 이 결과를 원래의 낮은 점수를 받았던 답변과 나란히 비교하며, 프롬프트 수정이 실제로 답변의 창의성을 향상시켰는지 직접 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "langfuse_human_score_md"
   },
   "source": [
    "### 9. Human Annotation\n",
    "\n",
    "LLM-as-a-Judge는 훌륭한 자동화 도구이지만, 답변의 미묘한 뉘앙스나 주관적인 창의성을 평가하는 데는 한계가 있을 수 있습니다.  \n",
    "최종적인 품질 보증을 위해서는 인간 전문가의 평가가 반드시 필요합니다. `Langfuse`는 Trace에 여러 개의 `Score`를 추가할 수 있도록 지원하므로, 자동 평가 점수와 인간 평가 점수를 함께 기록하고 관리할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "langfuse_human_score_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 인간 평가를 위한 샘플 Trace 생성 ---\n",
      "  - Agent 답변: AI 기술 전문가로서, AI 에이전트가 개인의 삶의 비서가 되었을 때 얻을 수 있는 이점을 창의적인 시나리오를 통해 단계별로 설명해 드리겠습니다.\n",
      "\n",
      "---\n",
      "\n",
      "**AI 에이전트: 당...\n",
      "  - Trace URL: https://us.cloud.langfuse.com/project/cmfeov2ig0060ad08l7gwa9go/traces/a786e81087250d6f083ead8adec8f000\n"
     ]
    }
   ],
   "source": [
    "from langfuse import get_client, observe\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# Langfuse 클라이언트 초기화\n",
    "langfuse = get_client()\n",
    "\n",
    "\n",
    "@observe(name=\"human_evaluation_sample\")\n",
    "def create_sample_for_human_eval():\n",
    "    \"\"\"인간 평가를 위한 샘플 Trace 생성\"\"\"\n",
    "\n",
    "    # Agent 실행\n",
    "    handler = CallbackHandler()\n",
    "    agent_answer = chain_v2.invoke({\"input\": evaluation_data[1][\"input\"]}, config={\"callbacks\": [handler]})\n",
    "\n",
    "    # 현재 trace에 정보 기록\n",
    "    langfuse_client = get_client()\n",
    "    langfuse_client.update_current_trace(\n",
    "        input=evaluation_data[1][\"input\"], output=agent_answer, metadata={\"purpose\": \"human_evaluation_sample\"}\n",
    "    )\n",
    "\n",
    "    # ✨ 데코레이터 내부에서 trace URL 가져오기\n",
    "    trace_url = langfuse_client.get_trace_url()\n",
    "\n",
    "    # 인간 평가자가 해당 Trace를 검토했다고 가정\n",
    "    human_score_value = 0.9\n",
    "    human_comment = \"LLM-Judge는 시나리오의 창의성을 약간 과소평가했습니다. 인간 관점에서는 매우 훌륭한 답변입니다.\"\n",
    "\n",
    "    # 현재 trace에 인간 평가 점수 추가\n",
    "\n",
    "    langfuse_client.score_current_trace(name=\"human_score\", value=human_score_value, comment=human_comment)\n",
    "\n",
    "    return agent_answer, trace_url\n",
    "\n",
    "\n",
    "print(\"--- 인간 평가를 위한 샘플 Trace 생성 ---\")\n",
    "agent_answer, trace_url = create_sample_for_human_eval()\n",
    "print(f\"  - Agent 답변: {agent_answer[:100]}...\")\n",
    "print(f\"  - Trace URL: {trace_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
