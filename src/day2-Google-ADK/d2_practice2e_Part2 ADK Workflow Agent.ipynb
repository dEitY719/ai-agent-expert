{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro_md"
   },
   "source": [
    "# [Lv4-Day2-Lab2] Part 2: ADK `Workflow Agent` - LangGraph í†µí•© ì‹¤ìŠµ\n",
    "\n",
    "### ì‹¤ìŠµ ëª©í‘œ\n",
    "Part 1ì—ì„œ í™•ì¸í•œ `LLM Agent`ì˜ Stateless í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´, ADKì˜ **Workflow Agent**ì™€ **LangGraph**ë¥¼ í†µí•©í•˜ì—¬ ë³µì¡í•œ ë‹¤ë‹¨ê³„ ì‘ì—…ì„ ì²˜ë¦¬í•˜ëŠ” Agentë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "\n",
    "1. **LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì¶•**: Self-Correcting ë¦¬ì„œì¹˜ Agentë¥¼ LangGraphë¡œ êµ¬í˜„\n",
    "2. **ADK Workflow Agent í†µí•©**: LangGraphë¥¼ ADKì˜ Toolë¡œ í†µí•©\n",
    "3. **Stateful ë™ì‘ í™•ì¸**: ì´ì „ ë‹¨ê³„ë¥¼ ê¸°ì–µí•˜ë©° ì—°ì†ì ìœ¼ë¡œ ì‘ì—… ìˆ˜í–‰\n",
    "4. **ë¹„êµ ë¶„ì„**: Stateless vs Stateful Agentì˜ ì°¨ì´ì  ëª…í™•íˆ ì´í•´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_md"
   },
   "source": [
    "## ğŸš€ 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "install_code"
   },
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "# !pip install --upgrade --quiet google-adk\n",
    "# !pip install --upgrade --quiet langchain-google-genai\n",
    "# !pip install --upgrade --quiet langchain-community\n",
    "# !pip install --upgrade --quiet langgraph\n",
    "# !pip install --upgrade --quiet tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "api_setup_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tavily API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” (https://tavily.com ë¬´ë£Œ ë°œê¸‰): Â·Â·Â·Â·Â·Â·Â·Â·\n",
      "âœ… API í‚¤ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# API í‚¤ ì„¤ì •\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    api_key = \"AIzaSyDVYEpxB86k5-Oi2BApqTr47nnGJ0BwkOc\"\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    tavily_key = getpass(\"Tavily API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” (https://tavily.com ë¬´ë£Œ ë°œê¸‰): \")\n",
    "    os.environ[\"TAVILY_API_KEY\"] = tavily_key\n",
    "\n",
    "print(\"âœ… API í‚¤ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "langgraph_md"
   },
   "source": [
    "## ğŸ—ï¸ 2. LangGraph ê¸°ë°˜ ë¦¬ì„œì¹˜ ì›Œí¬í”Œë¡œìš° êµ¬ì¶•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "langgraph_setup_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LangGraph ê¸°ë³¸ êµ¬ì„± ìš”ì†Œê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178/1925045884.py:21: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  web_search_tool = TavilySearchResults(max_results=3)\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import json\n",
    "\n",
    "\n",
    "# 1. State ì •ì˜\n",
    "class ResearchState(TypedDict):\n",
    "    topic: str\n",
    "    sub_questions: List[str]\n",
    "    current_question: str\n",
    "    researched_data: Annotated[list, operator.add]\n",
    "    final_report: str\n",
    "    step_count: int\n",
    "\n",
    "\n",
    "# 2. Tools ë° LLM ì´ˆê¸°í™”\n",
    "web_search_tool = TavilySearchResults(max_results=3)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "\n",
    "\n",
    "# 3. êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ìœ„í•œ ëª¨ë¸\n",
    "class SubQuestions(BaseModel):\n",
    "    questions: List[str] = Field(description=\"3ê°œì˜ í•µì‹¬ ì„¸ë¶€ ì§ˆë¬¸ë“¤\")\n",
    "\n",
    "\n",
    "structured_llm = llm.with_structured_output(SubQuestions)\n",
    "\n",
    "print(\"âœ… LangGraph ê¸°ë³¸ êµ¬ì„± ìš”ì†Œê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "langgraph_nodes_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤ì´ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤ ì •ì˜\n",
    "def planning_node(state: ResearchState):\n",
    "    \"\"\"ì£¼ì œ ë¶„ì„ ë° ì„¸ë¶€ ì§ˆë¬¸ ìƒì„±\"\"\"\n",
    "    print(f\"ğŸ“‹ ê³„íš ë‹¨ê³„: '{state['topic']}' ë¶„ì„ ì¤‘...\")\n",
    "\n",
    "    prompt = f\"\"\"ì£¼ì œ: {state['topic']}\n",
    "\n",
    "ì´ ì£¼ì œì— ëŒ€í•œ í¬ê´„ì ì¸ ë¦¬ì„œì¹˜ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ê¸° ìœ„í•´ í•„ìš”í•œ 3ê°œì˜ í•µì‹¬ ì„¸ë¶€ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.\n",
    "ê° ì§ˆë¬¸ì€ ì„œë¡œ ë‹¤ë¥¸ ì¸¡ë©´ì„ ë‹¤ë£¨ì–´ì•¼ í•˜ê³ , ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë‹µë³€ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤.\"\"\"\n",
    "\n",
    "    sub_questions_result = structured_llm.invoke(prompt)\n",
    "    questions = sub_questions_result.questions\n",
    "\n",
    "    print(f\"   â¤ ìƒì„±ëœ ì§ˆë¬¸ ìˆ˜: {len(questions)}\")\n",
    "    for i, q in enumerate(questions, 1):\n",
    "        print(f\"     {i}. {q}\")\n",
    "\n",
    "    return {\"sub_questions\": questions, \"step_count\": state.get(\"step_count\", 0) + 1}\n",
    "\n",
    "\n",
    "def research_node(state: ResearchState):\n",
    "    \"\"\"ì›¹ ê²€ìƒ‰ì„ í†µí•œ ê°œë³„ ì§ˆë¬¸ ì¡°ì‚¬\"\"\"\n",
    "    if not state[\"sub_questions\"]:\n",
    "        return state\n",
    "\n",
    "    current_question = state[\"sub_questions\"][0]\n",
    "    remaining_questions = state[\"sub_questions\"][1:]\n",
    "\n",
    "    print(f\"ğŸ” ì¡°ì‚¬ ë‹¨ê³„: '{current_question}' ê²€ìƒ‰ ì¤‘...\")\n",
    "\n",
    "    # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "    search_results = web_search_tool.invoke(current_question)\n",
    "\n",
    "    # ê²€ìƒ‰ ê²°ê³¼ ì •ë¦¬\n",
    "    search_summary = []\n",
    "    for result in search_results:\n",
    "        if \"content\" in result and result[\"content\"]:\n",
    "            search_summary.append(result[\"content\"][:300])  # ê° ê²°ê³¼ì˜ ì²« 300ìë§Œ\n",
    "\n",
    "    research_data = {\"question\": current_question, \"answer\": \" \".join(search_summary)}\n",
    "\n",
    "    print(f\"   â¤ ê²€ìƒ‰ ì™„ë£Œ (ê²°ê³¼ {len(search_results)}ê°œ)\")\n",
    "\n",
    "    return {\n",
    "        \"sub_questions\": remaining_questions,\n",
    "        \"current_question\": current_question,\n",
    "        \"researched_data\": [research_data],\n",
    "        \"step_count\": state.get(\"step_count\", 0) + 1,\n",
    "    }\n",
    "\n",
    "\n",
    "def synthesis_node(state: ResearchState):\n",
    "    \"\"\"ì¡°ì‚¬ ê²°ê³¼ ì¢…í•© ë° ìµœì¢… ë³´ê³ ì„œ ì‘ì„±\"\"\"\n",
    "    print(f\"ğŸ“ ì¢…í•© ë‹¨ê³„: ìµœì¢… ë³´ê³ ì„œ ì‘ì„± ì¤‘...\")\n",
    "\n",
    "    # ì¡°ì‚¬ ê²°ê³¼ ì •ë¦¬\n",
    "    research_summary = []\n",
    "    for data in state[\"researched_data\"]:\n",
    "        research_summary.append(f\"ì§ˆë¬¸: {data['question']}\\në‹µë³€: {data['answer']}\")\n",
    "\n",
    "    combined_research = \"\\n\\n\".join(research_summary)\n",
    "\n",
    "    # ìµœì¢… ë³´ê³ ì„œ ìƒì„±\n",
    "    final_prompt = f\"\"\"ì£¼ì œ: {state['topic']}\n",
    "\n",
    "ë‹¤ìŒì€ ì´ ì£¼ì œì— ëŒ€í•œ ìƒì„¸í•œ ì¡°ì‚¬ ê²°ê³¼ì…ë‹ˆë‹¤:\n",
    "\n",
    "{combined_research}\n",
    "\n",
    "ìœ„ ì¡°ì‚¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ êµ¬ì¡°ì˜ ì²´ê³„ì ì´ê³  ì „ë¬¸ì ì¸ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:\n",
    "\n",
    "1. **ê°œìš”** (ì£¼ì œì˜ ì¤‘ìš”ì„±ê³¼ í˜„í™©)\n",
    "2. **ì£¼ìš” ë‚´ìš©** (ê° ì¡°ì‚¬ ê²°ê³¼ì˜ í•µì‹¬ í¬ì¸íŠ¸)\n",
    "3. **ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸** (ì¡°ì‚¬ ê²°ê³¼ë“¤ ê°„ì˜ ì—°ê´€ì„±ê³¼ ì‹œì‚¬ì )\n",
    "4. **ê²°ë¡ ** (í•µì‹¬ ë©”ì‹œì§€ì™€ í–¥í›„ ì „ë§)\n",
    "\n",
    "ë³´ê³ ì„œëŠ” ì „ë¬¸ì ì´ê³  ì½ê¸° ì‰½ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.\"\"\"\n",
    "\n",
    "    final_report = llm.invoke(final_prompt)\n",
    "\n",
    "    print(f\"   â¤ ë³´ê³ ì„œ ì‘ì„± ì™„ë£Œ ({len(final_report.content)} ê¸€ì)\")\n",
    "\n",
    "    return {\"final_report\": final_report.content, \"step_count\": state.get(\"step_count\", 0) + 1}\n",
    "\n",
    "\n",
    "# ë¼ìš°íŒ… í•¨ìˆ˜\n",
    "def should_continue_research(state: ResearchState):\n",
    "    \"\"\"ë” ì¡°ì‚¬í•  ì§ˆë¬¸ì´ ìˆëŠ”ì§€ í™•ì¸\"\"\"\n",
    "    if state[\"sub_questions\"]:\n",
    "        return \"continue_research\"\n",
    "    else:\n",
    "        return \"synthesize\"\n",
    "\n",
    "\n",
    "print(\"âœ… LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤ì´ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "langgraph_graph_code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LangGraph ë¦¬ì„œì¹˜ ì›Œí¬í”Œë¡œìš°ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "   ğŸ“Š ë…¸ë“œ: planning â†’ research (ë°˜ë³µ) â†’ synthesis\n",
      "   ğŸ§  ë©”ëª¨ë¦¬: MemorySaverë¡œ ìƒíƒœ ìœ ì§€\n"
     ]
    }
   ],
   "source": [
    "# LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±\n",
    "def create_research_graph():\n",
    "    \"\"\"ë¦¬ì„œì¹˜ ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ìƒì„±\"\"\"\n",
    "\n",
    "    # StateGraph ìƒì„±\n",
    "    workflow = StateGraph(ResearchState)\n",
    "\n",
    "    # ë…¸ë“œ ì¶”ê°€\n",
    "    workflow.add_node(\"planning\", planning_node)\n",
    "    workflow.add_node(\"research\", research_node)\n",
    "    workflow.add_node(\"synthesis\", synthesis_node)\n",
    "\n",
    "    # ì—£ì§€ ì„¤ì •\n",
    "    workflow.set_entry_point(\"planning\")\n",
    "    workflow.add_edge(\"planning\", \"research\")\n",
    "\n",
    "    # ì¡°ê±´ë¶€ ì—£ì§€: ë” ì¡°ì‚¬í•  ì§ˆë¬¸ì´ ìˆìœ¼ë©´ research ë°˜ë³µ, ì—†ìœ¼ë©´ synthesisë¡œ\n",
    "    workflow.add_conditional_edges(\n",
    "        \"research\", should_continue_research, {\"continue_research\": \"research\", \"synthesize\": \"synthesis\"}\n",
    "    )\n",
    "\n",
    "    workflow.add_edge(\"synthesis\", END)\n",
    "\n",
    "    # ë©”ëª¨ë¦¬ì™€ í•¨ê»˜ ì»´íŒŒì¼\n",
    "    return workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "research_graph = create_research_graph()\n",
    "\n",
    "print(\"âœ… LangGraph ë¦¬ì„œì¹˜ ì›Œí¬í”Œë¡œìš°ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"   ğŸ“Š ë…¸ë“œ: planning â†’ research (ë°˜ë³µ) â†’ synthesis\")\n",
    "print(\"   ğŸ§  ë©”ëª¨ë¦¬: MemorySaverë¡œ ìƒíƒœ ìœ ì§€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_langgraph_md"
   },
   "source": [
    "## ğŸ§ª 3. LangGraph ì›Œí¬í”Œë¡œìš° ì§ì ‘ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_langgraph_code"
   },
   "outputs": [],
   "source": [
    "# LangGraph ì›Œí¬í”Œë¡œìš° ë‹¨ë… í…ŒìŠ¤íŠ¸\n",
    "def test_langgraph_workflow(topic: str):\n",
    "    \"\"\"LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ì§ì ‘ ì‹¤í–‰í•˜ì—¬ í…ŒìŠ¤íŠ¸\"\"\"\n",
    "\n",
    "    print(f\"ğŸ¯ í…ŒìŠ¤íŠ¸ ì£¼ì œ: {topic}\")\n",
    "    print(\"\" + \"=\" * 60)\n",
    "\n",
    "    # ì´ˆê¸° ìƒíƒœ ì„¤ì •\n",
    "    initial_state = {\n",
    "        \"topic\": topic,\n",
    "        \"sub_questions\": [],\n",
    "        \"current_question\": \"\",\n",
    "        \"researched_data\": [],\n",
    "        \"final_report\": \"\",\n",
    "        \"step_count\": 0,\n",
    "    }\n",
    "\n",
    "    # ì„¤ì • (thread_idë¡œ ìƒíƒœ ì¶”ì )\n",
    "    config = {\"configurable\": {\"thread_id\": \"test_research_thread\"}}\n",
    "\n",
    "    # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰\n",
    "    print(\"ğŸš€ LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œì‘...\\n\")\n",
    "\n",
    "    final_state = research_graph.invoke(initial_state, config)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"âœ… LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ“ˆ ì´ ë‹¨ê³„ ìˆ˜: {final_state.get('step_count', 0)}\")\n",
    "    print(f\"ğŸ“‹ ì¡°ì‚¬ëœ ì§ˆë¬¸ ìˆ˜: {len(final_state.get('researched_data', []))}\")\n",
    "    print(f\"ğŸ“„ ìµœì¢… ë³´ê³ ì„œ ê¸¸ì´: {len(final_state.get('final_report', ''))} ê¸€ì\")\n",
    "\n",
    "    return final_state\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "test_topic = \"2024ë…„ AI ì—ì´ì „íŠ¸ ê¸°ìˆ  íŠ¸ë Œë“œ\"\n",
    "langgraph_result = test_langgraph_workflow(test_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "langgraph_preview_code"
   },
   "outputs": [],
   "source": [
    "# LangGraph ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°\n",
    "if langgraph_result and langgraph_result.get(\"final_report\"):\n",
    "    report = langgraph_result[\"final_report\"]\n",
    "    print(\"ğŸ“‹ LangGraph ìµœì¢… ë³´ê³ ì„œ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "    print(\"\" + \"=\" * 60)\n",
    "    print(report[:800] + \"...\" if len(report) > 800 else report)\n",
    "    print(\"\" + \"=\" * 60)\n",
    "else:\n",
    "    print(\"âŒ LangGraph ì‹¤í–‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adk_integration_md"
   },
   "source": [
    "## ğŸ”§ 4. ADKì™€ LangGraph í†µí•© - Workflow Agent êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adk_workflow_code"
   },
   "outputs": [],
   "source": [
    "from google.adk.agents import Agent\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.genai import types\n",
    "import asyncio\n",
    "import uuid\n",
    "\n",
    "\n",
    "def langgraph_research_tool(topic: str) -> dict:\n",
    "    \"\"\"LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•˜ëŠ” ADK Tool\n",
    "\n",
    "    Args:\n",
    "        topic (str): ë¦¬ì„œì¹˜í•  ì£¼ì œ\n",
    "\n",
    "    Returns:\n",
    "        dict: ë‹¤ë‹¨ê³„ ë¦¬ì„œì¹˜ ê²°ê³¼\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"ğŸ”§ LangGraph Tool ì‹¤í–‰: '{topic}'\")\n",
    "\n",
    "        # ì´ˆê¸° ìƒíƒœ ì„¤ì •\n",
    "        initial_state = {\n",
    "            \"topic\": topic,\n",
    "            \"sub_questions\": [],\n",
    "            \"current_question\": \"\",\n",
    "            \"researched_data\": [],\n",
    "            \"final_report\": \"\",\n",
    "            \"step_count\": 0,\n",
    "        }\n",
    "\n",
    "        # ê³ ìœ í•œ thread_id ìƒì„±\n",
    "        thread_id = f\"adk_integration_{uuid.uuid4().hex[:8]}\"\n",
    "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "        # LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰\n",
    "        final_state = research_graph.invoke(initial_state, config)\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"topic\": topic,\n",
    "            \"steps_completed\": final_state.get(\"step_count\", 0),\n",
    "            \"questions_researched\": len(final_state.get(\"researched_data\", [])),\n",
    "            \"final_report\": final_state.get(\"final_report\", \"\"),\n",
    "            \"workflow_type\": \"LangGraph Multi-Step Research\",\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ LangGraph Tool ì˜¤ë¥˜: {str(e)}\")\n",
    "        return {\"status\": \"error\", \"error_message\": f\"LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}\"}\n",
    "\n",
    "\n",
    "# ADK Workflow Agent ìƒì„±\n",
    "workflow_agent = Agent(\n",
    "    name=\"langgraph_workflow_agent\",\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    description=\"LangGraph ê¸°ë°˜ ë‹¤ë‹¨ê³„ ë¦¬ì„œì¹˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•˜ëŠ” ê³ ê¸‰ Agentì…ë‹ˆë‹¤.\",\n",
    "    instruction=\"\"\"ë‹¹ì‹ ì€ ë³µì¡í•œ ë¦¬ì„œì¹˜ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì „ë¬¸ Agentì…ë‹ˆë‹¤.\n",
    "\n",
    "ì‚¬ìš©ìê°€ ì£¼ì œë¥¼ ì œì‹œí•˜ë©´, langgraph_research_toolì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ë‹¤ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:\n",
    "\n",
    "1. ì£¼ì œ ë¶„ì„ ë° ì„¸ë¶€ ì§ˆë¬¸ ìƒì„±\n",
    "2. ê° ì§ˆë¬¸ì— ëŒ€í•œ ì²´ê³„ì ì¸ ì›¹ ê²€ìƒ‰\n",
    "3. ê²°ê³¼ ì¢…í•© ë° ì „ë¬¸ì ì¸ ë³´ê³ ì„œ ì‘ì„±\n",
    "\n",
    "ì´ ë„êµ¬ëŠ” LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ í†µí•´ ìƒíƒœë¥¼ ìœ ì§€í•˜ë©° ë‹¨ê³„ë³„ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "ê²°ê³¼ë¥¼ ë°›ìœ¼ë©´ ì‚¬ìš©ìì—ê²Œ ìš”ì•½ê³¼ í•¨ê»˜ ìƒì„¸í•œ ë³´ê³ ì„œë¥¼ ì œê³µí•˜ì„¸ìš”.\"\"\",\n",
    "    tools=[langgraph_research_tool],\n",
    ")\n",
    "\n",
    "print(\"âœ… ADK Workflow Agent (LangGraph í†µí•©)ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(\"   ğŸ”— í†µí•©: ADK Agent + LangGraph Tool\")\n",
    "print(\"   ğŸ§  ìƒíƒœê´€ë¦¬: LangGraph MemorySaver\")\n",
    "print(\"   ğŸ”„ ì›Œí¬í”Œë¡œìš°: ê³„íš â†’ ì¡°ì‚¬(ë°˜ë³µ) â†’ ì¢…í•©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adk_test_md"
   },
   "source": [
    "## ğŸš€ 5. ADK Workflow Agent ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adk_test_code"
   },
   "outputs": [],
   "source": [
    "async def call_workflow_agent_async(agent, topic):\n",
    "    \"\"\"ADK Workflow Agent ë¹„ë™ê¸° ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
    "    session_service = InMemorySessionService()\n",
    "    session = await session_service.create_session(app_name=\"workflow_research_app\", user_id=\"workflow_user\")\n",
    "    runner = Runner(agent=agent, app_name=\"workflow_research_app\", session_service=session_service)\n",
    "\n",
    "    # ì‚¬ìš©ì ìš”ì²­ ë©”ì‹œì§€\n",
    "    prompt = f\"'{topic}' ì£¼ì œì— ëŒ€í•´ ì „ë¬¸ì ì¸ ë‹¤ë‹¨ê³„ ë¦¬ì„œì¹˜ë¥¼ ìˆ˜í–‰í•˜ê³  ìƒì„¸í•œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\"\n",
    "    content = types.Content(role=\"user\", parts=[types.Part(text=prompt)])\n",
    "\n",
    "    # Agent ì‹¤í–‰\n",
    "    events = runner.run_async(user_id=\"workflow_user\", session_id=session.id, new_message=content)\n",
    "\n",
    "    async for event in events:\n",
    "        if event.is_final_response():\n",
    "            return event.content.parts[0].text\n",
    "    return \"ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "\n",
    "# ADK Workflow Agent ì‹¤í–‰\n",
    "workflow_topic = \"2024ë…„ ìµœì‹  ë©€í‹°ëª¨ë‹¬ AI ê¸°ìˆ  ë™í–¥\"\n",
    "print(f\"ğŸ¯ Workflow Agent ì£¼ì œ: {workflow_topic}\")\n",
    "print(\"ğŸ¤– ADK Workflow Agent (LangGraph í†µí•©) ì‹¤í–‰ ì¤‘...\")\n",
    "print(\"   (ê³„íš â†’ ì¡°ì‚¬ â†’ ì¢…í•© ê³¼ì •ì„ ê±°ì³ 2-3ë¶„ ì†Œìš”ë©ë‹ˆë‹¤)\\n\")\n",
    "\n",
    "workflow_result = await call_workflow_agent_async(workflow_agent, workflow_topic)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"          âœ… ADK Workflow Agent (LangGraph í†µí•©) ì™„ë£Œ! âœ…\")\n",
    "print(\"=\" * 80)\n",
    "print(workflow_result)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adk_part2_conclusion_md"
   },
   "source": [
    "### 2.5. ê²°ë¡ : LLM Agent vs. Workflow Agent\n",
    "\n",
    "ì´ë²ˆ ì‹¤ìŠµì„ í†µí•´ ìš°ë¦¬ëŠ” ADKê°€ ì œê³µí•˜ëŠ” ë‘ ê°€ì§€ í•µì‹¬ Agent ìœ í˜•ì„ ëª¨ë‘ ê²½í—˜í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "| êµ¬ë¶„ | `LLM Agent` (Part 1) | `Workflow Agent` (Part 2) |\n",
    "| :--- | :--- | :--- |\n",
    "| **ìƒíƒœ ê´€ë¦¬** | **Stateless** (ê¸°ì–µ ì—†ìŒ) | **Stateful** (ê¸°ì–µ ìˆìŒ) |\n",
    "| **ì›Œí¬í”Œë¡œìš°** | **ì •ì  (Static)**: ë‹¨ì¼ LLM í˜¸ì¶œ | **ë™ì  (Dynamic)**: LangGraph ê¸°ë°˜ì˜ ë³µì¡í•œ íë¦„ ì œì–´ |\n",
    "| **ì£¼ìš” ìš©ë„** | ê°„ë‹¨í•œ ë‹¨ë°œì„± ì‘ì—… (ê³„ì‚°, ë‹¨ìˆœ API í˜¸ì¶œ) | ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ê±°ì¹˜ëŠ” ë³µì¡í•œ ì‘ì—… (ë¦¬ì„œì¹˜, ë°ì´í„° ë¶„ì„, ì¸ê°„ ê°œì…) |\n",
    "| **í•µì‹¬** | ë¹ ë¥´ê³  ê°„ë‹¨í•œ Tool ì‚¬ìš© | ìœ ì—°í•˜ê³  ê°•ë ¥í•œ í”„ë¡œì„¸ìŠ¤ ìë™í™” |\n",
    "\n",
    "**ìµœì¢… êµí›ˆ:** ADKë¥¼ ì‚¬ìš©í•˜ë©´, ì‘ì—…ì˜ ë³µì¡ì„±ì— ë§ì¶° ê°€ì¥ ì í•©í•œ Agent ìœ í˜•ì„ ì„ íƒí•˜ì—¬ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°„ë‹¨í•œ ì‘ì—…ì€ `LLM Agent`ë¡œ ë¹ ë¥´ê²Œ, ë³µì¡í•˜ê³  ìƒíƒœ ê´€ë¦¬ê°€ í•„ìš”í•œ ì‘ì—…ì€ `Workflow Agent`ì™€ `LangGraph`ë¥¼ í†µí•´ ì •êµí•˜ê³  ì•ˆì •ì ìœ¼ë¡œ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
