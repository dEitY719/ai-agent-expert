{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_intro_md"
   },
   "source": [
    "# [Final Project Demo] ë”¥ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸\n",
    "\n",
    "### ì‹¤ìŠµ ëª©í‘œ\n",
    "\n",
    "1. `LangGraph`, `CrewAI`, `ADK`ì˜ ê°œë…ì„ í•˜ë‚˜ì˜ ì›Œí¬í”Œë¡œìš° ì•ˆì—ì„œ ì—®ìŠµë‹ˆë‹¤.\n",
    "2. `FastAPI` ê¸°ë°˜ì˜ MCP(ìì› í—ˆë¸Œ) ì„œë²„ì™€ A2A(Agent-to-Agent) ê²Œì´íŠ¸ì›¨ì´ë¥¼ í†µí•´, í™•ì¥ ê°€ëŠ¥í•˜ê³  ìœ ì§€ë³´ìˆ˜ê°€ ìš©ì´í•œ Agent ì•„í‚¤í…ì²˜ë¥¼ ì„¤ê³„í•˜ê³  ìš´ì˜í•©ë‹ˆë‹¤.\n",
    "3. `Redis`ì™€ `ChromaDB`ë¥¼ í™œìš©í•œ `MemorySystem`ì„, ê³¼ê±°ì˜ ê²½í—˜ê³¼ ì‚¬ìš©ì í”¼ë“œë°±ìœ¼ë¡œë¶€í„° í•™ìŠµí•˜ê³  ì§„í™”í•˜ëŠ” Agentë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "4. `Langfuse`ë¥¼ ì‹œìŠ¤í…œ ì „ì²´ì— í†µí•©í•˜ì—¬, ì›Œí¬í”Œë¡œìš°ë¥¼ ì¶”ì í•˜ê³ , `Langfuse Dataset`ê³¼ `LLM-as-a-Judge`ë¥¼ í†µí•´ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ê°œì„ í•˜ëŠ” LLMOps/EvalOps ì‚¬ì´í´ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part0_setup_md"
   },
   "source": [
    "### 0. ì¤€ë¹„\n",
    "\n",
    "1ì¼ë¶€í„° 5ì¼ê¹Œì§€ ì‚¬ìš©í–ˆë˜ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê³ , Langfuseë¥¼ í¬í•¨í•œ ëª¨ë“  ì™¸ë¶€ ì„œë¹„ìŠ¤ì˜ API í‚¤ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "day6_install_code"
   },
   "outputs": [],
   "source": [
    "# !pip install fastapi uvicorn python-dotenv nest-asyncio aiohttp httpx newsapi-python arxiv tavily-python slowapi langfuse crewai crewai-tools google-adk langchain-google-genai pydantic instructor requests jsonref starlette redis chromadb sentence-transformers wolframalpha langgraph langchain -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "day6_api_keys_code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# LLM ë° ê¸°ë³¸ Tool í‚¤\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "if \"NEWS_API_KEY\" not in os.environ:\n",
    "    os.environ[\"NEWS_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "if \"WOLFRAM_ALPHA_APP_ID\" not in os.environ:\n",
    "    os.environ[\"WOLFRAM_ALPHA_APP_ID\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "# Langfuse Observability/Evaluation Platform í‚¤\n",
    "if \"LANGFUSE_PUBLIC_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "if \"LANGFUSE_SECRET_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGFUSE_SECRET_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\"\n",
    "\n",
    "# ì„œë²„ ê°„ í†µì‹ ì„ ìœ„í•œ ë§ˆìŠ¤í„° ì¸ì¦ í‚¤\n",
    "os.environ[\"MASTER_API_KEY\"] = \"samsung-llm-agent-lv4-master-key\"\n",
    "\n",
    "# CrewAI í˜¸í™˜ì„±ì„ ìœ„í•œ GEMINI_API_KEY ì„¤ì •\n",
    "if \"GEMINI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GEMINI_API_KEY\"] = os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "# --- ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—°ê²° ì •ë³´ ì„¤ì • ---\n",
    "if \"UPSTASH_REDIS_REST_URL\" not in os.environ:\n",
    "    os.environ[\"UPSTASH_REDIS_REST_URL\"] = \"YOUR_API_KEY\"\n",
    "if \"UPSTASH_REDIS_REST_TOKEN\" not in os.environ:\n",
    "    os.environ[\"UPSTASH_REDIS_REST_TOKEN\"] = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part1_intro_md"
   },
   "source": [
    "### 1. ìŠ¤í¬ë¦½íŠ¸ ìƒì„±\n",
    "\n",
    "ì•ì„  5ì¼ê°„ ì„¤ê³„í•˜ê³  êµ¬í˜„í–ˆë˜ ëª¨ë“  êµ¬ì„± ìš”ì†Œë“¤ì„ ë…ë¦½ì ì¸ Python íŒŒì¼ë¡œ ë‹¤ì‹œ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "  \n",
    "ê°€ì¥ ë¨¼ì €, ëª¨ë“  Agentë“¤ì˜ MCP ì„œë²„ì™€ ê·¸ ëŠ¥ë ¥ì´ ë  Toolë“¤ì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part1_mcp_tools_md"
   },
   "source": [
    "#### 1.1 Tool êµ¬í˜„ (`server_tools.py`)\n",
    "\n",
    "ê¸°ì¡´ì˜ ê²€ìƒ‰ Tool (`web_search`, `news_api_search`, `arxiv_search`)ì— ë”í•´, ì´ë²ˆ ìµœì¢… í”„ë¡œì íŠ¸ì˜ ë³µì¡í•œ ì£¼ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ 3ê°œì˜ ìƒˆë¡œìš´ ì „ë¬¸ê°€ Toolì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `python_calculator`: ì‹œì¥ ì„±ì¥ë¥  ì˜ˆì¸¡ì´ë‚˜ PPD(Pixels Per Degree) ê°™ì€ ì •ëŸ‰ì  ë°ì´í„°ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ Toolì…ë‹ˆë‹¤.\n",
    "- `ethics_check`: Embodied AIì˜ í”„ë¼ì´ë²„ì‹œ ë¬¸ì œì™€ ê°™ì€ ë¯¼ê°í•œ ì£¼ì œë¥¼ ë‹¤ë£° ë•Œ, ê¸€ì˜ ë‚´ìš©ì´ ìœ¤ë¦¬ì ìœ¼ë¡œ í¸í–¥ë˜ì§€ ì•Šì•˜ëŠ”ì§€ ê²€í† í•˜ëŠ” LLM ê¸°ë°˜ í‰ê°€ Toolì…ë‹ˆë‹¤.\n",
    "- `mermaid_generator`: ë³µì¡í•œ ê¸°ìˆ  ìŠ¤íƒì´ë‚˜ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ë¥¼ ë…ìê°€ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡, í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ Mermaid ë‹¤ì´ì–´ê·¸ë¨ ì½”ë“œë¥¼ ìƒì„±í•˜ëŠ” LLM ê¸°ë°˜ ì‹œê°í™” Toolì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "day6_write_server_tools_code"
   },
   "outputs": [],
   "source": [
    "%%writefile server_tools.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "from tavily import TavilyClient\n",
    "from newsapi import NewsApiClient\n",
    "import arxiv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# --- Tool ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•  LLM ì´ˆê¸°í™” ---\n",
    "tool_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0.0)\n",
    "\n",
    "# --- 1. ê¸°ì¡´ ê²€ìƒ‰ Toolë“¤ ---\n",
    "async def web_search(query: str) -> str:\n",
    "    \"\"\"ì›¹ì—ì„œ ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        client = TavilyClient(api_key=os.environ['TAVILY_API_KEY'])\n",
    "        response = client.search(query=query, max_results=5, search_depth=\"advanced\")\n",
    "        results = [{'title': obj['title'], 'url': obj['url'], 'content': obj['content'][:500]} for obj in response['results']]\n",
    "        return json.dumps(results, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"Tavily API ì˜¤ë¥˜: {e}\"}, ensure_ascii=False)\n",
    "\n",
    "async def news_api_search(query: str) -> str:\n",
    "    \"\"\"ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        client = NewsApiClient(api_key=os.environ['NEWS_API_KEY'])\n",
    "        response = client.get_everything(q=query, language='ko', sort_by='relevancy', page_size=3)\n",
    "        if response['status'] == 'ok':\n",
    "            articles = [{'title': article['title'], 'url': article['url'], 'description': article['description']} for article in response['articles']]\n",
    "            return json.dumps(articles, ensure_ascii=False)\n",
    "        else:\n",
    "            return json.dumps({\"error\": f\"News API ì˜¤ë¥˜: {response.get('message', 'Unknown error')}\"}, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"News API í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜: {e}\"}, ensure_ascii=False)\n",
    "\n",
    "async def arxiv_search(query: str) -> str:\n",
    "    \"\"\"í•™ìˆ  ë…¼ë¬¸ì„ Arxivì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        client = arxiv.Client()\n",
    "        search = arxiv.Search(query=query, max_results=2, sort_by=arxiv.SortCriterion.Relevance)\n",
    "        results = list(client.results(search))\n",
    "        papers = [{'title': result.title, 'authors': [str(a) for a in result.authors], 'summary': result.summary, 'pdf_url': result.pdf_url} for result in results]\n",
    "        return json.dumps(papers, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"Arxiv ê²€ìƒ‰ ì˜¤ë¥˜: {e}\"}, ensure_ascii=False)\n",
    "\n",
    "# --- 2. ì‹ ê·œ Toolë“¤ ---\n",
    "\n",
    "async def python_calculator(code: str) -> str:\n",
    "    \"\"\"ì£¼ì–´ì§„ Python ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ìˆ˜í•™ì /ê³µí•™ì  ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ê°„ë‹¨í•œ ê³„ì‚°ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "\n",
    "    try:\n",
    "        result = eval(code, {\"__builtins__\": {}}, {})\n",
    "        return json.dumps({\"result\": str(result)})\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"ì½”ë“œ ì‹¤í–‰ ì˜¤ë¥˜: {e}\"}, ensure_ascii=False)\n",
    "\n",
    "async def ethics_check(text: str) -> str:\n",
    "    \"\"\"ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì— ì ì¬ì ì¸ ìœ¤ë¦¬ì  ë¬¸ì œê°€ ìˆëŠ”ì§€(í¸í–¥, í”„ë¼ì´ë²„ì‹œ ì¹¨í•´ ë“±) ê²€í† í•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"ë‹¹ì‹ ì€ AI ìœ¤ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì ì¬ì ì¸ ìœ¤ë¦¬ì  ë¬¸ì œ(ì˜ˆ: í¸í–¥, ê°œì¸ì •ë³´ ì¹¨í•´, ì‚¬íšŒì  ê°ˆë“± ì¡°ì¥ ë“±)ê°€ ìˆëŠ”ì§€ ê²€í† í•˜ê³ , 'ë¬¸ì œ ì—†ìŒ' ë˜ëŠ” 'ì£¼ì˜ í•„ìš”' íŒì •ê³¼ í•¨ê»˜ ê·¸ ê·¼ê±°ë¥¼ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\\n\\n[ê²€í† í•  í…ìŠ¤íŠ¸]:\\n{text}\"\"\"\n",
    "        response = tool_llm.invoke(prompt).content\n",
    "        return json.dumps({\"ethics_review\": response}, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"ìœ¤ë¦¬ ê²€í†  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\"}, ensure_ascii=False)\n",
    "\n",
    "async def mermaid_generator(description: str) -> str:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ Mermaid.js ë‹¤ì´ì–´ê·¸ë¨ ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"ë‹¹ì‹ ì€ Mermaid.js ë‹¤ì´ì–´ê·¸ë¨ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì„¤ëª…ì„ ë³´ê³ , ì´ë¥¼ ì‹œê°í™”í•˜ëŠ” Mermaid ì½”ë“œ ë¸”ë¡(```mermaid ... ```)ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ ë¶™ì´ì§€ ë§ê³  ì˜¤ì§ ì½”ë“œ ë¸”ë¡ë§Œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.\\n\\n[ì„¤ëª…]:\\n{description}\"\"\"\n",
    "        response = tool_llm.invoke(prompt).content\n",
    "        # LLM ì‘ë‹µì—ì„œ ì½”ë“œ ë¸”ë¡ë§Œ ì •í™•íˆ ì¶”ì¶œ\n",
    "        import re\n",
    "        code_match = re.search(r\"```mermaid\\n(.*?)```\", response, re.DOTALL)\n",
    "        if code_match:\n",
    "            return json.dumps({\"mermaid_code\": code_match.group(1).strip()}, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"Mermaid ì½”ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\"}, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part1_mcp_resources_md"
   },
   "source": [
    "#### 1.2 MCP ì„œë²„ì˜ ì§€ì‹: ì •ì  ë¦¬ì†ŒìŠ¤ êµ¬í˜„ (`server_resources.py`)\n",
    "\n",
    "Agentë“¤ì´ ê³µí†µì ìœ¼ë¡œ ì°¸ê³ í•´ì•¼ í•  ì •ì ì¸ ë°ì´í„°(ë¸”ë¡œê·¸ í…œí”Œë¦¿, ìŠ¤íƒ€ì¼ ê°€ì´ë“œ ë“±)ë¥¼ ì œê³µí•˜ëŠ” ë¦¬ì†ŒìŠ¤ ëª¨ë“ˆì„ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "day6_write_server_resources_code"
   },
   "outputs": [],
   "source": [
    "%%writefile server_resources.py\n",
    "\n",
    "BLOG_TEMPLATES = {\n",
    "    \"deep_dive_tech_analysis\": \"## 1. ì„œë¡ : ê¸°ìˆ ì˜ ë“±ì¥ ë°°ê²½ ë° ì¤‘ìš”ì„±\\n\\n## 2. í•µì‹¬ ì•„í‚¤í…ì²˜ ë° ì‘ë™ ì›ë¦¬\\n   - ### 2.1. í•˜ë“œì›¨ì–´ êµ¬ì„± ìš”ì†Œ\\n   - ### 2.2. ì†Œí”„íŠ¸ì›¨ì–´ ìŠ¤íƒ\\n\\n## 3. ì£¼ìš” ê¸°ìˆ ì  ê³¼ì œ ë° í•´ê²° ë°©ì•ˆ\\n\\n## 4. ì‚°ì—…ë³„ íŒŒê¸‰ íš¨ê³¼ ë° ì‹œì¥ ì „ë§\\n\\n## 5. ì‚¬íšŒ/ìœ¤ë¦¬ì  ê³ ì°°\\n\\n## 6. ê²°ë¡ : ë¯¸ë˜ ì „ë§ ë° ì œì–¸\",\n",
    "    \"market_trend_report\": \"## 1. ì‹œì¥ ê°œìš” ë° í•µì‹¬ íŠ¸ë Œë“œ\\n\\n## 2. ì£¼ìš” í”Œë ˆì´ì–´ ë° ì „ëµ ë¶„ì„\\n   - ### 2.1. ì„ ë‘ ê¸°ì—… (Apple, Meta ë“±)\\n   - ### 2.2. ë„ì „ ê¸°ì—… (ìŠ¤íƒ€íŠ¸ì—… ë“±)\\n\\n## 3. ì •ëŸ‰ì  ì‹œì¥ ë°ì´í„° ë¶„ì„\\n\\n## 4. í–¥í›„ 5ë…„ê°„ ì‹œì¥ ì˜ˆì¸¡\\n\\n## 5. ê²°ë¡  ë° íˆ¬ì ì¸ì‚¬ì´íŠ¸\"\n",
    "}\n",
    "\n",
    "STYLE_GUIDES = {\n",
    "    \"samsung_technical_blog\": \"ë¬¸ì²´: ì „ë¬¸ì ì´ë©´ì„œë„ ìµœëŒ€í•œ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ.\\nëŒ€ìƒ ë…ì: ê¸°ìˆ ì— ëŒ€í•œ ê¹Šì€ ì´í•´ë¥¼ ê°€ì§„ ê°œë°œì ë° ì—°êµ¬ì›.\\nì–´ì¡°: ê°ê´€ì ì´ê³ , ë°ì´í„°ì™€ ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ë©°, ì‹ ë¢°ê°ì„ ì£¼ëŠ” í†¤ì•¤ë§¤ë„ˆë¥¼ ìœ ì§€.\\nì°¸ê³ : ëª¨ë“  ê¸°ìˆ  ìš©ì–´ëŠ” ì²« ë“±ì¥ ì‹œ ì˜ë¬¸ í‘œê¸°ë¥¼ ë³‘ê¸°.\",\n",
    "    \"general_public_tech_column\": \"ë¬¸ì²´: ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ, ë¹„ìœ ì™€ ì˜ˆì‹œë¥¼ ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©.\\nëŒ€ìƒ ë…ì: ê¸°ìˆ ì— ê´€ì‹¬ì€ ë§ì§€ë§Œ ì „ë¬¸ê°€ëŠ” ì•„ë‹Œ ì¼ë°˜ ëŒ€ì¤‘.\\nì–´ì¡°: í¥ë¯¸ë¥¼ ìœ ë°œí•˜ê³ , ê¸°ìˆ ì´ ìš°ë¦¬ ì‚¶ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ .\"\n",
    "}\n",
    "\n",
    "async def get_template(template_id: str):\n",
    "    return BLOG_TEMPLATES.get(template_id, {\"error\": \"Template not found\"})\n",
    "\n",
    "async def get_style_guide(guide_id: str):\n",
    "    return STYLE_GUIDES.get(guide_id, {\"error\": \"Style guide not found\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part1_mcp_server_md"
   },
   "source": [
    "#### 1.3 MCP ì„œë²„ ë³¸ì²´ êµ¬í˜„ (`mcp_server.py`)\n",
    "\n",
    "ì´ì œ Toolê³¼ ë¦¬ì†ŒìŠ¤ë¥¼ API ì—”ë“œí¬ì¸íŠ¸ë¡œ ë…¸ì¶œí•˜ê³ , ì¸ì¦(Authentication), ìš”ì²­ ìˆ˜ ì œí•œ(Rate Limiting), ê´€ì¸¡ ê°€ëŠ¥ì„±(Observability) ê¸°ëŠ¥ì„ ê°–ì¶˜ MCP ì„œë²„ì˜ ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "day6_write_mcp_server_code"
   },
   "outputs": [],
   "source": [
    "%%writefile mcp_server.py\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, Depends, Security, Request\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "from fastapi.security import APIKeyHeader\n",
    "from slowapi import Limiter, _rate_limit_exceeded_handler\n",
    "from slowapi.util import get_remote_address\n",
    "from slowapi.errors import RateLimitExceeded\n",
    "from langfuse import observe\n",
    "\n",
    "# ëª¨ë“ˆí™”ëœ Toolê³¼ ë¦¬ì†ŒìŠ¤ í•¨ìˆ˜ë“¤ì„ ì„í¬íŠ¸\n",
    "from server_tools import (\n",
    "    web_search, news_api_search, arxiv_search,\n",
    "    python_calculator, ethics_check, mermaid_generator\n",
    ")\n",
    "from server_resources import get_template, get_style_guide\n",
    "\n",
    "# FastAPI ì•± ë° ì•ˆì •ì„±/ë³´ì•ˆ ê¸°ëŠ¥ ì´ˆê¸°í™”\n",
    "app = FastAPI(title=\"MCP Server for Sentient Architect\", version=\"2.0.0\")\n",
    "limiter = Limiter(key_func=get_remote_address)\n",
    "app.state.limiter = limiter\n",
    "app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n",
    "\n",
    "API_KEY_NAME = \"X-API-Key\"\n",
    "api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)\n",
    "\n",
    "async def get_api_key(api_key: str = Security(api_key_header)):\n",
    "    if api_key == os.environ.get(\"MASTER_API_KEY\"):\n",
    "        return api_key\n",
    "    else:\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid or missing API Key\")\n",
    "\n",
    "# Pydantic ëª¨ë¸ ì •ì˜\n",
    "class ToolCallRequest(BaseModel):\n",
    "    query: str = Field(..., description=\"Toolì— ì „ë‹¬í•  ê²€ìƒ‰ì–´ ë˜ëŠ” ì…ë ¥ê°’\")\n",
    "\n",
    "class CodeExecutionRequest(BaseModel):\n",
    "    code: str = Field(..., description=\"ì‹¤í–‰í•  Python ì½”ë“œ\")\n",
    "\n",
    "class TextAnalysisRequest(BaseModel):\n",
    "    text: str = Field(..., description=\"ë¶„ì„í•  í…ìŠ¤íŠ¸\")\n",
    "\n",
    "class DiagramGenerationRequest(BaseModel):\n",
    "    description: str = Field(..., description=\"ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ ë³€í™˜í•  í…ìŠ¤íŠ¸ ì„¤ëª…\")\n",
    "\n",
    "# --- API ì—”ë“œí¬ì¸íŠ¸ ---\n",
    "\n",
    "@app.get(\"/\", summary=\"ì„œë²„ ìƒíƒœ í™•ì¸\", tags=[\"Status\"])\n",
    "async def read_root():\n",
    "    return {\"status\": \"ok\", \"message\": \"MCP Server is running and ready to serve.\"}\n",
    "\n",
    "# ê²€ìƒ‰ Tool ì—”ë“œí¬ì¸íŠ¸\n",
    "@app.post(\"/api/v1/tools/web_search\", summary=\"ì›¹ ê²€ìƒ‰\", tags=[\"Search Tools\"])\n",
    "@limiter.limit(\"30/minute\")\n",
    "@observe(name=\"mcp-tool-web-search\")\n",
    "async def prod_web_search(request: Request, data: ToolCallRequest, api_key: str = Depends(get_api_key)):\n",
    "    result = await web_search(data.query)\n",
    "    return {\"tool\": \"web_search\", \"query\": data.query, \"result\": result}\n",
    "\n",
    "@app.post(\"/api/v1/tools/news_api_search\", summary=\"ë‰´ìŠ¤ ê²€ìƒ‰\", tags=[\"Search Tools\"])\n",
    "@limiter.limit(\"30/minute\")\n",
    "@observe(name=\"mcp-tool-news-api-search\")\n",
    "async def prod_news_search(request: Request, data: ToolCallRequest, api_key: str = Depends(get_api_key)):\n",
    "    result = await news_api_search(data.query)\n",
    "    return {\"tool\": \"news_api_search\", \"query\": data.query, \"result\": result}\n",
    "\n",
    "@app.post(\"/api/v1/tools/arxiv_search\", summary=\"ë…¼ë¬¸ ê²€ìƒ‰\", tags=[\"Search Tools\"])\n",
    "@limiter.limit(\"30/minute\")\n",
    "@observe(name=\"mcp-tool-arxiv-search\")\n",
    "async def prod_arxiv_search(request: Request, data: ToolCallRequest, api_key: str = Depends(get_api_key)):\n",
    "    result = await arxiv_search(data.query)\n",
    "    return {\"tool\": \"arxiv_search\", \"query\": data.query, \"result\": result}\n",
    "\n",
    "# ì‹ ê·œ ì „ë¬¸ê°€ Tool ì—”ë“œí¬ì¸íŠ¸\n",
    "@app.post(\"/api/v1/tools/python_calculator\", summary=\"Python ì½”ë“œ ê³„ì‚°\", tags=[\"Expert Tools\"])\n",
    "@limiter.limit(\"60/minute\")\n",
    "@observe(name=\"mcp-tool-python-calculator\")\n",
    "async def prod_python_calculator(request: Request, data: CodeExecutionRequest, api_key: str = Depends(get_api_key)):\n",
    "    result = await python_calculator(data.code)\n",
    "    return {\"tool\": \"python_calculator\", \"code\": data.code, \"result\": result}\n",
    "\n",
    "@app.post(\"/api/v1/tools/ethics_check\", summary=\"ìœ¤ë¦¬ì„± ê²€í† \", tags=[\"Expert Tools\"])\n",
    "@limiter.limit(\"60/minute\")\n",
    "@observe(name=\"mcp-tool-ethics-check\")\n",
    "async def prod_ethics_check(request: Request, data: TextAnalysisRequest, api_key: str = Depends(get_api_key)):\n",
    "    result = await ethics_check(data.text)\n",
    "    return {\"tool\": \"ethics_check\", \"text_length\": len(data.text), \"result\": result}\n",
    "\n",
    "@app.post(\"/api/v1/tools/mermaid_generator\", summary=\"Mermaid ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±\", tags=[\"Expert Tools\"])\n",
    "@limiter.limit(\"30/minute\")\n",
    "@observe(name=\"mcp-tool-mermaid-generator\")\n",
    "async def prod_mermaid_generator(request: Request, data: DiagramGenerationRequest, api_key: str = Depends(get_api_key)):\n",
    "    result = await mermaid_generator(data.description)\n",
    "    return {\"tool\": \"mermaid_generator\", \"description\": data.description, \"result\": result}\n",
    "\n",
    "# ì •ì  ë¦¬ì†ŒìŠ¤ ì—”ë“œí¬ì¸íŠ¸\n",
    "@app.get(\"/api/v1/resources/templates/{template_id}\", summary=\"í…œí”Œë¦¿ ì¡°íšŒ\", tags=[\"Static Resources\"])\n",
    "@limiter.limit(\"100/minute\")\n",
    "@observe(name=\"mcp-resource-template\")\n",
    "async def prod_read_template(request: Request, template_id: str, api_key: str = Depends(get_api_key)):\n",
    "    content = await get_template(template_id)\n",
    "    if isinstance(content, dict) and \"error\" in content:\n",
    "        raise HTTPException(status_code=404, detail=content[\"error\"])\n",
    "    return {\"resource\": \"template\", \"id\": template_id, \"content\": content}\n",
    "\n",
    "@app.get(\"/api/v1/resources/style_guides/{guide_id}\", summary=\"ìŠ¤íƒ€ì¼ ê°€ì´ë“œ ì¡°íšŒ\", tags=[\"Static Resources\"])\n",
    "@limiter.limit(\"100/minute\")\n",
    "@observe(name=\"mcp-resource-style-guide\")\n",
    "async def prod_read_style_guide(request: Request, guide_id: str, api_key: str = Depends(get_api_key)):\n",
    "    content = await get_style_guide(guide_id)\n",
    "    if isinstance(content, dict) and \"error\" in content:\n",
    "        raise HTTPException(status_code=404, detail=content[\"error\"])\n",
    "    return {\"resource\": \"style_guide\", \"id\": guide_id, \"content\": content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part1_a2a_intro_md"
   },
   "source": [
    "#### 1.4 ë©€í‹° ì—ì´ì „íŠ¸ í†µì‹  í”„ë¡œí† ì½œ êµ¬í˜„(`a2a_protocol.py`)\n",
    "\n",
    "ì„œë¡œ ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬ë¡œ ë§Œë“¤ì–´ì§„ Agentë“¤ì´ ì†Œí†µí•˜ê¸° ìœ„í•œ í‘œì¤€ ë°ì´í„° í˜•ì‹ì„ ì •ì˜í•©ë‹ˆë‹¤. ì´ í”„ë¡œí† ì½œì€ ì‹œìŠ¤í…œì˜ ì•ˆì •ì„±ê³¼ í™•ì¥ì„±ì„ ë³´ì¥í•˜ëŠ” API ëª…ì„¸(Contract) ì—­í• ì„ í•©ë‹ˆë‹¤.  \n",
    "\n",
    "ë‹¤ë§Œ A2Aë¥¼ ì‹¤ìŠµ í™˜ê²½ ìƒ êµ¬í˜„í•˜ê¸° í˜ë“¤ê¸°ì—, ì´ì™€ ìœ ì‚¬í•œ FastAPI ê¸°ë°˜ í˜¸ì¶œì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "day6_write_a2a_protocol_code"
   },
   "outputs": [],
   "source": [
    "%%writefile a2a_protocol.py\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "# --- 1. ëŒ€í™” ê´€ë¦¬ì (Orchestrator)ì˜ ì…ì¶œë ¥ ---\n",
    "class OrchestrationRequest(BaseModel):\n",
    "    session_id: str = Field(description=\"ê° ëŒ€í™” ì„¸ì…˜ì„ ì‹ë³„í•˜ëŠ” ê³ ìœ  ID\")\n",
    "    user_input: str = Field(description=\"ì‚¬ìš©ìì˜ ìµœì´ˆ ë¦¬ì„œì¹˜ ìš”ì²­\")\n",
    "\n",
    "class OrchestrationResponse(BaseModel):\n",
    "    session_id: str\n",
    "    final_report: str = Field(description=\"ìµœì¢…ì ìœ¼ë¡œ ì™„ì„±ëœ ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œ\")\n",
    "    status: str = Field(default=\"COMPLETED\", description=\"ì „ì²´ ì›Œí¬í”Œë¡œìš°ì˜ ìµœì¢… ìƒíƒœ\")\n",
    "    trace_url: Optional[str] = Field(default=None, description=\"Langfuseì˜ ì „ì²´ ì‹¤í–‰ ê³¼ì • ì¶”ì  URL\")\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "# --- 2. ì „ë¬¸ê°€ ì„œë¹„ìŠ¤ ê°„ ë‚´ë¶€ í†µì‹ ìš© ëª¨ë¸  ---\n",
    "class ContentCreationRequest(BaseModel):\n",
    "    topic: str\n",
    "    user_preferences: str\n",
    "    memory_context: str\n",
    "\n",
    "class QualityControlRequest(BaseModel):\n",
    "    topic: str\n",
    "    draft_content: str\n",
    "    user_preferences: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part1_mcp_client_md"
   },
   "source": [
    "#### 1.5 MCP ì„œë²„ì™€ì˜ í†µì‹  ì±„ë„: `mcp_client.py`\n",
    "\n",
    "A2A ê²Œì´íŠ¸ì›¨ì´ ë’¤ì—ì„œ ì‘ë™í•˜ëŠ” ì „ë¬¸ê°€ Agent íŒ€ë“¤(íŠ¹íˆ CrewAI)ì´ MCP ì„œë²„ì˜ Toolê³¼ ë¦¬ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡, API í†µì‹ ì„ ìº¡ìŠí™”í•œ í´ë¼ì´ì–¸íŠ¸ ëª¨ë“ˆì„ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "day6_write_mcp_client_code"
   },
   "outputs": [],
   "source": [
    "%%writefile mcp_client.py\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "class MCPClient:\n",
    "    def __init__(self, base_url: str, api_key: str):\n",
    "        if not base_url:\n",
    "            raise ValueError(\"MCP ì„œë²„ì˜ base_urlì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.headers = {\n",
    "            \"X-API-Key\": api_key,\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "    def call_tool(self, tool_name: str, payload: dict) -> dict:\n",
    "        \"\"\"MCP ì„œë²„ì˜ Tool ì—”ë“œí¬ì¸íŠ¸ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤. payloadëŠ” ê° Toolì˜ Pydantic ëª¨ë¸ì— ë§ì¶° ì „ë‹¬ë©ë‹ˆë‹¤.\"\"\"\n",
    "        endpoint = f\"{self.base_url}/api/v1/tools/{tool_name}\"\n",
    "        try:\n",
    "            response = requests.post(endpoint, headers=self.headers, json=payload, timeout=180)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            return {\"error\": f\"HTTP ì˜¤ë¥˜: {e.response.status_code}\", \"detail\": e.response.text}\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return {\"error\": f\"Tool í˜¸ì¶œ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë°œìƒ: {e}\"}\n",
    "\n",
    "    def get_resource(self, resource_type: str, resource_id: str) -> dict:\n",
    "        \"\"\"MCP ì„œë²„ì˜ ì •ì  ë¦¬ì†ŒìŠ¤ ì—”ë“œí¬ì¸íŠ¸ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
    "        endpoint = f\"{self.base_url}/api/v1/resources/{resource_type}/{resource_id}\"\n",
    "        try:\n",
    "            response = requests.get(endpoint, headers=self.headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            return {\"error\": f\"HTTP ì˜¤ë¥˜: {e.response.status_code}\", \"detail\": e.response.text}\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return {\"error\": f\"ë¦¬ì†ŒìŠ¤ ì¡°íšŒ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë°œìƒ: {e}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part1_specialist_services_md"
   },
   "source": [
    "#### 1.6 ì „ë¬¸ê°€ ì„œë¹„ìŠ¤ ëª¨ë“ˆ êµ¬í˜„\n",
    "\n",
    "ì´ì œ ê°ê¸° ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬ë¡œ êµ¬í˜„ëœ 3ê°œì˜ í•µì‹¬ ì „ë¬¸ê°€ ì„œë¹„ìŠ¤ ë¡œì§ì„ ë…ë¦½ëœ íŒŒì¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.  \n",
    "\n",
    "ì´ ëª¨ë“ˆë“¤ì€ ê²Œì´íŠ¸ì›¨ì´ë¡œë¶€í„° ì‘ì—…ì„ ìœ„ì„ë°›ì•„ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "day6_write_crewai_service_code"
   },
   "outputs": [],
   "source": [
    "%%writefile service_crewai_research.py\n",
    "\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.tools import BaseTool\n",
    "import json\n",
    "from mcp_client import MCPClient\n",
    "from langfuse.langchain import CallbackHandler as LangfuseCallbackHandler\n",
    "from langfuse import observe\n",
    "\n",
    "class MCPToolWrapper(BaseTool):\n",
    "    name: str\n",
    "    description: str\n",
    "    client: MCPClient\n",
    "    payload_keys: list[str]\n",
    "\n",
    "    @observe(name=\"crewai-mcp-tool-call\")\n",
    "    def _run(self, kwargs) -> str:\n",
    "        print(f\"  [MCP Bridge] CrewAI -> MCP: Calling tool '{self.name}' with args {kwargs}\")\n",
    "        # Pydantic ëª¨ë¸ì— ë§ëŠ” payload êµ¬ì„±\n",
    "        payload = {key: kwargs.get(key) for key in self.payload_keys}\n",
    "        response = self.client.call_tool(self.name, payload)\n",
    "        return json.dumps(response, ensure_ascii=False)\n",
    "\n",
    "@observe(name=\"crewai-research-team-execution\")\n",
    "def run_research_crew(topic: str, user_preferences: str, memory_context: str, mcp_client: MCPClient):\n",
    "    print(f\"[CrewAI Service] ğŸ‘¥ ë”¥ ë¦¬ì„œì¹˜ íŒ€ ê°€ë™ (ì£¼ì œ: {topic[:30]}...)\")\n",
    "    try:\n",
    "        handler = LangfuseCallbackHandler()\n",
    "        llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", callbacks=[handler])\n",
    "\n",
    "        # MCP Tool Wrappers\n",
    "        web_search = MCPToolWrapper(name=\"web_search\", description=\"ì›¹ ê²€ìƒ‰\", client=mcp_client, payload_keys=[\"query\"])\n",
    "        arxiv_search = MCPToolWrapper(name=\"arxiv_search\", description=\"í•™ìˆ  ë…¼ë¬¸ ê²€ìƒ‰\", client=mcp_client, payload_keys=[\"query\"])\n",
    "        news_search = MCPToolWrapper(name=\"news_api_search\", description=\"ìµœì‹  ë‰´ìŠ¤ ê²€ìƒ‰\", client=mcp_client, payload_keys=[\"query\"])\n",
    "\n",
    "        # Agents\n",
    "        tech_analyst = Agent(role=\"ê¸°ìˆ  ë¶„ì„ê°€\", goal=f\"{topic}ì˜ í•µì‹¬ ê¸°ìˆ (HW/SW) ë¶„ì„\", backstory=\"ì‹¤ë¦¬ì½˜ë°¸ë¦¬ 20ë…„ ê²½ë ¥ì˜ ìµœê³  ê¸°ìˆ  ë¶„ì„ê°€.\", tools=[web_search, arxiv_search], llm=llm, verbose=True)\n",
    "        market_analyst = Agent(role=\"ì‹œì¥ ë¶„ì„ê°€\", goal=\"ì£¼ìš” ê¸°ì—… ì „ëµ ë° ì‹œì¥ ë™í–¥ ë¶„ì„\", backstory=\"ì›”ìŠ¤íŠ¸ë¦¬íŠ¸ ì¶œì‹  ì‹œì¥ ë¶„ì„ ì „ë¬¸ê°€.\", tools=[web_search, news_search], llm=llm, verbose=True)\n",
    "        social_analyst = Agent(role=\"ì‚¬íšŒ/ìœ¤ë¦¬ ë¶„ì„ê°€\", goal=\"ê¸°ìˆ ì˜ ì‚¬íšŒì , ìœ¤ë¦¬ì  ì˜í–¥ ë¶„ì„\", backstory=\"í“¨ì²˜ë¼ë§ˆ ì—°êµ¬ì†Œì˜ ì‚¬íšŒí•™ì.\", tools=[web_search], llm=llm, verbose=True)\n",
    "\n",
    "        # Tasks\n",
    "        tech_task = Task(description=f\"'{topic}'ì˜ ê¸°ìˆ ì  ì¸¡ë©´(ì„¼ì„œ, ì¹©ì…‹, SLAM ì•Œê³ ë¦¬ì¦˜ ë“±)ì„ ë¶„ì„í•˜ë¼.\", expected_output=\"ê¸°ìˆ  ë¶„ì„ ë³´ê³ ì„œ\", agent=tech_analyst)\n",
    "        market_task = Task(description=f\"'{topic}' ê´€ë ¨ ì‹œì¥ ë™í–¥, ì£¼ìš” ê¸°ì—…(Apple, Meta ë“±)ì˜ ì „ëµì„ ë¶„ì„í•˜ë¼.\", expected_output=\"ì‹œì¥ ë™í–¥ ë³´ê³ ì„œ\", agent=market_analyst)\n",
    "        social_task = Task(description=f\"'{topic}'ì´ ê°€ì ¸ì˜¬ ì‚¬íšŒì , ìœ¤ë¦¬ì  ë¬¸ì œ(í”„ë¼ì´ë²„ì‹œ, ë””ì§€í„¸ ê²©ì°¨ ë“±)ë¥¼ ë¶„ì„í•˜ë¼.\", expected_output=\"ì‚¬íšŒ/ìœ¤ë¦¬ ë¶„ì„ ë³´ê³ ì„œ\", agent=social_analyst)\n",
    "\n",
    "        # Final Writer Agent\n",
    "        report_writer = Agent(role=\"ì¢…í•© ë³´ê³ ì„œ ì‘ì„±ê°€\", goal=\"ì—¬ëŸ¬ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œ ì´ˆì•ˆ ì‘ì„±\", backstory=\"The Economistì˜ ìˆ˜ì„ ì—ë””í„°.\", llm=llm, verbose=True)\n",
    "        write_task = Task(description=f\"ê¸°ìˆ , ì‹œì¥, ì‚¬íšŒ ë¶„ì„ ë³´ê³ ì„œë¥¼ ëª¨ë‘ ì¢…í•©í•˜ê³ , ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­('{user_preferences}')ê³¼ ê³¼ê±°ì˜ ì»¨í…ìŠ¤íŠ¸('{memory_context}')ë¥¼ ë°˜ì˜í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œ ì´ˆì•ˆì„ ì‘ì„±í•˜ë¼.\", expected_output=\"ì™„ì„±ë„ ë†’ì€ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ì´ˆì•ˆ\", agent=report_writer, context=[tech_task, market_task, social_task])\n",
    "\n",
    "        crew = Crew(agents=[tech_analyst, market_analyst, social_analyst, report_writer], tasks=[tech_task, market_task, social_task, write_task], process=Process.hierarchical, manager_llm=llm)\n",
    "        result = crew.kickoff()\n",
    "        print(\"[CrewAI Service]  ë¦¬ì„œì¹˜ ë° ì´ˆì•ˆ ì‘ì„± ì™„ë£Œ.\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"CrewAI ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "day6_write_adk_service_code"
   },
   "outputs": [],
   "source": [
    "%%writefile service_adk_quality.py\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langfuse.langchain import CallbackHandler\n",
    "from mcp_client import MCPClient\n",
    "import json\n",
    "import os\n",
    "\n",
    "def run_quality_control(topic: str, draft_content: str, mcp_client: MCPClient):\n",
    "    print(f\"[ADK Service] ğŸ§ í’ˆì§ˆ ê´€ë¦¬ ë° í¸ì§‘ ì‘ì—… ì‹œì‘...\")\n",
    "    handler = CallbackHandler()\n",
    "\n",
    "    try:\n",
    "        ethics_review_res = mcp_client.call_tool(\"ethics_check\", {\"text\": draft_content})\n",
    "        ethics_review = ethics_review_res.get('result', '{\"ethics_review\": \"N/A\"}')\n",
    "    except Exception as e:\n",
    "        ethics_review = '{\"ethics_review\": \"N/A\"}'\n",
    "        print(f\"! ADK Service: ethics_check ë„êµ¬ í˜¸ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    try:\n",
    "        readability_res = mcp_client.get_resource(\"style_guides\", \"samsung_technical_blog\")\n",
    "        style_guide = readability_res.get('content', 'N/A')\n",
    "    except Exception as e:\n",
    "        style_guide = 'N/A'\n",
    "        print(f\"! ADK Service: style_guides ë¦¬ì†ŒìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    editor_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", temperature=0.1, callbacks=[handler])\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"ë‹¹ì‹ ì€ ì‚¼ì„±ì „ì ê¸°ìˆ  ë¸”ë¡œê·¸ì˜ ìµœê³  ìˆ˜ì¤€ í¸ì§‘ìì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì£¼ì–´ì§„ ì´ˆì•ˆì„ ìµœì¢… ë°œí–‰ë³¸ìœ¼ë¡œ ì™„ì„±í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "    [ì£¼ì œ]: {topic}\n",
    "    [ì´ˆì•ˆ]:\n",
    "{draft_content}\n",
    "    [ìŠ¤íƒ€ì¼ ê°€ì´ë“œ]:\n",
    "{style_guide}\n",
    "    [ìœ¤ë¦¬ì„± ê²€í†  ê²°ê³¼]:\n",
    "{ethics_review}\n",
    "\n",
    "    [ìš”ì²­ì‚¬í•­]:\n",
    "    1. ìŠ¤íƒ€ì¼ ê°€ì´ë“œë¥¼ ì™„ë²½í•˜ê²Œ ì¤€ìˆ˜í•˜ì—¬ ë¬¸ì²´ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.\n",
    "    2. ìœ¤ë¦¬ì„± ê²€í†  ê²°ê³¼ë¥¼ ë°˜ì˜í•˜ì—¬ ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” í‘œí˜„ì„ ìˆ˜ì •í•˜ì„¸ìš”.\n",
    "    3. ì „ì²´ì ì¸ ë…¼ë¦¬ íë¦„ì„ ê°•í™”í•˜ê³ , ë…ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ë¬¸ì¥ì„ ë‹¤ë“¬ì–´ì£¼ì„¸ìš”.\n",
    "    4. ìµœì¢…ì ìœ¼ë¡œ, ë…ìì—ê²Œ ê¹Šì€ ì¸ì‚¬ì´íŠ¸ë¥¼ ì£¼ëŠ” ì™„ë²½í•œ ê¸°ìˆ  ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ Markdown í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "    \"\"\"\n",
    "    final_post = editor_llm.invoke(prompt).content\n",
    "\n",
    "    diagram_desc = \"ì‚¬ìš©ìê°€ AR ê¸€ë˜ìŠ¤ë¡œ ê°€ìƒ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´, AI ë¡œë´‡ì´ ë¬¼ë¦¬ì ìœ¼ë¡œ ì „ë“±ì„ ì¼œëŠ” ê³¼ì •ì„ ë³´ì—¬ì£¼ëŠ” ìˆœì„œë„\"\n",
    "    mermaid_code = \"\"\n",
    "    try:\n",
    "        mermaid_res = mcp_client.call_tool(\"mermaid_generator\", {\"description\": diagram_desc})\n",
    "\n",
    "        if 'error' in mermaid_res:\n",
    "             mermaid_code = \"graph TD; A[Mermaid ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì˜¤ë¥˜] --> B[MCP ì„œë²„ ì‘ë‹µ í™•ì¸ í•„ìš”];\"\n",
    "        else:\n",
    "            result_str = mermaid_res.get('result', '{}')\n",
    "            try:\n",
    "                result_data = json.loads(result_str)\n",
    "                mermaid_code = result_data.get('mermaid_code', \"graph TD; A[Mermaid ì½”ë“œ ì—†ìŒ] --> B[ì‘ë‹µ êµ¬ì¡° ë¬¸ì œ];\")\n",
    "            except json.JSONDecodeError:\n",
    "                mermaid_code = \"graph TD; A[JSON íŒŒì‹± ì˜¤ë¥˜] --> B[ì‘ë‹µ í˜•ì‹ ë¬¸ì œ];\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" ADK Service: mermaid_generator í˜¸ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "        mermaid_code = \"graph TD; A[Mermaid ìƒì„± ì˜ˆì™¸ ë°œìƒ] --> B[ADK ì„œë¹„ìŠ¤ ë¡œê·¸ í™•ì¸ í•„ìš”];\"\n",
    "\n",
    "    final_report_with_diagram = f\"{final_post}\\\\n\\\\n## í•µì‹¬ ê¸°ìˆ  ì•„í‚¤í…ì²˜\\\\n\\\\n```mermaid\\\\n{mermaid_code}\\\\n```\"\n",
    "\n",
    "    print(\"[ADK Service]  ìµœì¢… í¸ì§‘ ë° ì‹œê°í™” ìë£Œ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    return final_report_with_diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part1_memory_system_md"
   },
   "source": [
    "#### 1.7 ì‹œìŠ¤í…œì˜ ë‘ë‡Œ: í†µí•© ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ êµ¬í˜„ (`memory_system.py`)\n",
    "\n",
    "Agentê°€ ê³¼ê±°ì˜ ê²½í—˜ìœ¼ë¡œë¶€í„° í•™ìŠµí•  ìˆ˜ ìˆë„ë¡, 3ì¼ì°¨ì— ì™„ì„±í–ˆë˜ `MemorySystem`ì˜ ì™„ì „í•œ ì½”ë“œë¥¼ ì¬êµ¬ì„±í•©ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ `Redis`ë¥¼ ë‹¨ê¸° ê¸°ì–µ(ëŒ€í™” ë§¥ë½)ìœ¼ë¡œ, `ChromaDB`ë¥¼ ì¥ê¸° ê¸°ì–µ(ê³¼ê±° ì§€ì‹, ì‚¬ìš©ì ì„ í˜¸ë„)ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "day6_write_memory_system_code"
   },
   "outputs": [],
   "source": [
    "%%writefile memory_system.py\n",
    "\n",
    "import json\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "import redis\n",
    "import chromadb\n",
    "from langchain.schema import Document\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import instructor\n",
    "import uuid\n",
    "\n",
    "class ShortTermMemoryManager:\n",
    "    def __init__(self, client: redis.Redis):\n",
    "        self._client = client\n",
    "        self._llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0)\n",
    "\n",
    "    def _get_session_key(self, session_id: str) -> str:\n",
    "        return f\"session:short_term:{session_id}\"\n",
    "\n",
    "    def log_message(self, session_id: str, role: str, content: str):\n",
    "        key = self._get_session_key(session_id)\n",
    "        message = json.dumps({\"role\": role, \"content\": content})\n",
    "        self._client.rpush(key, message)\n",
    "        self._client.expire(key, 86400)\n",
    "\n",
    "    def get_history(self, session_id: str) -> list:\n",
    "        key = self._get_session_key(session_id)\n",
    "        messages = self._client.lrange(key, 0, -1)\n",
    "        return [json.loads(msg) for msg in messages]\n",
    "\n",
    "    def summarize_and_get_context(self, session_id: str, max_tokens: int = 1500) -> str:\n",
    "        history = self.get_history(session_id)\n",
    "        if not history:\n",
    "            return \"(ìƒˆë¡œìš´ ëŒ€í™”ì…ë‹ˆë‹¤. ì´ì „ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)\"\n",
    "        history_text = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in history])\n",
    "        if len(history_text) / 2.5 > max_tokens:\n",
    "            prompt = f\"ë‹¤ìŒ ëŒ€í™” ê¸°ë¡ì˜ í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì¤˜: {history_text}\"\n",
    "            try:\n",
    "                summary = self._llm.invoke(prompt).content\n",
    "                return f\"(ì´ì „ ëŒ€í™” ìš”ì•½): {summary}\"\n",
    "            except Exception:\n",
    "                return f\"(ìµœê·¼ ëŒ€í™”): ...{history_text[-max_tokens:]}\"\n",
    "        else:\n",
    "            return f\"(ì´ì „ ëŒ€í™” ê¸°ë¡):\\n{history_text}\"\n",
    "\n",
    "class UserPreferences(BaseModel):\n",
    "    writing_style: str = Field(description=\"ì‚¬ìš©ìê°€ ì„ í˜¸í•˜ëŠ” ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼ (ì˜ˆ: ì „ë¬¸ì , ì¹œê·¼í•¨)\")\n",
    "    preferred_format: str = Field(description=\"ì‚¬ìš©ìê°€ ì„ í˜¸í•˜ëŠ” ê¸€ì˜ êµ¬ì¡° (ì˜ˆ: ë‘ê´„ì‹, Q&A í˜•ì‹)\")\n",
    "    key_topics_of_interest: List[str] = Field(description=\"ì‚¬ìš©ìì˜ ì£¼ìš” ê´€ì‹¬ í† í”½\")\n",
    "\n",
    "class LongTermMemoryManager:\n",
    "    def __init__(self, client: chromadb.Client):\n",
    "        self._client = client\n",
    "        self._embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "        self.collection = self._client.get_or_create_collection(name=\"unified_agent_knowledge_base\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "    def add_memory(self, content: str, memory_type: str, metadata: dict):\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        metadata['memory_type'] = memory_type\n",
    "        self.collection.add(documents=[content], metadatas=[metadata], ids=[doc_id])\n",
    "        print(f\"ğŸ§  ìƒˆë¡œìš´ ì¥ê¸° ê¸°ì–µ ì¶”ê°€ (Type: {memory_type})\")\n",
    "\n",
    "    def search_memory(self, query: str, n_results: int = 3, memory_type: str = None) -> list:\n",
    "        where_filter = {\"memory_type\": memory_type} if memory_type else {}\n",
    "        results = self.collection.query(query_texts=[query], n_results=n_results, where=where_filter)\n",
    "        retrieved_docs = []\n",
    "        if results and results['ids'] and results['ids'][0]:\n",
    "            for i in range(len(results['ids'][0])):\n",
    "                doc = Document(page_content=results['documents'][0][i], metadata=results['metadatas'][0][i])\n",
    "                retrieved_docs.append(doc)\n",
    "        return retrieved_docs\n",
    "\n",
    "class MemorySystem:\n",
    "    def __init__(self, short_term: ShortTermMemoryManager, long_term: LongTermMemoryManager):\n",
    "        self.short_term = short_term\n",
    "        self.long_term = long_term\n",
    "        self.preference_extractor = instructor.from_provider('google/gemini-2.5-flash-lite')\n",
    "\n",
    "    def extract_and_store_preferences(self, session_id: str):\n",
    "        history_text = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in self.short_term.get_history(session_id)])\n",
    "        if not history_text:\n",
    "            return\n",
    "        prompt = f\"ë‹¤ìŒ ëŒ€í™” ê¸°ë¡ì„ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ ê¸€ì“°ê¸° ì„ í˜¸ë„ë¥¼ ìš”ì•½í•´ì¤˜: {history_text}\"\n",
    "        try:\n",
    "            preferences = self.preference_extractor.chat.completions.create(messages=[{\"role\": \"user\", \"content\": prompt}], response_model=UserPreferences)\n",
    "            self.long_term.add_memory(content=preferences.model_dump_json(indent=2), memory_type=\"user_preference\", metadata={\"session_id\": session_id})\n",
    "        except Exception as e:\n",
    "            print(f\"! ì„ í˜¸ë„ ì¶”ì¶œ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "    def retrieve_comprehensive_context(self, session_id: str, query: str) -> str:\n",
    "        print(f\"ğŸ“š ì¿¼ë¦¬ '{query[:30]}...'ì— ëŒ€í•œ ì¢…í•© ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...\")\n",
    "        short_term_context = self.short_term.summarize_and_get_context(session_id)\n",
    "        preferences = self.long_term.search_memory(\"ì‚¬ìš©ì ê¸€ì“°ê¸° ì„ í˜¸ë„\", n_results=1, memory_type=\"user_preference\")\n",
    "        past_knowledge = self.long_term.search_memory(query, n_results=2, memory_type=\"final_report\")\n",
    "\n",
    "        full_context = \"--- ì¢…í•© ì»¨í…ìŠ¤íŠ¸ ---\\n\"\n",
    "        full_context += f\"[í˜„ì¬ ëŒ€í™” ë§¥ë½]\\n{short_term_context}\\n\"\n",
    "        if preferences:\n",
    "            full_context += f\"\\n[í•™ìŠµëœ ì‚¬ìš©ì ì„ í˜¸ë„]\\n{preferences[0].page_content}\\n\"\n",
    "        if past_knowledge:\n",
    "            knowledge_str = \"\\n\".join([f\"- (ê³¼ê±° ë³´ê³ ì„œ): {doc.page_content[:200]}...\" for doc in past_knowledge])\n",
    "            full_context += f\"\\n[ê´€ë ¨ ê³¼ê±° ì§€ì‹]\\n{knowledge_str}\\n\"\n",
    "        full_context += \"--- ì»¨í…ìŠ¤íŠ¸ ë ---\"\n",
    "        return full_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part1_orchestrator_md"
   },
   "source": [
    "#### 1.8 `LangGraph` ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° êµ¬í˜„ (`service_langgraph_orchestrator.py`)\n",
    "\n",
    "ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ì´ê´„í•˜ëŠ” `LangGraph` ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ë¡œì§ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ì‘ì—…ì˜ ì¤‘ê°„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë™ì ìœ¼ë¡œ ê²°ì •í•˜ëŠ” ì¡°ê±´ë¶€ ë¼ìš°íŒ… ë¡œì§ì„ í¬í•¨í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "day6_write_orchestrator_service_code"
   },
   "outputs": [],
   "source": [
    "%%writefile service_langgraph_orchestrator.py\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "import operator\n",
    "from langfuse.langchain import CallbackHandler\n",
    "from langfuse import observe\n",
    "\n",
    "from memory_system import MemorySystem\n",
    "from service_crewai_research import run_research_crew\n",
    "from service_adk_quality import run_quality_control\n",
    "from mcp_client import MCPClient\n",
    "\n",
    "class OrchestratorState(TypedDict):\n",
    "    session_id: str\n",
    "    user_input: str\n",
    "    topic: str\n",
    "    user_preferences: str\n",
    "    memory_context: str\n",
    "    research_report: Optional[str]\n",
    "    final_report: Optional[str]\n",
    "    log: Annotated[List[str], operator.add]\n",
    "\n",
    "@observe(name=\"orchestrator-node-memory-retrieval\")\n",
    "def memory_retrieval_node(state: OrchestratorState, memory_system: MemorySystem) -> dict:\n",
    "    print(\"--- Node: ğŸ§  Memory Retrieval ---\")\n",
    "    session_id = state['session_id']\n",
    "    user_input = state['user_input']\n",
    "    context = memory_system.retrieve_comprehensive_context(session_id, user_input)\n",
    "    topic = user_input\n",
    "    preferences = \"ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰¬ìš´ í†¤ìœ¼ë¡œ ì‘ì„±\"\n",
    "    return {\"memory_context\": context, \"topic\": topic, \"user_preferences\": preferences, \"log\": [\"Memory context retrieved.\"]}\n",
    "\n",
    "@observe(name=\"orchestrator-node-research\")\n",
    "def research_node(state: OrchestratorState, mcp_client: MCPClient) -> dict:\n",
    "    print(\"--- Node: âœï¸  Delegating to CrewAI Research Team ---\")\n",
    "    report = run_research_crew(state['topic'], state['user_preferences'], state['memory_context'], mcp_client)\n",
    "    return {\"research_report\": report, \"log\": [\"CrewAI research and drafting completed.\"]}\n",
    "\n",
    "@observe(name=\"orchestrator-node-quality-control\")\n",
    "def quality_control_node(state: OrchestratorState, mcp_client: MCPClient) -> dict:\n",
    "    print(\"--- Node: ğŸ§ Delegating to ADK Quality Team ---\")\n",
    "    final_report = run_quality_control(state['topic'], state['research_report'], mcp_client)\n",
    "    return {\"final_report\": final_report, \"log\": [\"ADK quality control and finalization completed.\"]}\n",
    "\n",
    "def create_orchestrator(memory_system: MemorySystem, mcp_client: MCPClient):\n",
    "    workflow = StateGraph(OrchestratorState)\n",
    "\n",
    "    workflow.add_node(\"memory_retrieval\", lambda state: memory_retrieval_node(state, memory_system))\n",
    "    workflow.add_node(\"research\", lambda state: research_node(state, mcp_client))\n",
    "    workflow.add_node(\"quality_control\", lambda state: quality_control_node(state, mcp_client))\n",
    "\n",
    "    workflow.set_entry_point(\"memory_retrieval\")\n",
    "    workflow.add_edge(\"memory_retrieval\", \"research\")\n",
    "    workflow.add_edge(\"research\", \"quality_control\")\n",
    "    workflow.add_edge(\"quality_control\", END)\n",
    "\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part1_a2a_gateway_md"
   },
   "source": [
    "#### 1.9 A2A ìœ ì‚¬ ê²Œì´íŠ¸ì›¨ì´ êµ¬í˜„ (`a2a_gateway.py`)\n",
    "\n",
    "ë§ˆì§€ë§‰ìœ¼ë¡œ, ì™¸ë¶€ì˜ ìš”ì²­ì„ ë°›ì•„ ì²˜ë¦¬í•˜ëŠ”  `FastAPI` ê¸°ë°˜ ê²Œì´íŠ¸ì›¨ì´ì˜ ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.  \n",
    "\n",
    "ì´ ê²Œì´íŠ¸ì›¨ì´ëŠ” `LangGraph` ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ë¥¼ í˜¸ì¶œí•˜ê³ , `Langfuse`ë¥¼ í†µí•´ ìš”ì²­ì— ëŒ€í•œ ìµœìƒìœ„ Traceë¥¼ ìƒì„±í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "day6_write_a2a_gateway_code"
   },
   "outputs": [],
   "source": [
    "%%writefile a2a_gateway.py\n",
    "\n",
    "from fastapi import FastAPI, Depends, HTTPException, Security, Request\n",
    "from fastapi.security import APIKeyHeader\n",
    "from contextlib import asynccontextmanager\n",
    "import os\n",
    "import redis\n",
    "import chromadb\n",
    "import traceback\n",
    "\n",
    "from langfuse import Langfuse\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "from a2a_protocol import OrchestrationRequest, OrchestrationResponse\n",
    "from mcp_client import MCPClient\n",
    "from memory_system import MemorySystem, ShortTermMemoryManager, LongTermMemoryManager\n",
    "from service_langgraph_orchestrator import create_orchestrator\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    app.state.is_ready = False\n",
    "    try:\n",
    "        mcp_server_url = \"http://localhost:8501\"\n",
    "        master_api_key = os.environ[\"MASTER_API_KEY\"]\n",
    "\n",
    "        app.state.langfuse_client = Langfuse()\n",
    "        app.state.mcp_client = MCPClient(base_url=mcp_server_url, api_key=master_api_key)\n",
    "\n",
    "        upstash_rest_url = os.environ['UPSTASH_REDIS_REST_URL']\n",
    "        upstash_rest_token = os.environ['UPSTASH_REDIS_REST_TOKEN']\n",
    "        full_redis_url = f\"rediss://:{upstash_rest_token}@{upstash_rest_url.replace('https://', '')}\"\n",
    "\n",
    "        redis_client = redis.Redis.from_url(full_redis_url, decode_responses=True)\n",
    "        chroma_client = chromadb.PersistentClient(path=\"final_project_db\")\n",
    "        app.state.memory_system = MemorySystem(ShortTermMemoryManager(redis_client), LongTermMemoryManager(chroma_client))\n",
    "        app.state.orchestrator = create_orchestrator(app.state.memory_system, app.state.mcp_client)\n",
    "\n",
    "        app.state.is_ready = True\n",
    "        print(\"--- [A2A Gateway] LIFESPAN: All components initialized successfully. Server is ready. ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"! [A2A Gateway] LIFESPAN: CRITICAL STARTUP ERROR! \")\n",
    "        print(traceback.format_exc())\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        app.state.orchestrator = None\n",
    "        app.state.memory_system = None\n",
    "\n",
    "    yield\n",
    "    if getattr(app.state, 'langfuse_client', None):\n",
    "        app.state.langfuse_client.flush()\n",
    "\n",
    "app = FastAPI(title=\"A2A Gateway for Sentient Architect\", version=\"1.0.0\", lifespan=lifespan)\n",
    "API_KEY_NAME = \"X-API-Key\"\n",
    "api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)\n",
    "\n",
    "async def get_api_key(api_key: str = Security(api_key_header)):\n",
    "    if api_key == os.environ.get(\"MASTER_API_KEY\"):\n",
    "        return api_key\n",
    "    raise HTTPException(status_code=401, detail=\"Invalid API Key\")\n",
    "\n",
    "@app.get(\"/\", tags=[\"Status\"])\n",
    "async def root(request: Request):\n",
    "    if getattr(request.app.state, 'is_ready', False):\n",
    "        return {\"status\": \"ok\", \"message\": \"A2A Gateway is alive and well.\"}\n",
    "    else:\n",
    "        raise HTTPException(status_code=503, detail=\"Service not ready. Check startup logs.\")\n",
    "\n",
    "@app.post(\"/api/v1/generate-deep-research-report\", response_model=OrchestrationResponse, tags=[\"Core Workflow\"])\n",
    "async def generate_report(fastapi_req: Request, data: OrchestrationRequest, api_key: str = Depends(get_api_key)):\n",
    "    orchestrator = fastapi_req.app.state.orchestrator\n",
    "    memory_system = fastapi_req.app.state.memory_system\n",
    "\n",
    "    if not orchestrator or not memory_system:\n",
    "        raise HTTPException(status_code=503, detail=\"Orchestrator or MemorySystem not initialized. Check gateway startup logs.\")\n",
    "\n",
    "    # ì´ˆê¸° ìƒíƒœì— ëª¨ë“  í•„ìˆ˜ í‚¤ë¥¼ ë¯¸ë¦¬ ì„¤ì •\n",
    "    initial_state = {\n",
    "        \"session_id\": data.session_id,\n",
    "        \"user_input\": data.user_input,\n",
    "        \"topic\": \"\",\n",
    "        \"user_preferences\": \"\",\n",
    "        \"memory_context\": \"\",\n",
    "        \"research_report\": None,\n",
    "        \"final_report\": None,\n",
    "        \"log\": []\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(f\"[A2A Gateway] Starting orchestration for session: {data.session_id}\")\n",
    "\n",
    "        # LangGraph orchestrator ì‹¤í–‰\n",
    "        final_state = orchestrator.invoke(initial_state)\n",
    "\n",
    "        print(f\"[A2A Gateway] Orchestration completed. Final state keys: {list(final_state.keys())}\")\n",
    "\n",
    "        # ì•ˆì „í•˜ê²Œ ê²°ê³¼ ì¶”ì¶œ\n",
    "        final_report_content = final_state.get('final_report', '')\n",
    "        topic_content = final_state.get('topic', 'Unknown Topic')\n",
    "\n",
    "        # ë©”ëª¨ë¦¬ì— ì €ì¥ (ë‚´ìš©ì´ ìˆì„ ë•Œë§Œ)\n",
    "        if final_report_content and len(final_report_content.strip()) > 0:\n",
    "            try:\n",
    "                memory_system.long_term.add_memory(\n",
    "                    content=final_report_content,\n",
    "                    memory_type=\"final_report\",\n",
    "                    metadata={\"topic\": topic_content}\n",
    "                )\n",
    "                memory_system.extract_and_store_preferences(data.session_id)\n",
    "                print(\"[A2A Gateway] Successfully stored results in long-term memory\")\n",
    "            except Exception as mem_error:\n",
    "                print(f\"[A2A Gateway] Warning: Memory storage failed: {mem_error}\")\n",
    "\n",
    "        # ì„±ê³µ ì‘ë‹µ ë°˜í™˜\n",
    "        response = OrchestrationResponse(\n",
    "            session_id=data.session_id,\n",
    "            final_report=final_report_content,\n",
    "            status=\"COMPLETED\"\n",
    "        )\n",
    "\n",
    "        print(f\"[A2A Gateway] Successfully generated report with {len(final_report_content)} characters\")\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"! [A2A Gateway] ERROR during report generation:\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "        # ì—ëŸ¬ ì‘ë‹µ ë°˜í™˜\n",
    "        return OrchestrationResponse(\n",
    "            session_id=data.session_id,\n",
    "            final_report=\"\",\n",
    "            status=\"FAILED\",\n",
    "            error_message=str(e)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part2_intro_md"
   },
   "source": [
    "### 2. ì‹œìŠ¤í…œ ì‹¤í–‰\n",
    "\n",
    "ì´ì œ ëª¨ë“  ì„¤ê³„ì™€ êµ¬í˜„ì´ ëë‚¬ìŠµë‹ˆë‹¤. ì—ì´ì „íŠ¸ë¥¼ ê°€ë™í•˜ê³ , ë‹¨ì¼ ìš”ì²­ìœ¼ë¡œ ì „ì²´ ì—ì´ì „íŠ¸ê°€ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ê³¼ì •ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë¨¼ì €, ë‘ ê°œì˜ FastAPI ì„œë²„ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê³  ì™¸ë¶€ì—ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ëŠ” í•µì‹¬ ìœ í‹¸ë¦¬í‹°ì¸ ì„œë²„ ëŸ°ì²˜ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part2_launcher_md"
   },
   "source": [
    "#### 2.1 ëŸ°ì²˜ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "day6_relaunch_servers_code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import nest_asyncio\n",
    "import threading\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "upstash_url = os.environ.get(\"UPSTASH_REDIS_REST_URL\", \"\").replace(\"https://\", \"\")\n",
    "upstash_token = os.environ.get(\"UPSTASH_REDIS_REST_TOKEN\", \"\")\n",
    "os.environ[\"UPSTASH_REDIS_URL\"] = f\"rediss://:{upstash_token}@{upstash_url}\"\n",
    "print(\" ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—°ê²° ì •ë³´ ì„¤ì • ì™„ë£Œ.\")\n",
    "\n",
    "print(\"\\n--- [Step 1] ëª¨ë“  ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤... ---\")\n",
    "subprocess.run(\"lsof -t -i:8501 | xargs -r kill -9\", shell=True, capture_output=True)\n",
    "subprocess.run(\"lsof -t -i:8502 | xargs -r kill -9\", shell=True, capture_output=True)\n",
    "print(\" ê¸°ì¡´ ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì™„ë£Œ.\")\n",
    "time.sleep(2)\n",
    "\n",
    "print(\"\\n--- [Step 2] MCP ë° A2A ì„œë²„ë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤... ---\")\n",
    "\n",
    "\n",
    "def unified_logger(process, name):\n",
    "    # stdoutê³¼ stderrë¥¼ í†µí•©í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "    for line in iter(process.stdout.readline, \"\"):\n",
    "        if line:\n",
    "            print(f\"[{name} LOG] {line.strip()}\")\n",
    "    for line in iter(process.stderr.readline, \"\"):\n",
    "        if line:\n",
    "            print(f\"[{name} ERR] {line.strip()}\")\n",
    "\n",
    "\n",
    "current_env = os.environ.copy()\n",
    "\n",
    "# MCP ì„œë²„ ì‹œì‘\n",
    "mcp_process = subprocess.Popen(\n",
    "    [sys.executable, \"-m\", \"uvicorn\", \"mcp_server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8501\", \"--reload\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True,\n",
    "    encoding=\"utf-8\",\n",
    "    env=current_env,\n",
    ")\n",
    "threading.Thread(target=unified_logger, args=(mcp_process, \"mcp_server\"), daemon=True).start()\n",
    "print(\" MCP ì„œë²„(8501) ì‹œì‘ ì¤‘...\")\n",
    "\n",
    "# A2A ê²Œì´íŠ¸ì›¨ì´ ì‹œì‘\n",
    "a2a_process = subprocess.Popen(\n",
    "    [sys.executable, \"-m\", \"uvicorn\", \"a2a_gateway:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8502\", \"--reload\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True,\n",
    "    encoding=\"utf-8\",\n",
    "    env=current_env,\n",
    ")\n",
    "threading.Thread(target=unified_logger, args=(a2a_process, \"a2a_gateway\"), daemon=True).start()\n",
    "print(\"ğŸš€ A2A ê²Œì´íŠ¸ì›¨ì´(8502) ì‹œì‘ ì¤‘...\")\n",
    "\n",
    "print(\"\\n--- [Step 3] ì„œë²„ URL ì„¤ì • ---\")\n",
    "mcp_url = \"http://localhost:8501\"\n",
    "a2a_url = \"http://localhost:8502\"\n",
    "os.environ[\"MCP_SERVER_URL\"] = mcp_url\n",
    "print(f\" MCP ì„œë²„ URL: {mcp_url}\")\n",
    "print(f\" A2A ê²Œì´íŠ¸ì›¨ì´ URL: {a2a_url}\")\n",
    "display(HTML(f'<b><a href=\"{mcp_url}/docs\" target=\"_blank\"> MCP ì„œë²„ API Docs</a></b>'))\n",
    "display(HTML(f'<b><a href=\"{a2a_url}/docs\" target=\"_blank\"> A2A ê²Œì´íŠ¸ì›¨ì´ API Docs</a></b>'))\n",
    "\n",
    "print(\"\\n--- [Step 4] ì‹œìŠ¤í…œ ìƒíƒœ ìµœì¢… í™•ì¸ --- \")\n",
    "\n",
    "\n",
    "def wait_for_server(url, server_name, max_retries=20, delay=2):\n",
    "    print(f\"    '{server_name}' ì„œë²„ì˜ ì‘ë‹µì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...\")\n",
    "    for i in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                print(f\"    '{server_name}' ì„œë²„ ì¤€ë¹„ ì™„ë£Œ! (ìƒíƒœ ì½”ë“œ: 200)\")\n",
    "                return True\n",
    "            else:  # 503 ì—ëŸ¬ ë“±ì„ í¬í•¨\n",
    "                print(f\"   ... ëŒ€ê¸° ì¤‘ ({i+1}/{max_retries}) - ìƒíƒœ ì½”ë“œ: {response.status_code}\")\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"   ... ëŒ€ê¸° ì¤‘ ({i+1}/{max_retries}) - ì•„ì§ ì—°ê²°í•  ìˆ˜ ì—†ìŒ\")\n",
    "        time.sleep(delay)\n",
    "    print(f\"    '{server_name}' ì„œë²„ê°€ ì‹œê°„ ë‚´ì— ì‘ë‹µí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    return False\n",
    "\n",
    "\n",
    "if wait_for_server(mcp_url, \"MCP Server\") and wait_for_server(a2a_url, \"A2A Gateway\"):\n",
    "    print(\"\\n ëª¨ë“  ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ ì˜¨ë¼ì¸ ìƒíƒœê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. ìµœì¢… ì‹¤í–‰ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"\\n! ì¼ë¶€ ì‹œìŠ¤í…œ ì‹œì‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìœ„ì˜ ë¡œê·¸ì—ì„œ ì§„ì§œ ì›ì¸ì„ ì°¾ì•„ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part2_client_run_md"
   },
   "source": [
    "#### 2.2 ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì‹¤í–‰\n",
    "\n",
    "ì•„ë˜ ì½”ë“œëŠ” ìµœì¢… ì‚¬ìš©ì ë˜ëŠ” ìƒìœ„ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì—­í• ì„ ìˆ˜í–‰í•˜ëŠ” 'ì§€íœ˜ì í´ë¼ì´ì–¸íŠ¸'ì…ë‹ˆë‹¤. ì´ í´ë¼ì´ì–¸íŠ¸ëŠ” ê²Œì´íŠ¸ì›¨ì´ì˜ ì—”ë“œí¬ì¸íŠ¸(`/api/v1/generate-deep-research-report`)ì— ìš”ì²­ì„ ë³´ë‚´ëŠ” ê²ƒìœ¼ë¡œ, ì‹œìŠ¤í…œ ì „ì²´ë¥¼ ê°€ë™ì‹œí‚µë‹ˆë‹¤.\n",
    "\n",
    "ì‹¤í–‰ì„ ì‹œì‘í•˜ë©´, A2A ê²Œì´íŠ¸ì›¨ì´ì™€ MCP ì„œë²„ì˜ ì‹¤ì‹œê°„ ë¡œê·¸, ê·¸ë¦¬ê³  ì´ í´ë¼ì´ì–¸íŠ¸ ì…€ì˜ ì¶œë ¥ì„ í†µí•´ ì „ì²´ ì›Œí¬í”Œë¡œìš°ê°€ ì–´ë–»ê²Œ ì§„í–‰ë˜ëŠ”ì§€, LangGraph ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ê°€ CrewAI íŒ€ê³¼ ADK íŒ€ì—ê²Œ ì–´ë–»ê²Œ ì‘ì—…ì„ ìœ„ì„í•˜ëŠ”ì§€ ê·¸ ê³¼ì •ì„ ê´€ì°°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "day6_final_client_run_code"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import uuid\n",
    "import json\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import time\n",
    "\n",
    "if \"a2a_url\" in locals() and a2a_url:\n",
    "    # --- 1. ìµœì¢… ì‹¤í–‰ì„ ìœ„í•œ ìš”ì²­ ë°ì´í„° ì¤€ë¹„ ---\n",
    "    session_id = f\"final-project-session-{uuid.uuid4().hex[:8]}\"\n",
    "    user_request = \"ì°¨ì„¸ëŒ€ ì¸ê°„-ì»´í“¨í„° ìƒí˜¸ì‘ìš©(HCI): ê³µê°„ ì»´í“¨íŒ…(Spatial Computing)ê³¼ ë¬¼ë¦¬ì  AI ì—ì´ì „íŠ¸(Embodied AI)ì˜ ìœµí•©ì´ ê°€ì ¸ì˜¬ ì‚°ì—… ë° ì‚¬íšŒ ë³€í™”ì— ëŒ€í•œ ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì¤˜.\"\n",
    "    headers = {\n",
    "        \"X-API-Key\": os.environ[\"MASTER_API_KEY\"],\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    payload = {\"session_id\": session_id, \"user_input\": user_request}\n",
    "\n",
    "    print(\"--- ğŸš€ ìµœì¢… ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ í´ë¼ì´ì–¸íŠ¸ ì‹¤í–‰ ---\")\n",
    "    print(f\"ì„¸ì…˜ ID: {session_id}\")\n",
    "    print(f\"ìš”ì²­ ì£¼ì œ: {user_request}\")\n",
    "    print(\"A2A ê²Œì´íŠ¸ì›¨ì´ë¡œ ìš”ì²­ì„ ì „ì†¡í•©ë‹ˆë‹¤... (ì´ ê³¼ì •ì€ ëª‡ ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n",
    "\n",
    "    # --- 2. A2A ê²Œì´íŠ¸ì›¨ì´ì— ìµœì¢… ë³´ê³ ì„œ ìƒì„± ìš”ì²­ ---\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.post(\n",
    "            f\"{a2a_url}/api/v1/generate-deep-research-report\", json=payload, headers=headers, timeout=600\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        response.raise_for_status()\n",
    "\n",
    "        result_data = response.json()\n",
    "\n",
    "        if result_data.get(\"status\") == \"FAILED\":\n",
    "            print(f\"\\n A2A ê²Œì´íŠ¸ì›¨ì´ì—ì„œ ì‘ì—… ì‹¤íŒ¨ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤ (ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ).\")\n",
    "            print(f\"   - ì„œë²„ ì˜¤ë¥˜ ë©”ì‹œì§€: {result_data.get('error_message')}\")\n",
    "\n",
    "            if result_data.get(\"trace_url\"):\n",
    "                display(\n",
    "                    HTML(\n",
    "                        f'<b><a href=\"{result_data[\"trace_url\"]}\" target=\"_blank\">ğŸ‘‰ Langfuseì—ì„œ ì‹¤íŒ¨ ì›ì¸ì„ ì¶”ì í•´ë³´ì„¸ìš”.</a></b>'\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            # --- 3. ì„±ê³µ ì‹œ ìµœì¢… ê²°ê³¼ë¬¼ ì¶œë ¥ ---\n",
    "            print(f\"\\n---  ì‘ì—… ì™„ë£Œ (ì´ ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ) ---\")\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"                            ìµœì¢… ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œ \")\n",
    "            print(\"=\" * 80)\n",
    "            display(Markdown(result_data.get(\"final_report\", \"ê²°ê³¼ ë³´ê³ ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\")))\n",
    "\n",
    "            # --- 4. Langfuse ì¶”ì  URL ì œê³µ ---\n",
    "            if result_data.get(\"trace_url\"):\n",
    "                trace_url = result_data[\"trace_url\"]\n",
    "                print(\"\\n\" + \"=\" * 80)\n",
    "                print(\"        ğŸ•µï¸â€â™‚ï¸ Langfuseì—ì„œ ì „ì²´ ì‹¤í–‰ ê³¼ì • ì¶”ì í•˜ê¸°\")\n",
    "                print(\"=\" * 80)\n",
    "                display(\n",
    "                    HTML(\n",
    "                        f'<b><a href=\"{trace_url}\" target=\"_blank\">ğŸ‘‰ ì—¬ê¸°ë¥¼ í´ë¦­í•˜ì—¬ ì—”ë“œíˆ¬ì—”ë“œ Traceë¥¼ í™•ì¸í•˜ì„¸ìš”.</a></b>'\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"\\n HTTP ì˜¤ë¥˜ ë°œìƒ: {e.response.status_code}\")\n",
    "        print(f\"   - ì‘ë‹µ ë‚´ìš©: {e.response.text}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"\\n ìš”ì²­ ì‹¤íŒ¨: {e}\")\n",
    "        print(\"   - ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ ì¤‘ì¸ì§€, URLì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "else:\n",
    "    print(\" A2A ê²Œì´íŠ¸ì›¨ì´ URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ì „ ì…€ì„ ë¨¼ì € ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part3_intro_md"
   },
   "source": [
    "### 3. ì‹œìŠ¤í…œ í‰ê°€\n",
    "\n",
    "ì´ íŒŒíŠ¸ì—ì„œëŠ” `Langfuse`ë¥¼ í™œìš©í•˜ì—¬ ìš°ë¦¬ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •í•˜ê³ , ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ê°œì„ í•  ìˆ˜ ìˆëŠ” í‰ê°€ íŒŒì´í”„ë¼ì¸(Evaluation Pipeline)ì„ êµ¬ì¶•í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part3_dataset_design_md"
   },
   "source": [
    "#### 3.1 ë²¤ì¹˜ë§ˆí¬ ìƒì„±\n",
    "\n",
    "ë¨¼ì € Agent ì‹œìŠ¤í…œì˜ ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ `Langfuse Dataset`ì„ ì„¤ê³„í•©ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ ë‹¤ì–‘í•œ ì¸¡ë©´ì˜ ë¬¸ì œì™€, ê·¸ì— ëŒ€í•œ ëª¨ë²” ë‹µì•ˆìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "day6_define_dataset_code"
   },
   "outputs": [],
   "source": [
    "evaluation_benchmark_data = [\n",
    "    {\n",
    "        \"name\": \"Technical Comparison\",\n",
    "        \"input\": \"Apple Vision Proì™€ Meta Quest 3ì˜ ë””ìŠ¤í”Œë ˆì´ ê¸°ìˆ (í•´ìƒë„, PPD, íŒ¨ë„ íƒ€ì…)ì„ ë¹„êµí•˜ê³ , ê³µê°„ ì»´í“¨íŒ… ê²½í—˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì˜ ì°¨ì´ì ì„ ê¸°ìˆ ì ìœ¼ë¡œ ë¶„ì„í•˜ì‹œì˜¤.\",\n",
    "        \"expected_output\": \"Apple Vision ProëŠ” Micro-OLED íŒ¨ë„ì„ ì‚¬ìš©í•˜ì—¬ ì¸ì¹˜ë‹¹ 3,386 í”½ì…€ì˜ ì´ˆê³ í•´ìƒë„ë¥¼ ë‹¬ì„±, ì•½ 34-40 PPD(Pixels Per Degree)ë¥¼ ì œê³µí•˜ì—¬ í˜„ì‹¤ê³¼ ê±°ì˜ êµ¬ë¶„ì´ ë¶ˆê°€ëŠ¥í•œ ì„ ëª…í•¨ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ëŠ” í…ìŠ¤íŠ¸ ê°€ë…ì„±ì„ ê·¹ëŒ€í™”í•˜ê³  ì¥ì‹œê°„ ì‚¬ìš© ì‹œì˜ í”¼ë¡œë„ë¥¼ ì¤„ì—¬ì¤ë‹ˆë‹¤. ë°˜ë©´ Meta Quest 3ëŠ” LCD íŒ¨ë„ì„ ì‚¬ìš©í•˜ë©° ì•½ 25 PPDë¥¼ ì œê³µí•˜ì—¬ Vision Pro ëŒ€ë¹„ ì„ ëª…ë„ëŠ” ë–¨ì–´ì§€ì§€ë§Œ, ë” ë„“ì€ ì‹œì•¼ê°ê³¼ ë¹„ìš© íš¨ìœ¨ì„±ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤. ì´ ì°¨ì´ë¡œ ì¸í•´ Vision ProëŠ” 'ì‘ì—… ìƒì‚°ì„±'ê³¼ 'ë¯¸ë””ì–´ ì†Œë¹„'ì—, Quest 3ëŠ” 'ê²Œì„'ê³¼ 'ëŒ€ì¤‘ì  VR ê²½í—˜'ì— ë” ê°•ì ì„ ê°€ì§‘ë‹ˆë‹¤.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Creative Scenario\",\n",
    "        \"input\": \"ë¬¼ë¦¬ì  AI ì—ì´ì „íŠ¸(Embodied AI)ê°€ ë¯¸ë˜ì˜ ê°€ì • ë‚´ ë…¸ì¸ ëŒë´„(elderly care)ì„ ì–´ë–»ê²Œ í˜ì‹ í•  ìˆ˜ ìˆëŠ”ì§€, êµ¬ì²´ì ì¸ ê¸°ìˆ ê³¼ í•¨ê»˜ ì°½ì˜ì ì¸ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ 3ê°€ì§€ ì œì‹œí•˜ì‹œì˜¤.\",\n",
    "        \"expected_output\": \"1. ë™ë°˜ì ë° ê±´ê°• ëª¨ë‹ˆí„°ë§ ì—ì´ì „íŠ¸: SLAM ê¸°ìˆ ë¡œ ì§‘ì•ˆì„ ììœ ë¡­ê²Œ ì´ë™í•˜ë©°, ì»´í“¨í„° ë¹„ì „ìœ¼ë¡œ ë…¸ì¸ì˜ ì–¼êµ´ í‘œì •ê³¼ ê±¸ìŒê±¸ì´ë¥¼ ë¶„ì„í•˜ì—¬ ê±´ê°• ì´ìƒ ì§•í›„ë¥¼ ì¡°ê¸°ì— ë°œê²¬í•˜ê³ , ëŒ€í™”í˜• AIë¡œ ì •ì„œì  ì§€ì§€ë¥¼ ì œê³µí•©ë‹ˆë‹¤. 2. ë§ì¶¤í˜• ì¬í™œ ë³´ì¡° ì—ì´ì „íŠ¸: 3D ì„¼ì„œë¡œ ë…¸ì¸ì˜ ì›€ì§ì„ì„ ì •ë°€í•˜ê²Œ ì¶”ì í•˜ë©°, ì¬í™œ ìš´ë™ì˜ ì •í™•í•œ ìì„¸ë¥¼ ì½”ì¹­í•˜ê³  ê²Œì„í™”ëœ ì½˜í…ì¸ ë¡œ ì¬í™œì˜ ë™ê¸°ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤. 3. ì‘ê¸‰ ìƒí™© ëŒ€ì‘ ì—ì´ì „íŠ¸: ë‚™ìƒ ê°ì§€ ì„¼ì„œì™€ ìƒì²´ ì‹ í˜¸ ëª¨ë‹ˆí„°ë§ì„ í†µí•´ ì‘ê¸‰ ìƒí™© ë°œìƒ ì‹œ ì¦‰ì‹œ 119ì— ìë™ìœ¼ë¡œ ì‹ ê³ í•˜ê³ , ê°€ì¡±ì—ê²Œ ì˜ìƒ í†µí™”ë¥¼ ì—°ê²°í•˜ì—¬ ìƒí™©ì„ ì „ë‹¬í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Ethical Challenge Analysis\",\n",
    "        \"input\": \"'ìƒì‹œ ì‘ë™í•˜ëŠ”(always-on)' ê³µê°„ ì»´í“¨íŒ… ê¸°ê¸°ê°€ ëŒ€ì¤‘í™”ë  ë•Œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ê°€ì¥ ì‹¬ê°í•œ í”„ë¼ì´ë²„ì‹œ ë¬¸ì œ 3ê°€ì§€ë¥¼ ì‹ë³„í•˜ê³ , ì´ì— ëŒ€í•œ ì ì¬ì ì¸ ê¸°ìˆ ì /ì •ì±…ì  í•´ê²°ì±…ì„ ë…¼í•˜ì‹œì˜¤.\",\n",
    "        \"expected_output\": \"1. ê³µê°„ ë°ì´í„° ìœ ì¶œ: ê¸°ê¸°ê°€ ìˆ˜ì§‘í•˜ëŠ” 3D ê³µê°„ ì •ë³´(ì§‘ ë‚´ë¶€ êµ¬ì¡°, ë°©ë¬¸ê° ë“±)ê°€ í•´í‚¹ë˜ê±°ë‚˜ ì˜¤ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (í•´ê²°ì±…: ì˜¨ë””ë°”ì´ìŠ¤ AI ì²˜ë¦¬ ê°•í™”, ë°ì´í„° ìµœì†Œí™” ì›ì¹™ ì ìš©, ì—°í•© í•™ìŠµ ë„ì…). 2. ìƒì²´ ì •ë³´ì˜ ìƒì—…ì  ì´ìš©: ì‹œì„  ì¶”ì , í‘œì •, ë‡ŒíŒŒ(ë¯¸ë˜) ë°ì´í„°ê°€ ì‚¬ìš©ìì˜ ë™ì˜ ì—†ì´ ê´‘ê³ ë‚˜ ë§ˆì¼€íŒ…ì— í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (í•´ê²°ì±…: ê°•ë ¥í•œ ê°œì¸ì •ë³´ë³´í˜¸ë²•(GDPR ë“±) í™•ëŒ€ ì ìš©, ë°ì´í„° ì‚¬ìš©ì²˜ì— ëŒ€í•œ íˆ¬ëª…í•œ ê³µê°œ ì˜ë¬´í™”). 3. ì‚¬íšŒì  ê°ì‹œ ë° í†µì œ: ê³µê³µì¥ì†Œì—ì„œ íƒ€ì¸ì˜ í–‰ë™ê³¼ ëŒ€í™”ê°€ ë¬´ë¶„ë³„í•˜ê²Œ ê¸°ë¡ë˜ì–´ ì‚¬íšŒì  ìƒí˜¸ì‘ìš©ì„ ìœ„ì¶•ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (í•´ê²°ì±…: ë…¹í™” ì‹œ ëª…í™•í•œ ì‹œê°ì  í‘œì‹œ(LED ë“±) ì˜ë¬´í™”, íŠ¹ì • ì¥ì†Œ(í™”ì¥ì‹¤ ë“±)ì—ì„œì˜ ìë™ ë¹„í™œì„±í™” ê¸°ëŠ¥ íƒ‘ì¬, ê´€ë ¨ ë²•ê·œ ì œì •.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Business Strategy\",\n",
    "        \"input\": \"ì‚¼ì„±ì´ 2026ë…„ì— ì†Œë¹„ììš© AR ê¸€ë˜ìŠ¤ ì‹œì¥ì— ì§„ì¶œí•œë‹¤ê³  ê°€ì •í•  ë•Œ, Apple Vision Proì™€ ì°¨ë³„í™”í•˜ê¸° ìœ„í•œ ê°€ì¥ íš¨ê³¼ì ì¸ ì‹œì¥ ì§„ì… ì „ëµì€ ë¬´ì—‡ì¼ê¹Œ? ì œí’ˆ í¬ì§€ì…”ë‹ê³¼ í•µì‹¬ ê¸°ëŠ¥ ê´€ì ì—ì„œ ì œì•ˆí•˜ì‹œì˜¤.\",\n",
    "        \"expected_output\": \"ì‚¼ì„±ì€ Appleì˜ 'ìµœê³ ê¸‰ ê³µê°„ ì»´í“¨í„°' í¬ì§€ì…”ë‹ì„ í”¼í•˜ê³ , 'ì¼ìƒê³¼ ë§¤ë„ëŸ½ê²Œ í†µí•©ë˜ëŠ” AI ë¹„ì„œ'ë¡œ í¬ì§€ì…”ë‹í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ 1) ê²½ëŸ‰í™” ë° ë””ìì¸: í•˜ë£¨ ì¢…ì¼ ì°©ìš©í•´ë„ ë¶€ë‹´ ì—†ëŠ” ì¼ë°˜ ì•ˆê²½ í˜•íƒœì˜ ë””ìì¸ì„ ìµœìš°ì„ ìœ¼ë¡œ í•©ë‹ˆë‹¤. 2) ê°¤ëŸ­ì‹œ ìƒíƒœê³„ ì—°ë™: ìŠ¤ë§ˆíŠ¸í°, ì›Œì¹˜, ê°€ì „ì œí’ˆê³¼ ìœ ê¸°ì ìœ¼ë¡œ ì—°ë™í•˜ì—¬, AR ê¸€ë˜ìŠ¤ê°€ ëª¨ë“  ë””ë°”ì´ìŠ¤ë¥¼ ì œì–´í•˜ëŠ” 'ì»¨íŠ¸ë¡¤ íƒ€ì›Œ' ì—­í• ì„ ìˆ˜í–‰í•˜ê²Œ í•©ë‹ˆë‹¤. 3) ê°€ê²© ê²½ìŸë ¥: 100ë§Œì›ëŒ€ ì´ˆë°˜ì˜ ê³µê²©ì ì¸ ê°€ê²© ì •ì±…ìœ¼ë¡œ ì‹œì¥ ì§„ì… ì¥ë²½ì„ ë‚®ì¶° ëŒ€ì¤‘í™”ë¥¼ ì„ ë„í•˜ëŠ” ì „ëµì´ íš¨ê³¼ì ì¼ ê²ƒì…ë‹ˆë‹¤. í•µì‹¬ ê¸°ëŠ¥ì€ ê³ ì‚¬ì–‘ ê²Œì„ì´ ì•„ë‹Œ, ì‹¤ì‹œê°„ í†µì—­, ê¸¸ì•ˆë‚´, AI ë¹„ì„œ(ì˜¨ë””ë°”ì´ìŠ¤ 'ê°€ìš°ìŠ¤')ì™€ì˜ ìƒí˜¸ì‘ìš©ì— ì§‘ì¤‘í•´ì•¼ í•©ë‹ˆë‹¤.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\" ì´ {len(evaluation_benchmark_data)}ê°œì˜ í‰ê°€ ë¬¸í•­ì´ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part3_dataset_upload_md"
   },
   "source": [
    "#### 3.2 ë²¤ì¹˜ë§ˆí¬ ìƒì„±: Langfuse ë°ì´í„°ì…‹ ì—…ë¡œë“œ\n",
    "\n",
    "ì„¤ê³„í•œ í‰ê°€ ë¬¸í•­ë“¤ì„ `Langfuse` ì„œë²„ì— ë°ì´í„°ì…‹ìœ¼ë¡œ ì—…ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "day6_upload_dataset_code"
   },
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "try:\n",
    "    langfuse = Langfuse()\n",
    "    langfuse.auth_check()\n",
    "    print(\"Langfuse í´ë¼ì´ì–¸íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    DATASET_NAME = \"Sentient_Architect_Benchmark_v1\"\n",
    "\n",
    "    # ë°ì´í„°ì…‹ì´ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸\n",
    "    try:\n",
    "        dataset = langfuse.get_dataset(name=DATASET_NAME)\n",
    "        print(f\"ë°ì´í„°ì…‹ '{DATASET_NAME}'ì€(ëŠ”) ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ê¸°ì¡´ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "    except Exception:\n",
    "        # ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒˆë¡œ ìƒì„±\n",
    "        langfuse.create_dataset(name=DATASET_NAME)\n",
    "        print(f\"ìƒˆë¡œìš´ ë°ì´í„°ì…‹ '{DATASET_NAME}'ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # ë°ì´í„°ì…‹ ì•„ì´í…œ ì—…ë¡œë“œ\n",
    "    print(\"\\n--- ë°ì´í„°ì…‹ ì•„ì´í…œ ì—…ë¡œë“œ ì‹œì‘ ---\")\n",
    "    for item in evaluation_benchmark_data:\n",
    "        # ë™ì¼í•œ inputì„ ê°€ì§„ ì•„ì´í…œì´ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ ì¤‘ë³µ ë°©ì§€\n",
    "        existing_items = langfuse.get_dataset(DATASET_NAME).items\n",
    "        is_duplicate = any(i.input == item[\"input\"] for i in existing_items)\n",
    "\n",
    "        if not is_duplicate:\n",
    "            langfuse.create_dataset_item(\n",
    "                dataset_name=DATASET_NAME,\n",
    "                input=item[\"input\"],\n",
    "                expected_output=item[\"expected_output\"],\n",
    "                metadata={\"name\": item[\"name\"]},\n",
    "            )\n",
    "            print(f\"  - Item '{item['name']}' ì—…ë¡œë“œ ì„±ê³µ\")\n",
    "        else:\n",
    "            print(f\"  - Item '{item['name']}'ì€(ëŠ”) ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "\n",
    "    print(\"\\nëª¨ë“  í‰ê°€ ë¬¸í•­ì´ Langfuse ë°ì´í„°ì…‹ì— ì„±ê³µì ìœ¼ë¡œ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Langfuse ì—°ê²° ë˜ëŠ” ë°ì´í„°ì…‹ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "    print(\"   - Langfuse API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part3_llm_judge_md"
   },
   "source": [
    "#### 3.3 ìë™ ì±„ì ê´€ êµ¬í˜„: LLM-as-a-Judge\n",
    "\n",
    "ìš°ë¦¬ ì‹œìŠ¤í…œì´ ìƒì„±í•œ ë³µì¡í•œ ë³´ê³ ì„œë¥¼ ì¼ê´€ëœ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ê¸° ìœ„í•´, 'AI ì±„ì ê´€' ì—­í• ì„ í•  `LLM-as-a-Judge`ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ì´ ì±„ì ê´€ì€ ì„¸ ê°€ì§€ êµ¬ì²´ì ì¸ í‰ê°€ ê¸°ì¤€(ê¸°ìˆ ì  ê¹Šì´, ë…¼ë¦¬ì  ëª…í™•ì„±, ë…ì°½ì„±)ì— ëŒ€í•´ ê°ê° ì ìˆ˜ë¥¼ ë§¤ê¸°ê³ , ê·¸ ê·¼ê±°ë¥¼ ì„œìˆ í•˜ë„ë¡ ì„¤ê³„í•˜ì—¬ ë‹¨ìˆœí•œ ì ìˆ˜ ì´ìƒì˜ ê¹Šì´ ìˆëŠ” í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "day6_define_llm_judge_code"
   },
   "outputs": [],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from langfuse import Langfuse\n",
    "\n",
    "# í‰ê°€ì LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "try:\n",
    "    evaluator_llm_client = instructor.from_provider(\"google/gemini-2.5-flash\", api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "    print(\" LLM-as-a-Judgeë¥¼ ìœ„í•œ 'gemini-2.5-flash' í‰ê°€ì LLMì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\" í‰ê°€ì LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "    evaluator_llm_client = None\n",
    "\n",
    "\n",
    "# í‰ê°€ ê²°ê³¼ë¥¼ ë‹´ì„ Pydantic ëª¨ë¸\n",
    "class SubScore(BaseModel):\n",
    "    criteria: str = Field(description=\"í‰ê°€ ê¸°ì¤€\")\n",
    "    score: float = Field(description=\"í•´ë‹¹ ê¸°ì¤€ì— ëŒ€í•œ ì ìˆ˜ (0.0 ~ 1.0)\")\n",
    "    reasoning: str = Field(description=\"ì ìˆ˜ë¥¼ ë§¤ê¸´ êµ¬ì²´ì ì¸ ì´ìœ \")\n",
    "\n",
    "\n",
    "class ComprehensiveEvaluation(BaseModel):\n",
    "    \"\"\"AI Agentì˜ ë‹µë³€ì— ëŒ€í•œ ì¢…í•©ì ì¸ í‰ê°€ ê²°ê³¼\"\"\"\n",
    "\n",
    "    overall_score: float = Field(description=\"ëª¨ë“  ê¸°ì¤€ì„ ì¢…í•©í•œ ìµœì¢… í‰ê·  ì ìˆ˜ (0.0 ~ 1.0)\")\n",
    "    scores: List[SubScore] = Field(description=\"ì„¸ë¶€ í‰ê°€ ê¸°ì¤€ë³„ ì ìˆ˜ ë° ê·¼ê±°\")\n",
    "    overall_comment: str = Field(description=\"ë‹µë³€ì— ëŒ€í•œ ì´í‰ ë° ê°œì„  ì œì•ˆ\")\n",
    "\n",
    "\n",
    "# LLM-as-a-Judge í•¨ìˆ˜\n",
    "def evaluate_report_quality(\n",
    "    input_question: str, generated_report: str, expected_answer: str\n",
    ") -> ComprehensiveEvaluation:\n",
    "    \"\"\"LLMì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ ë³´ê³ ì„œì˜ í’ˆì§ˆì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if not evaluator_llm_client:\n",
    "        raise ConnectionError(\"í‰ê°€ì LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    prompt = f\"\"\"ë‹¹ì‹ ì€ ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ ê¸°ìˆ  ë¶„ì„ ë³´ê³ ì„œ í‰ê°€ ìœ„ì›ì…ë‹ˆë‹¤.\n",
    "    ë‹¤ìŒ [ì‚¬ìš©ì ì§ˆë¬¸], [AIê°€ ìƒì„±í•œ ë³´ê³ ì„œ], ê·¸ë¦¬ê³  [ëª¨ë²” ë‹µì•ˆ ì˜ˆì‹œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë³´ê³ ì„œì˜ í’ˆì§ˆì„ ì„¸ ê°€ì§€ ê¸°ì¤€ì— ë”°ë¼ ì—„ê²©í•˜ê³  ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "    [í‰ê°€ ê¸°ì¤€]\n",
    "    1.  Technical_Depth (ê¸°ìˆ ì  ê¹Šì´): ê¸°ìˆ ì  ê°œë…ì„ ì •í™•í•˜ê³  ê¹Šì´ ìˆê²Œ ë‹¤ë£¨ì—ˆëŠ”ê°€? í•µì‹¬ì ì¸ ê¸°ìˆ  ìš”ì†Œë¥¼ ë†“ì¹˜ì§€ ì•Šì•˜ëŠ”ê°€?\n",
    "    2.  Logical_Clarity (ë…¼ë¦¬ì  ëª…í™•ì„±): ë³´ê³ ì„œì˜ êµ¬ì¡°ê°€ ë…¼ë¦¬ì ì´ë©°, ì£¼ì¥ì´ ëª…í™•í•˜ê³ , ê·¼ê±°ê°€ íƒ€ë‹¹í•œê°€? ë…ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?\n",
    "    3.  Insightfulness (ë…ì°½ì„± ë° í†µì°°ë ¥): ë‹¨ìˆœíˆ ì •ë³´ë¥¼ ë‚˜ì—´í•˜ëŠ” ê²ƒì„ ë„˜ì–´, ë…ì°½ì ì¸ ë¶„ì„ì´ë‚˜ ê¹Šì´ ìˆëŠ” í†µì°°ë ¥ì„ ì œê³µí•˜ëŠ”ê°€? ëª¨ë²” ë‹µì•ˆì˜ í•µì‹¬ì„ ì˜ íŒŒì•…í–ˆëŠ”ê°€?\n",
    "\n",
    "    --- START OF DATA ---\n",
    "    [ì‚¬ìš©ì ì§ˆë¬¸]: {input_question}\n",
    "\n",
    "    [AIê°€ ìƒì„±í•œ ë³´ê³ ì„œ]:\\n{generated_report}\n",
    "\n",
    "    [ëª¨ë²” ë‹µì•ˆ ì˜ˆì‹œ]:\\n{expected_answer}\n",
    "    --- END OF DATA ---\n",
    "\n",
    "    ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ComprehensiveEvaluation JSON ê°ì²´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.\n",
    "    \"\"\"\n",
    "\n",
    "    evaluation_result = evaluator_llm_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt.format(\n",
    "                    input_question=input_question, generated_report=generated_report, expected_answer=expected_answer\n",
    "                ),\n",
    "            }\n",
    "        ],\n",
    "        response_model=ComprehensiveEvaluation,\n",
    "    )\n",
    "    return evaluation_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part3_pipeline_md"
   },
   "source": [
    "#### 3.4 í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "\n",
    "ì´ì œ ëª¨ë“  ì¤€ë¹„ê°€ ëë‚¬ìŠµë‹ˆë‹¤. ì•„ë˜ ì½”ë“œëŠ” 'í‘œì¤€ ì‹œí—˜ì§€(Dataset)'ì˜ ê° ë¬¸í•­ì„ ìš°ë¦¬ ì‹œìŠ¤í…œì— í•˜ë‚˜ì”© í’€ê²Œ í•˜ê³ , ê·¸ ê²°ê³¼ë¬¼ì„ 'AI ì±„ì ê´€(LLM-as-a-Judge)'ì´ ì±„ì í•œ ë’¤, ëª¨ë“  ê³¼ì •ì„ `Langfuse`ì— ìë™ìœ¼ë¡œ ê¸°ë¡í•˜ëŠ” ì „ì²´ í‰ê°€ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "day6_run_evaluation_pipeline_code"
   },
   "outputs": [],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import time\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "import uuid\n",
    "from langfuse import Langfuse\n",
    "import os\n",
    "\n",
    "# í‰ê°€ì LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "try:\n",
    "    client = instructor.from_provider(\"google/gemini-2.5-flash\", api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "    print(\" LLM-as-a-Judge í‰ê°€ì ì¤€ë¹„ ì™„ë£Œ\")\n",
    "except Exception as e:\n",
    "    print(f\" í‰ê°€ì LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "    client = None\n",
    "\n",
    "\n",
    "# í‰ê°€ ê²°ê³¼ ëª¨ë¸\n",
    "class Score(BaseModel):\n",
    "    technical: float = Field(description=\"Technical accuracy score 0.0-1.0\")\n",
    "    clarity: float = Field(description=\"Logical clarity score 0.0-1.0\")\n",
    "    insight: float = Field(description=\"Insightfulness score 0.0-1.0\")\n",
    "    comment: str = Field(description=\"Brief evaluation comment\")\n",
    "\n",
    "\n",
    "def evaluate_report_quality(question: str, report: str, expected: str) -> Score:\n",
    "    \"\"\"LLMì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ ë³´ê³ ì„œì˜ í’ˆì§ˆì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if not client:\n",
    "        return Score(technical=0.5, clarity=0.5, insight=0.5, comment=\"Client not available\")\n",
    "\n",
    "    prompt = f\"\"\"Evaluate this technical report:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Generated Report: {report}\n",
    "\n",
    "Reference Answer: {expected}\n",
    "\n",
    "Rate on 3 criteria (0.0-1.0 scale):\n",
    "- Technical accuracy and depth\n",
    "- Logical clarity and structure\n",
    "- Insightfulness and creativity\n",
    "\n",
    "Provide scores and brief comment.\"\"\"\n",
    "\n",
    "    try:\n",
    "        evaluation = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}], response_model=Score, max_retries=3\n",
    "        )\n",
    "        return evaluation\n",
    "\n",
    "    except Exception as e:\n",
    "        return Score(technical=0.5, clarity=0.5, insight=0.5, comment=f\"Evaluation error: {type(e).__name__}\")\n",
    "\n",
    "\n",
    "def run_evaluation_pipeline():\n",
    "    \"\"\"ì™„ì „ ìë™í™”ëœ í‰ê°€ íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "    if \"a2a_url\" not in globals() or not globals()[\"a2a_url\"]:\n",
    "        print(\" A2A ê²Œì´íŠ¸ì›¨ì´ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        langfuse = Langfuse()\n",
    "        dataset = langfuse.get_dataset(name=\"Sentient_Architect_Benchmark_v1\")\n",
    "        print(f\" ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(dataset.items)}ê°œ í•­ëª©\")\n",
    "    except Exception as e:\n",
    "        print(f\" ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        return\n",
    "\n",
    "    headers = {\n",
    "        \"X-API-Key\": os.environ[\"MASTER_API_KEY\"],\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    print(\"--- ğŸš€ í‰ê°€ ì‹œì‘ ---\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for idx, item in enumerate(tqdm(dataset.items, desc=\"í‰ê°€ ì§„í–‰\")):\n",
    "        session_id = f\"eval-{uuid.uuid4().hex[:8]}\"\n",
    "        print(f\"\\n[{idx+1}/{len(dataset.items)}] {item.metadata['name']}\")\n",
    "\n",
    "        with item.run(run_name=f\"eval-{item.metadata['name']}-{session_id}\") as run_manager:\n",
    "            try:\n",
    "                # 1. ë³´ê³ ì„œ ìƒì„±\n",
    "                response = requests.post(\n",
    "                    f\"{a2a_url}/api/v1/generate-deep-research-report\",\n",
    "                    json={\"session_id\": session_id, \"user_input\": item.input},\n",
    "                    headers=headers,\n",
    "                    timeout=600,\n",
    "                )\n",
    "\n",
    "                if response.status_code != 200:\n",
    "                    raise Exception(f\"API ì˜¤ë¥˜: {response.status_code} - {response.text}\")\n",
    "\n",
    "                report = response.json().get(\"final_report\", \"\")\n",
    "                if not report:\n",
    "                    raise Exception(\"ë¹ˆ ë³´ê³ ì„œ ìˆ˜ì‹ \")\n",
    "\n",
    "                print(f\"   ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ ({len(report)}ì)\")\n",
    "                run_manager.update(output=report)\n",
    "\n",
    "                # 2. í‰ê°€ ìˆ˜í–‰\n",
    "                score = evaluate_report_quality(question=item.input, report=report, expected=item.expected_output)\n",
    "\n",
    "                # 3. ì ìˆ˜ ê¸°ë¡\n",
    "                overall = (score.technical + score.clarity + score.insight) / 3\n",
    "\n",
    "                run_manager.score(name=\"Overall_Score\", value=overall, comment=score.comment)\n",
    "                run_manager.score(name=\"Technical_Accuracy\", value=score.technical)\n",
    "                run_manager.score(name=\"Logical_Clarity\", value=score.clarity)\n",
    "                run_manager.score(name=\"Insightfulness\", value=score.insight)\n",
    "\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"name\": item.metadata[\"name\"],\n",
    "                        \"overall\": overall,\n",
    "                        \"technical\": score.technical,\n",
    "                        \"clarity\": score.clarity,\n",
    "                        \"insight\": score.insight,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"   í‰ê°€ ì™„ë£Œ: ì „ì²´ {overall:.2f} (ê¸°ìˆ {score.technical:.2f}/ë…¼ë¦¬{score.clarity:.2f}/í†µì°°{score.insight:.2f})\"\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
    "                run_manager.update(output={\"error\": str(e)}, level=\"ERROR\")\n",
    "                run_manager.score(name=\"Overall_Score\", value=0.0, comment=f\"ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "                results.append(\n",
    "                    {\"name\": item.metadata[\"name\"], \"overall\": 0.0, \"technical\": 0.0, \"clarity\": 0.0, \"insight\": 0.0}\n",
    "                )\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    langfuse.flush()\n",
    "\n",
    "    # ê²°ê³¼ ìš”ì•½\n",
    "    print(\"\\n--- ğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼ ---\")\n",
    "    total_overall = 0\n",
    "    for result in results:\n",
    "        print(\n",
    "            f\"{result['name']:25} | ì „ì²´: {result['overall']:.2f} | ê¸°ìˆ : {result['technical']:.2f} | ë…¼ë¦¬: {result['clarity']:.2f} | í†µì°°: {result['insight']:.2f}\"\n",
    "        )\n",
    "        total_overall += result[\"overall\"]\n",
    "\n",
    "    avg_score = total_overall / len(results) if results else 0\n",
    "    print(f\"\\ní‰ê·  ì ìˆ˜: {avg_score:.2f}\")\n",
    "    print(\"---  í‰ê°€ ì™„ë£Œ! Langfuseì—ì„œ ìƒì„¸ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”. ---\")\n",
    "\n",
    "\n",
    "# ì‹¤í–‰\n",
    "run_evaluation_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part4_intro_md"
   },
   "source": [
    "### Part 4: ì‹¬ì¸µ ë¶„ì„ ë° ê°œì„ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "day6_part4_analysis_guide_md"
   },
   "source": [
    "#### Langfuse ëŒ€ì‹œë³´ë“œ íƒí—˜ ê³¼ì œ (Action Item)\n",
    "\n",
    "1.  Langfuse Cloud ([https://cloud.langfuse.com/](https://cloud.langfuse.com/)) ë¡œ ì´ë™í•˜ì—¬ ì—¬ëŸ¬ë¶„ì˜ í”„ë¡œì íŠ¸ì— ì ‘ì†í•˜ì„¸ìš”.\n",
    "\n",
    "2.  Dataset ìƒì„¸ ë¶„ì„ í˜ì´ì§€ë¡œ ì´ë™:\n",
    "    - ì™¼ìª½ ë©”ë‰´ì—ì„œ 'Datasets'ë¥¼ í´ë¦­í•˜ê³ , `Sentient_Architect_Benchmark_v1` ë°ì´í„°ì…‹ì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "    - 'Runs' íƒ­ì„ í´ë¦­í•˜ì—¬ Part 3ì—ì„œ ì‹¤í–‰í•œ í‰ê°€ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "3.  ì¢…í•© ì„±ëŠ¥ ë¶„ì„:\n",
    "    - í‰ê·  ì ìˆ˜ í™•ì¸: í˜ì´ì§€ ìƒë‹¨ì— í‘œì‹œëœ 'Average Scores'ë¥¼ í™•ì¸í•˜ì„¸ìš”. 'Overall'ì˜ í‰ê·  ì ìˆ˜ëŠ” ëª‡ ì ì¸ê°€ìš”? 'Technical', 'Logical', 'Insightfulness' ì¤‘ ê°€ì¥ ì ìˆ˜ê°€ ë‚®ê²Œ ë‚˜ì˜¨ í•­ëª©ì€ ë¬´ì—‡ì¸ê°€ìš”? \n",
    "    - ì‹¤í–‰ ì‹œê°„ ë° ë¹„ìš© ë¶„ì„: ê° ì‹¤í–‰(Run)ë³„ 'Latency' ì™€ 'Total Cost' ë¥¼ ë¹„êµí•´ë³´ì„¸ìš”. íŠ¹ì • ìœ í˜•ì˜ ì§ˆë¬¸(ì˜ˆ: 'Ethical Challenge')ì„ ì²˜ë¦¬í•˜ëŠ” ë° ìœ ë… ë§ì€ ì‹œê°„ì´ë‚˜ ë¹„ìš©ì´ ì†Œìš”ë˜ì§€ëŠ” ì•Šì•˜ë‚˜ìš”?\n",
    "\n",
    "4.  ì‹¤íŒ¨ ì‚¬ë¡€(Worst-Case) ì‹¬ì¸µ ë¶„ì„:\n",
    "    - 'Overall' ì ìˆ˜ê°€ ê°€ì¥ ë‚®ê²Œ ë‚˜ì˜¨ ì‹¤í–‰(Run)ì„ ì°¾ìœ¼ì„¸ìš”.\n",
    "    - í•´ë‹¹ ì‹¤í–‰ì˜ Trace ì•„ì´ì½˜ì„ í´ë¦­í•˜ì—¬ ì—”ë“œíˆ¬ì—”ë“œ Trace ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤.\n",
    "    - ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ íë¦„ ê²€í† : `LangGraph` ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ê°€ ê° ì „ë¬¸ê°€ íŒ€ì„ ì–´ë–»ê²Œ í˜¸ì¶œí–ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "    - CrewAI ë‚´ë¶€ ë™ì‘ ë¶„ì„: `crewai-research-team-execution` Spanì„ í™•ì¥í•˜ì—¬, ë¦¬ì„œì¹˜ íŒ€ ë‚´ë¶€ì˜ ì–´ë–¤ Agentê°€ ì–´ë–¤ Toolì„ í˜¸ì¶œí–ˆê³ , ì–´ë–¤ ê²°ê³¼ë¥¼ ì–»ì—ˆëŠ”ì§€ ì¶”ì í•©ë‹ˆë‹¤. (ì˜ˆ: `Tavily` ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì‹¤í•˜ì§€ëŠ” ì•Šì•˜ëŠ”ê°€?)\n",
    "    - ADK í¸ì§‘ ê³¼ì • ê²€í† : `ADK quality control` Spanì„ í™•ì¥í•˜ì—¬, ìµœì¢… í¸ì§‘ ë‹¨ê³„ì—ì„œ ì–´ë–¤ í”„ë¡¬í”„íŠ¸ê°€ ì‚¬ìš©ë˜ì—ˆê³ , ì´ˆì•ˆ ëŒ€ë¹„ ì–´ë–¤ ë¶€ë¶„ì´ í¬ê²Œ ìˆ˜ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "    - ì ìˆ˜ ê·¼ê±° í™•ì¸: Traceì˜ 'Scores' íƒ­ì—ì„œ 'AI ì±„ì ê´€'ì´ ì™œ ë‚®ì€ ì ìˆ˜ë¥¼ ì£¼ì—ˆëŠ”ì§€ì— ëŒ€í•œ 'Comment'ë¥¼ ê¼¼ê¼¼íˆ ì½ì–´ë³´ê³ , ì‹¤íŒ¨ì˜ ê·¼ë³¸ì ì¸ ì›ì¸ì„ íŒŒì•…í•©ë‹ˆë‹¤.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
