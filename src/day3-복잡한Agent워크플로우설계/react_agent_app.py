# streamlit run react_agent_app.py

import collections.abc
import json
import os
import re
import urllib.parse
import urllib.request
from typing import Any, Dict, List, Literal, Type

import instructor
import streamlit as st
import wolframalpha
from dotenv import load_dotenv
from langchain.tools import BaseTool, tool
from langchain.tools.render import render_text_description
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from pydantic import BaseModel, Field, PrivateAttr

st.set_page_config(page_title="ReAct Agent", page_icon="ğŸ¤–", layout="wide")

# --- ì´ˆê¸° ì„¤ì • ---

load_dotenv()

google_api_key = os.getenv("GOOGLE_API_KEY")
tavily_api_key = os.getenv("TAVILY_API_KEY")
wolfram_app_id = os.getenv("WOLFRAM_ALPHA_APP_ID")

if not all([google_api_key, tavily_api_key, wolfram_app_id]):
    st.sidebar.title("ğŸ”‘ API í‚¤ ì„¤ì •")
    google_api_key = st.sidebar.text_input("Google API Key", type="password", key="google_api_key") or google_api_key
    tavily_api_key = st.sidebar.text_input("Tavily API Key", type="password", key="tavily_api_key") or tavily_api_key
    wolfram_app_id = (
        st.sidebar.text_input("WolframAlpha App ID", type="password", key="wolfram_app_id") or wolfram_app_id
    )

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "messages" not in st.session_state:
    st.session_state.messages = []
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []


# --- 2. Agent êµ¬ì„± ìš”ì†Œ (Tool, LLM) ì •ì˜ ---
@st.cache_resource
def get_tools():
    # --- Tool 1: ì›¹ ê²€ìƒ‰ ---
    search_tool = TavilySearchResults(max_results=3, name="tavily_search_results_json")

    # --- Tool 2: ê°„ë‹¨í•œ ê³„ì‚°ê¸° ---
    class CalculatorInput(BaseModel):
        expression: str = Field(description="í‰ê°€í•  ìˆ˜í•™ì  í‘œí˜„ì‹")

    @tool(args_schema=CalculatorInput)
    def calculator_tool(expression: str) -> str:
        """ê°„ë‹¨í•œ ì‚¬ì¹™ì—°ì‚°ì´ë‚˜ ìˆ«ì ê³„ì‚°ì—ë§Œ ì‚¬ìš©í•˜ì„¸ìš”."""
        try:
            return str(eval(expression))
        except Exception as e:
            return f"ê³„ì‚° ì˜¤ë¥˜: {e}"

    # --- Tool 3: ë³µí•© ê³¼í•™/ìˆ˜í•™ ì—”ì§„ (WolframAlpha LLM API) ---
    class WolframAlphaInput(BaseModel):
        query: str = Field(description="WolframAlphaì— ë³´ë‚¼ ë³µì¡í•œ ì§ˆë¬¸ì´ë‚˜ ìˆ˜í•™/ê³¼í•™ ìˆ˜ì‹")

    @tool(args_schema=WolframAlphaInput)
    def wolfram_alpha_tool(query: str) -> str:
        """ë³µì¡í•œ ìˆ˜í•™ ë¬¸ì œ, ë°©ì •ì‹, ë¯¸ì ë¶„, í™”í•™ì‹, ë¬¼ë¦¬ ê³µì‹, ë‹¨ìœ„ ë³€í™˜ ë“±ì— ì‚¬ìš©í•©ë‹ˆë‹¤."""
        try:
            import urllib.parse
            import urllib.request

            api_key = os.environ.get("WOLFRAM_ALPHA_APP_ID")
            if not api_key:
                return "API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

            # LLM API ì‚¬ìš© (í…ìŠ¤íŠ¸ ê²°ê³¼ ë°˜í™˜)
            base_url = "https://www.wolframalpha.com/api/v1/llm-api"

            # URL íŒŒë¼ë¯¸í„° êµ¬ì„±
            params = {"input": query, "appid": api_key, "maxchars": 2000}  # ì‘ë‹µ ê¸¸ì´ ì œí•œ

            # URL ì¸ì½”ë”©
            encoded_params = urllib.parse.urlencode(params)
            full_url = f"{base_url}?{encoded_params}"

            print(f"ë””ë²„ê¹…: ìš”ì²­ URL: {full_url}")

            # ìš”ì²­ ì‹¤í–‰
            with urllib.request.urlopen(full_url) as response:
                result = response.read().decode("utf-8")
                print(f"ë””ë²„ê¹…: LLM API ì‘ë‹µ ì„±ê³µ")

                # ê²°ê³¼ì—ì„œ í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œ
                if "Result:" in result:
                    # "Result:" ë‹¤ìŒ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                    result_section = result.split("Result:")[1].split("\n")[0:3]
                    clean_result = "Result: " + "\n".join(result_section)
                    return clean_result.strip()

                return result[:500] + "..." if len(result) > 500 else result

        except urllib.error.HTTPError as e:
            error_code = e.code
            error_msg = e.read().decode("utf-8") if hasattr(e, "read") else str(e)

            if error_code == 501:
                return f"WolframAlphaê°€ ì¿¼ë¦¬ë¥¼ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: '{query}'. ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ì‹œë„í•´ë³´ì„¸ìš”."
            elif error_code == 403:
                return "API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤."
            else:
                return f"HTTP ì˜¤ë¥˜ {error_code}: {error_msg}"

        except Exception as e:
            return f"WolframAlpha ì˜¤ë¥˜: {str(e)}"

    # --- Tool 4: ë¸”ë¡œê·¸ í…œí”Œë¦¿ ---
    class BlogTemplateInput(BaseModel):
        style: str = Field(
            description="ë¸”ë¡œê·¸ ê¸€ì˜ ê¸°ë³¸ êµ¬ì¡°(í…œí”Œë¦¿)ë¥¼ ìƒì„±í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. 'ê¸°ìˆ  ë¶„ì„' ë˜ëŠ” 'ì œí’ˆ ë¦¬ë·°' ìŠ¤íƒ€ì¼ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”."
        )

    @tool(args_schema=BlogTemplateInput)
    def blog_template_tool(style: str) -> str:
        """ë¸”ë¡œê·¸ ê¸€ì˜ ê¸°ë³¸ êµ¬ì¡°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if style == "ê¸°ìˆ  ë¶„ì„":
            return "## ì œëª©\\n\\n### 1. ê¸°ìˆ  ê°œìš”\\n\\n### 2. í•µì‹¬ ì‘ë™ ì›ë¦¬\\n\\n### 3. ì¥ë‹¨ì  ë¶„ì„\\n\\n### 4. ì‹¤ë¬´ ì ìš© ì‚¬ë¡€\\n\\n### 5. ê²°ë¡  ë° í–¥í›„ ì „ë§"
        elif style == "ì œí’ˆ ë¦¬ë·°":
            return "## ì œëª©\\n\\n### 1. ì²«ì¸ìƒ ë° ë””ìì¸\\n\\n### 2. ì£¼ìš” ê¸°ëŠ¥ ë° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\\n\\n### 3. ì‹¤ì‚¬ìš© í›„ê¸°\\n\\n### 4. ì´í‰ ë° ì¶”ì²œ ëŒ€ìƒ"
        else:
            return "ì˜¤ë¥˜: 'ê¸°ìˆ  ë¶„ì„' ë˜ëŠ” 'ì œí’ˆ ë¦¬ë·°' ìŠ¤íƒ€ì¼ë§Œ ì§€ì›í•©ë‹ˆë‹¤."

    # --- Tool 5: ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸í•˜ê¸° ---
    class QuestionInput(BaseModel):
        question: str = Field(description="ì‚¬ìš©ìì—ê²Œ ë¬¼ì–´ë³¼ ì§ˆë¬¸")

    @tool(args_schema=QuestionInput)
    def ask_user_tool(question: str) -> str:
        """ì‚¬ìš©ìì—ê²Œ ì¶”ê°€ ì •ë³´ë¥¼ ì§ˆë¬¸í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. ê³„íš ìˆ˜ë¦½ì´ë‚˜ ë§ì¶¤ ì¡°ì–¸ì„ ìœ„í•´ í•„ìš”í•œ ì •ë³´ë¥¼ ì–»ìŠµë‹ˆë‹¤."""
        return f"ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸: {question}"

    # --- Tool 6: ë§ì¶¤í˜• ê³„íš ìƒì„± ---
    class PlanningInput(BaseModel):
        user_info: str = Field(description="ì‚¬ìš©ì ì •ë³´ë‚˜ ìƒí™©, ë‹¬ì„±í•˜ê³ ì í•˜ëŠ” ëª©í‘œ")

    @tool(args_schema=PlanningInput)
    def create_study_plan_tool(user_info: str) -> str:
        """ì‚¬ìš©ì ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§ì¶¤í˜• í•™ìŠµ ê³„íšì„ ìƒì„±í•©ë‹ˆë‹¤."""

        # ê°„ë‹¨í•œ ê³„íš ìƒì„± ë¡œì§
        if "ìˆ˜ëŠ¥" in user_info.lower() or "ìˆ˜í•™" in user_info.lower():
            return f"""
    ## ğŸ“š ë§ì¶¤í˜• ìˆ˜ëŠ¥ ìˆ˜í•™ í•™ìŠµ ê³„íš

    **ì‚¬ìš©ì í˜„í™©:** {user_info}

    ### 1ë‹¨ê³„: í˜„ì¬ ì‹¤ë ¥ ì ê²€ (1ì£¼ì°¨)
    - ê¸°ì¶œë¬¸ì œ 3ê°œë…„ í’€ì–´ë³´ê¸°
    - ì•½ì  ì˜ì—­ íŒŒì•…

    ### 2ë‹¨ê³„: ê°œë… ì •ë¦¬ (2-4ì£¼ì°¨)
    - ë¶€ì¡±í•œ ë‹¨ì› ì§‘ì¤‘ í•™ìŠµ
    - ê³µì‹ ì•”ê¸° ë° ì´í•´

    ### 3ë‹¨ê³„: ë¬¸ì œ ìœ í˜•ë³„ ì—°ìŠµ (5-8ì£¼ì°¨)
    - í‚¬ëŸ¬ ë¬¸ì œ ìœ í˜• ë¶„ì„
    - ì‹œê°„ ë‹¨ì¶• ì—°ìŠµ

    ### 4ë‹¨ê³„: ì‹¤ì „ ëª¨ì˜ê³ ì‚¬ (9-12ì£¼ì°¨)
    - ì£¼ 2íšŒ ëª¨ì˜ê³ ì‚¬
    - ì˜¤ë‹µ ë…¸íŠ¸ ì‘ì„±
            """
        elif "ë¸”ë¡œê·¸" in user_info.lower():
            return f"""
    ## âœï¸ ë§ì¶¤í˜• ë¸”ë¡œê·¸ ì‹œì‘ ê³„íš

    **ì‚¬ìš©ì í˜„í™©:** {user_info}

    ### 1ë‹¨ê³„: ì£¼ì œ ë° íƒ€ê²Ÿ ë…ì ì„¤ì •
    - ê´€ì‹¬ ë¶„ì•¼ì™€ ì „ë¬¸ì„± ì—°ê²°
    - ë…ìì¸µ ëª…í™•í™”

    ### 2ë‹¨ê³„: ì½˜í…ì¸  ì „ëµ ìˆ˜ë¦½
    - í¬ìŠ¤íŒ… ì£¼ê¸° ê²°ì •
    - ì½˜í…ì¸  ìº˜ë¦°ë” ì‘ì„±

    ### 3ë‹¨ê³„: ì²« í¬ìŠ¤íŒ… ë°œí–‰
    - ìê¸°ì†Œê°œ ê¸€ ì‘ì„±
    - SEO ìµœì í™” ì ìš©
            """

    tools = [
        search_tool,
        calculator_tool,
        wolfram_alpha_tool,
        blog_template_tool,
        ask_user_tool,
        create_study_plan_tool,
    ]
    return tools


@st.cache_resource
def get_llm_and_template(_google_api_key, _tools):
    os.environ["GOOGLE_API_KEY"] = _google_api_key
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0)
    structured_llm = instructor.from_provider("google/gemini-2.5-flash")
    rendered_tools = render_text_description(_tools)
    action_schema = json.dumps(Action.model_json_schema(), indent=2).replace("{", "{{").replace("}", "}}")
    template = f"""
    # MISSION
    ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ë³µì¡í•œ ìš”êµ¬ì‚¬í•­ì„ í•´ê²°í•˜ëŠ” ìµœìƒìœ„ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    ë‹¹ì‹ ì€ 'Thought'ì™€ 'Action'ì„ ë°˜ë³µí•˜ëŠ” ReAct íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì œë¥¼ ë‹¨ê³„ì ìœ¼ë¡œ í•´ê²°í•´ì•¼ í•©ë‹ˆë‹¤.

    # TOOLS
    ë‹¤ìŒì€ ë‹¹ì‹ ì´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë„êµ¬ ëª©ë¡ì…ë‹ˆë‹¤:
    --- TOOLS ---
    {rendered_tools}
    --- END TOOLS ---

    # OUTPUT FORMAT
    ë‹¹ì‹ ì˜ ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ ì„¸ ê°€ì§€ í‚¤ë¥¼ í¬í•¨í•œ JSON ê°ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤:
    1. 'thought': í˜„ì¬ ìƒí™© ë¶„ì„ê³¼ ë‹¤ìŒ í–‰ë™ ê³„íšì„ ì„œìˆ í•˜ëŠ” ë¬¸ìì—´
    2. 'tool': ì‚¬ìš©í•  ë„êµ¬ì˜ ì´ë¦„ ë˜ëŠ” 'Final Answer'
    3. 'tool_input': ì„ íƒëœ ë„êµ¬ì— ì „ë‹¬í•  ì…ë ¥ê°’

    --- ACTION SCHEMA ---
    {action_schema}
    --- END ACTION SCHEMA ---

    # WORKFLOW
    1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ë¶„ì„í•˜ì—¬ í˜„ì¬ ìƒí™©ì„ íŒŒì•…í•©ë‹ˆë‹¤.
    2. 'thought'ì— ë‹¤ìŒ í–‰ë™ ê³„íšì„ ìƒì„¸íˆ ì„œìˆ í•©ë‹ˆë‹¤.
    3. ê³„íšì— ê°€ì¥ ì í•©í•œ ë„êµ¬ì™€ ì…ë ¥ê°’ì„ ê²°ì •í•©ë‹ˆë‹¤.
    4. ë§Œì•½ ëª¨ë“  ì •ë³´ê°€ ìˆ˜ì§‘ë˜ì–´ ìµœì¢… ë‹µë³€ì„ í•  ìˆ˜ ìˆë‹¤ë©´, 'tool'ë¡œ 'Final Answer'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

    --- CONTEXT ---
    ì´ì „ ëŒ€í™” ê¸°ë¡:
    {{chat_history}}

    ì´ì „ í–‰ë™ ë° ê´€ì°° ê¸°ë¡:
    {{intermediate_steps}}

    ì‚¬ìš©ì ì§ˆë¬¸: {{user_query}}
    """
    prompt_template = ChatPromptTemplate.from_template(template)
    return llm, structured_llm, prompt_template


class Action(BaseModel):
    thought: str = Field(description="í˜„ì¬ ìƒí™© ë¶„ì„ê³¼ ë‹¤ìŒ í–‰ë™ ê³„íš")

    tool: Literal[
        "tavily_search_results_json",
        "calculator_tool",
        "wolfram_alpha_tool",
        "blog_template_tool",
        "ask_user_tool",
        "create_study_plan_tool",
        "Final Answer",
    ] = Field(description="ì‚¬ìš©í•  Toolì˜ ì´ë¦„ ë˜ëŠ” ìµœì¢… ë‹µë³€ì„ ìœ„í•œ 'Final Answer'")

    tool_input: Any = Field(description="ì„ íƒëœ Toolì— ì „ë‹¬í•  ì…ë ¥ê°’. ë¬¸ìì—´ ë˜ëŠ” JSON ê°ì²´ í˜•íƒœê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


# --- 3. ReAct ì—”ì§„ í•¨ìˆ˜ ---
def run_structured_react_engine(user_query: str, chat_history: list, _tools, _structured_llm, _prompt_template):
    tool_map = {tool.name: tool for tool in _tools}

    history_str = "\n".join([f"Human: {h[0]}\nAssistant: {h[1]}" for h in chat_history])
    intermediate_steps_str = ""

    # ììœ¨ì  ë£¨í”„ ì‹œì‘
    for i in range(10):  # ìµœëŒ€ 10ë²ˆì˜ ë°˜ë³µìœ¼ë¡œ ì•ˆì „ì¥ì¹˜ ì„¤ì •
        # ì‚¬ì´ë“œë°”ì— ì§„í–‰ ìƒí™© í‘œì‹œ
        with st.sidebar:
            st.write(f"ğŸ”„ ReAct Loop: Iteration {i+1}")

        # 1. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = _prompt_template.format(
            user_query=user_query, chat_history=history_str, intermediate_steps=intermediate_steps_str
        )

        # 2. instructorë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„±
        try:
            response_object = _structured_llm.chat.completions.create(
                model="gemini-2.5-flash", response_model=Action, messages=[{"role": "user", "content": prompt}]
            )

            thought = response_object.thought
            action_obj = response_object

            # ì‚¬ì´ë“œë°”ì— Thought í‘œì‹œ
            with st.sidebar:
                st.write(f"ğŸ¤” **Thought:** {thought[:1000]}...")
                st.write(f"ğŸ¬ **Action:** {action_obj.tool}")

            # 3. ìµœì¢… ë‹µë³€ì¸ì§€ í™•ì¸
            if action_obj.tool == "Final Answer":
                return action_obj.tool_input

            # 4. ask_user_tool íŠ¹ë³„ ì²˜ë¦¬
            if action_obj.tool == "ask_user_tool":
                question = action_obj.tool_input
                # ì§ˆë¬¸ì„ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì£¼ê³  ReAct ë£¨í”„ ì¤‘ë‹¨
                return f"ğŸ’¬ **ì§ˆë¬¸**: {question}\n\nìœ„ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì‹œë©´, ê·¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§ì¶¤í˜• ê³„íšì„ ì„¸ì›Œë“œë¦¬ê² ìŠµë‹ˆë‹¤!"

            # 5. Tool ì‹¤í–‰
            if action_obj.tool in tool_map:
                tool_to_use = tool_map[action_obj.tool]
                tool_input = action_obj.tool_input

                with st.sidebar.expander(f"ğŸ¬ **Action:** {action_obj.tool}", expanded=False):
                    st.text(f"ì…ë ¥: {tool_input}")

                try:
                    observation = tool_to_use.invoke(tool_input)
                    # ì‚¬ì´ë“œë°”ì— ê´€ì°° ê²°ê³¼ í‘œì‹œ
                    with st.sidebar:
                        st.write(f"ğŸ‘€ **ê´€ì°°:** {str(observation)[:1000]}...")
                except Exception as e:
                    observation = f"Tool ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            else:
                observation = f"ì˜¤ë¥˜: '{action_obj.tool}' Toolì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # 5. ë‹¤ìŒ ë£¨í”„ë¥¼ ìœ„í•œ ê¸°ë¡ ì—…ë°ì´íŠ¸
            intermediate_steps_str += (
                f"\nThought: {thought}\nAction: {action_obj.model_dump_json()}\nObservation: {observation}"
            )

        except Exception as e:
            return f"ReAct ì—”ì§„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    return "ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ì— ë„ë‹¬í•˜ì—¬ ì‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤."


# --- 4. Streamlit UI ë©”ì¸ ë¡œì§ ---
st.title("ğŸ¤– ì§€ëŠ¥í˜• ReAct ë¬¸ì œ í•´ê²°ì‚¬")
st.write("ìˆ˜ëŠ¥ ë¬¸ì œ í’€ì´, ë¸”ë¡œê·¸ ì´ˆì•ˆ ì‘ì„± ë“± ë³µì¡í•œ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”!")

if not all([google_api_key, tavily_api_key, wolfram_app_id]):
    st.info("ì‚¬ì´ë“œë°”ì— ëª¨ë“  API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
else:
    os.environ["TAVILY_API_KEY"] = tavily_api_key
    os.environ["WOLFRAM_ALPHA_APP_ID"] = wolfram_app_id

    # ë„êµ¬ì™€ LLM/í…œí”Œë¦¿ ì´ˆê¸°í™” (ìºì‹œ ì‚¬ìš©)
    tools = get_tools()
    llm, structured_llm, prompt_template = get_llm_and_template(google_api_key, tools)

    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Agentê°€ ìƒê°í•˜ê³  í–‰ë™í•˜ëŠ” ì¤‘..."):
                # ì‚¬ì´ë“œë°” ì´ˆê¸°í™”
                with st.sidebar:
                    st.header("Agentì˜ ìƒê° ê³¼ì • ì—¿ë³´ê¸°")
                    st.empty()

                response = run_structured_react_engine(
                    prompt, st.session_state.chat_history, tools, structured_llm, prompt_template
                )
                st.markdown(response)

                st.session_state.messages.append({"role": "assistant", "content": response})
                st.session_state.chat_history.append((prompt, response))
