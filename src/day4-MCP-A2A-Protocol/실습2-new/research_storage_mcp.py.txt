#!/usr/bin/env python3
"""
ì—°êµ¬ ìë£Œ ì €ì¥ MCP ì„œë²„
FastMCPë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°êµ¬ ìë£Œë¥¼ ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ëŠ” MCP ì„œë²„ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import logging
import os
import shutil
import uuid
from datetime import datetime
from pathlib import Path
from threading import Lock
from typing import Any, Dict, List, Optional

from fastmcp import FastMCP
from pydantic import BaseModel, Field, validator

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('research_mcp.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ë°ì´í„° ëª¨ë¸ ì •ì˜
class ResearchMaterial(BaseModel):
    """ì—°êµ¬ ìë£Œ ëª¨ë¸"""
    id: str = Field(..., description="ì—°êµ¬ ìë£Œ ê³ ìœ  ID")
    title: str = Field(..., description="ì—°êµ¬ ìë£Œ ì œëª©", min_length=1)
    content: str = Field(..., description="ì—°êµ¬ ìë£Œ ë‚´ìš©", min_length=1)
    category: str = Field(..., description="ì—°êµ¬ ìë£Œ ì¹´í…Œê³ ë¦¬", min_length=1)
    tags: List[str] = Field(default_factory=list, description="ì—°êµ¬ ìë£Œ íƒœê·¸")
    source_url: Optional[str] = Field(None, description="ì›ë³¸ ì¶œì²˜ URL")
    created_at: datetime = Field(default_factory=datetime.now, description="ìƒì„± ì¼ì‹œ")
    updated_at: datetime = Field(default_factory=datetime.now, description="ìˆ˜ì • ì¼ì‹œ")
    author: Optional[str] = Field(None, description="ì €ì")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="ì¶”ê°€ ë©”íƒ€ë°ì´í„°")

    @validator('tags')
    def validate_tags(cls, v):
        """íƒœê·¸ ìœ íš¨ì„± ê²€ì‚¬"""
        return [tag.strip() for tag in v if tag.strip()]

    @validator('category')
    def validate_category(cls, v):
        """ì¹´í…Œê³ ë¦¬ ìœ íš¨ì„± ê²€ì‚¬"""
        return v.strip()

    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }

# MCP ì„œë²„ ìƒì„±
research_server = FastMCP(
    name="Research Storage MCP Server",
    instructions="ì—°êµ¬ ìë£Œë¥¼ ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ëŠ” MCP ì„œë²„ì…ë‹ˆë‹¤. ì—°êµ¬ ìë£Œì˜ ì €ì¥, ê²€ìƒ‰, ìˆ˜ì •, ì‚­ì œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤."
)

# ë°ì´í„° ì €ì¥ ê²½ë¡œ
DATA_DIR = Path(__file__).parent / "research_data"
DATA_DIR.mkdir(exist_ok=True)
BACKUP_DIR = DATA_DIR / "backups"
BACKUP_DIR.mkdir(exist_ok=True)
MATERIALS_FILE = DATA_DIR / "materials.json"

# íŒŒì¼ ì ‘ê·¼ ë™ê¸°í™”ë¥¼ ìœ„í•œ ë½
file_lock = Lock()

def create_backup():
    """í˜„ì¬ ë°ì´í„°ì˜ ë°±ì—…ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if MATERIALS_FILE.exists():
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = BACKUP_DIR / f"materials_backup_{timestamp}.json"
        try:
            shutil.copy2(MATERIALS_FILE, backup_file)
            logger.info(f"ë°±ì—… ìƒì„±ë¨: {backup_file}")
            
            # ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì •ë¦¬ (ìµœê·¼ 10ê°œë§Œ ìœ ì§€)
            backup_files = sorted(BACKUP_DIR.glob("materials_backup_*.json"))
            if len(backup_files) > 10:
                for old_backup in backup_files[:-10]:
                    old_backup.unlink()
                    logger.info(f"ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œë¨: {old_backup}")
        except Exception as e:
            logger.error(f"ë°±ì—… ìƒì„± ì‹¤íŒ¨: {e}")

def custom_json_serializer(obj):
    """ì‚¬ìš©ì ì •ì˜ JSON ì§ë ¬í™” í•¨ìˆ˜"""
    if isinstance(obj, datetime):
        return obj.isoformat()
    raise TypeError(f"Object of type {type(obj).__name__} is not JSON serializable")

def load_materials() -> Dict[str, ResearchMaterial]:
    """ì €ì¥ëœ ì—°êµ¬ ìë£Œë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not MATERIALS_FILE.exists():
        logger.info("ìë£Œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¹ˆ ì €ì¥ì†Œë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        return {}

    try:
        with file_lock:
            with open(MATERIALS_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                materials = {}
                for material_id, material_data in data.items():
                    try:
                        # datetime ë¬¸ìì—´ì„ ë‹¤ì‹œ datetime ê°ì²´ë¡œ ë³€í™˜
                        if 'created_at' in material_data and isinstance(material_data['created_at'], str):
                            material_data['created_at'] = datetime.fromisoformat(material_data['created_at'])
                        if 'updated_at' in material_data and isinstance(material_data['updated_at'], str):
                            material_data['updated_at'] = datetime.fromisoformat(material_data['updated_at'])
                        
                        materials[material_id] = ResearchMaterial(**material_data)
                    except Exception as e:
                        logger.error(f"ìë£Œ íŒŒì‹± ì˜¤ë¥˜ (ID: {material_id}): {e}")
                        continue
                
                logger.info(f"ìë£Œ ë¡œë“œ ì™„ë£Œ: {len(materials)}ê°œ")
                return materials
    except Exception as e:
        logger.error(f"ìë£Œ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return {}

def save_materials(materials: Dict[str, ResearchMaterial]):
    """ì—°êµ¬ ìë£Œë“¤ì„ ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        # ë°±ì—… ìƒì„±
        create_backup()
        
        with file_lock:
            data = {
                material_id: material.dict()
                for material_id, material in materials.items()
            }
            
            # ì„ì‹œ íŒŒì¼ì— ë¨¼ì € ì €ì¥
            temp_file = MATERIALS_FILE.with_suffix('.tmp')
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, default=custom_json_serializer)
            
            # ì›ìì  êµì²´
            temp_file.replace(MATERIALS_FILE)
            logger.info(f"ìë£Œ ì €ì¥ ì™„ë£Œ: {len(materials)}ê°œ")
            
    except Exception as e:
        logger.error(f"ìë£Œ ì €ì¥ ì˜¤ë¥˜: {e}")
        raise

# ì „ì—­ ìë£Œ ì €ì¥ì†Œ
try:
    materials_store = load_materials()
except Exception as e:
    logger.error(f"ì´ˆê¸° ìë£Œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    materials_store = {}

@research_server.resource(
    "research://materials",
    description="ì €ì¥ëœ ëª¨ë“  ì—°êµ¬ ìë£Œ ëª©ë¡"
)
async def list_all_materials():
    """ì €ì¥ëœ ëª¨ë“  ì—°êµ¬ ìë£Œì˜ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        materials_list = []
        for material in materials_store.values():
            materials_list.append({
                "id": material.id,
                "title": material.title,
                "category": material.category,
                "tags": material.tags,
                "created_at": material.created_at.isoformat(),
                "author": material.author,
                "content_preview": material.content[:100] + "..." if len(material.content) > 100 else material.content
            })

        return {
            "total_count": len(materials_list),
            "materials": sorted(materials_list, key=lambda x: x["created_at"], reverse=True)
        }
    except Exception as e:
        logger.error(f"ìë£Œ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return {"error": "ìë£Œ ëª©ë¡ì„ ì¡°íšŒí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

@research_server.resource(
    "research://materials/{material_id}",
    description="íŠ¹ì • ì—°êµ¬ ìë£Œì˜ ìƒì„¸ ì •ë³´"
)
async def get_material(material_id: str):
    """íŠ¹ì • ì—°êµ¬ ìë£Œì˜ ìƒì„¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        if material_id not in materials_store:
            return {"error": f"ì—°êµ¬ ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {material_id}"}

        material = materials_store[material_id]
        return {
            "material": material.dict(),
            "word_count": len(material.content.split()),
            "character_count": len(material.content),
            "line_count": len(material.content.split('\n'))
        }
    except Exception as e:
        logger.error(f"ìë£Œ ì¡°íšŒ ì˜¤ë¥˜ (ID: {material_id}): {e}")
        return {"error": "ìë£Œë¥¼ ì¡°íšŒí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

@research_server.resource(
    "research://materials/category/{category}",
    description="íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ì—°êµ¬ ìë£Œ ëª©ë¡"
)
async def get_materials_by_category(category: str):
    """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ì—°êµ¬ ìë£Œë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        category_materials = [
            material for material in materials_store.values()
            if material.category.lower() == category.lower()
        ]

        return {
            "category": category,
            "count": len(category_materials),
            "materials": [
                {
                    "id": material.id,
                    "title": material.title,
                    "tags": material.tags,
                    "created_at": material.created_at.isoformat(),
                    "author": material.author,
                    "content_preview": material.content[:100] + "..." if len(material.content) > 100 else material.content
                }
                for material in sorted(category_materials, key=lambda x: x.created_at, reverse=True)
            ]
        }
    except Exception as e:
        logger.error(f"ì¹´í…Œê³ ë¦¬ë³„ ìë£Œ ì¡°íšŒ ì˜¤ë¥˜ (ì¹´í…Œê³ ë¦¬: {category}): {e}")
        return {"error": "ì¹´í…Œê³ ë¦¬ë³„ ìë£Œë¥¼ ì¡°íšŒí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

@research_server.resource(
    "research://materials/tag/{tag}",
    description="íŠ¹ì • íƒœê·¸ê°€ í¬í•¨ëœ ì—°êµ¬ ìë£Œ ëª©ë¡"
)
async def get_materials_by_tag(tag: str):
    """íŠ¹ì • íƒœê·¸ê°€ í¬í•¨ëœ ì—°êµ¬ ìë£Œë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        tag_materials = [
            material for material in materials_store.values()
            if tag.lower() in [t.lower() for t in material.tags]
        ]

        return {
            "tag": tag,
            "count": len(tag_materials),
            "materials": [
                {
                    "id": material.id,
                    "title": material.title,
                    "category": material.category,
                    "created_at": material.created_at.isoformat(),
                    "author": material.author,
                    "content_preview": material.content[:100] + "..." if len(material.content) > 100 else material.content
                }
                for material in sorted(tag_materials, key=lambda x: x.created_at, reverse=True)
            ]
        }
    except Exception as e:
        logger.error(f"íƒœê·¸ë³„ ìë£Œ ì¡°íšŒ ì˜¤ë¥˜ (íƒœê·¸: {tag}): {e}")
        return {"error": "íƒœê·¸ë³„ ìë£Œë¥¼ ì¡°íšŒí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

@research_server.resource(
    "research://materials/search/{query}",
    description="í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼"
)
async def search_materials(query: str):
    """ì œëª©, ë‚´ìš©, íƒœê·¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        if not query.strip():
            return {"error": "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."}

        query_lower = query.lower().strip()
        search_results = []

        for material in materials_store.values():
            relevance_score = 0

            # ì œëª© ê²€ìƒ‰ (ê°€ì¤‘ì¹˜ ë†’ìŒ)
            if query_lower in material.title.lower():
                relevance_score += 10

            # ë‚´ìš© ê²€ìƒ‰
            if query_lower in material.content.lower():
                relevance_score += 5

            # íƒœê·¸ ê²€ìƒ‰ (ê°€ì¤‘ì¹˜ ë†’ìŒ)
            if any(query_lower in tag.lower() for tag in material.tags):
                relevance_score += 8

            # ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰
            if query_lower in material.category.lower():
                relevance_score += 6

            # ì €ì ê²€ìƒ‰
            if material.author and query_lower in material.author.lower():
                relevance_score += 7

            if relevance_score > 0:
                search_results.append({
                    "material": {
                        "id": material.id,
                        "title": material.title,
                        "category": material.category,
                        "tags": material.tags,
                        "created_at": material.created_at.isoformat(),
                        "author": material.author
                    },
                    "relevance_score": relevance_score,
                    "preview": material.content[:200] + "..." if len(material.content) > 200 else material.content
                })

        # ê´€ë ¨ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        search_results.sort(key=lambda x: x["relevance_score"], reverse=True)

        return {
            "query": query,
            "total_results": len(search_results),
            "results": search_results
        }
    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ì˜¤ë¥˜ (ì¿¼ë¦¬: {query}): {e}")
        return {"error": "ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

# ë„êµ¬(Tools) ì •ì˜
@research_server.tool(
    name="create_research_material",
    description="ìƒˆë¡œìš´ ì—°êµ¬ ìë£Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
)
async def create_research_material(
    title: str,
    content: str,
    category: str,
    tags: List[str] = None,
    source_url: str = None,
    author: str = None
) -> str:
    """ìƒˆë¡œìš´ ì—°êµ¬ ìë£Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        if not title.strip():
            return "ì˜¤ë¥˜: ì œëª©ì€ í•„ìˆ˜ ì…ë ¥ í•­ëª©ì…ë‹ˆë‹¤."
        
        if not content.strip():
            return "ì˜¤ë¥˜: ë‚´ìš©ì€ í•„ìˆ˜ ì…ë ¥ í•­ëª©ì…ë‹ˆë‹¤."
        
        if not category.strip():
            return "ì˜¤ë¥˜: ì¹´í…Œê³ ë¦¬ëŠ” í•„ìˆ˜ ì…ë ¥ í•­ëª©ì…ë‹ˆë‹¤."

        if tags is None:
            tags = []

        material_id = str(uuid.uuid4())
        material = ResearchMaterial(
            id=material_id,
            title=title.strip(),
            content=content.strip(),
            category=category.strip(),
            tags=tags,
            source_url=source_url.strip() if source_url else None,
            author=author.strip() if author else None
        )

        materials_store[material_id] = material
        save_materials(materials_store)
        logger.info(f"ìƒˆ ìë£Œ ìƒì„±ë¨: {material_id} - {title}")

        return f"ì—°êµ¬ ìë£Œê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ID: {material_id}, ì œëª©: {title}"
    except Exception as e:
        logger.error(f"ìë£Œ ìƒì„± ì˜¤ë¥˜: {e}")
        return f"ìë£Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

@research_server.tool(
    name="update_research_material",
    description="ê¸°ì¡´ ì—°êµ¬ ìë£Œë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤."
)
async def update_research_material(
    material_id: str,
    title: str = None,
    content: str = None,
    category: str = None,
    tags: List[str] = None,
    source_url: str = None,
    author: str = None
) -> str:
    """ê¸°ì¡´ ì—°êµ¬ ìë£Œë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤."""
    try:
        if material_id not in materials_store:
            return f"ì˜¤ë¥˜: ì—°êµ¬ ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ID: {material_id}"

        material = materials_store[material_id]
        original_title = material.title

        # ì œê³µëœ ê°’ë“¤ë§Œ ì—…ë°ì´íŠ¸
        if title is not None and title.strip():
            material.title = title.strip()
        if content is not None and content.strip():
            material.content = content.strip()
        if category is not None and category.strip():
            material.category = category.strip()
        if tags is not None:
            material.tags = [tag.strip() for tag in tags if tag.strip()]
        if source_url is not None:
            material.source_url = source_url.strip() if source_url.strip() else None
        if author is not None:
            material.author = author.strip() if author.strip() else None

        material.updated_at = datetime.now()

        save_materials(materials_store)
        logger.info(f"ìë£Œ ìˆ˜ì •ë¨: {material_id} - {original_title}")

        return f"ì—°êµ¬ ìë£Œê°€ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ID: {material_id}, ì œëª©: {material.title}"
    except Exception as e:
        logger.error(f"ìë£Œ ìˆ˜ì • ì˜¤ë¥˜ (ID: {material_id}): {e}")
        return f"ìë£Œ ìˆ˜ì • ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

@research_server.tool(
    name="delete_research_material",
    description="ì—°êµ¬ ìë£Œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."
)
async def delete_research_material(material_id: str) -> str:
    """ì—°êµ¬ ìë£Œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
    try:
        if material_id not in materials_store:
            return f"ì˜¤ë¥˜: ì—°êµ¬ ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ID: {material_id}"

        deleted_material = materials_store.pop(material_id)
        save_materials(materials_store)
        logger.info(f"ìë£Œ ì‚­ì œë¨: {material_id} - {deleted_material.title}")

        return f"ì—°êµ¬ ìë£Œê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. ì œëª©: {deleted_material.title}"
    except Exception as e:
        logger.error(f"ìë£Œ ì‚­ì œ ì˜¤ë¥˜ (ID: {material_id}): {e}")
        return f"ìë£Œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

@research_server.tool(
    name="get_research_statistics",
    description="ì—°êµ¬ ìë£Œì— ëŒ€í•œ í†µê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
)
async def get_research_statistics() -> Dict[str, Any]:
    """ì—°êµ¬ ìë£Œì— ëŒ€í•œ í†µê³„ë¥¼ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        if not materials_store:
            return {"message": "ì €ì¥ëœ ì—°êµ¬ ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤."}

        total_materials = len(materials_store)
        categories = {}
        tags = {}
        authors = {}
        total_words = 0
        total_characters = 0

        for material in materials_store.values():
            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
            categories[material.category] = categories.get(material.category, 0) + 1

            # íƒœê·¸ë³„ í†µê³„
            for tag in material.tags:
                tags[tag] = tags.get(tag, 0) + 1

            # ì €ìë³„ í†µê³„
            if material.author:
                authors[material.author] = authors.get(material.author, 0) + 1

            # ì´ ë‹¨ì–´ ìˆ˜ì™€ ë¬¸ì ìˆ˜
            words = len(material.content.split())
            total_words += words
            total_characters += len(material.content)

        return {
            "total_materials": total_materials,
            "total_words": total_words,
            "total_characters": total_characters,
            "categories": categories,
            "top_tags": sorted(tags.items(), key=lambda x: x[1], reverse=True)[:10],
            "authors": authors,
            "average_words_per_material": total_words / total_materials if total_materials > 0 else 0,
            "average_characters_per_material": total_characters / total_materials if total_materials > 0 else 0
        }
    except Exception as e:
        logger.error(f"í†µê³„ ê³„ì‚° ì˜¤ë¥˜: {e}")
        return {"error": "í†µê³„ë¥¼ ê³„ì‚°í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

@research_server.tool(
    name="export_research_materials",
    description="ì—°êµ¬ ìë£Œë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤."
)
async def export_research_materials(category: str = None, format: str = "json") -> str:
    """ì—°êµ¬ ìë£Œë¥¼ ì§€ì •ëœ í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤."""
    try:
        if format.lower() != "json":
            return "ì˜¤ë¥˜: í˜„ì¬ JSON í˜•ì‹ë§Œ ì§€ì›í•©ë‹ˆë‹¤."

        export_data = {}

        if category:
            # íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ ë‚´ë³´ë‚´ê¸°
            export_data = {
                material_id: material.dict()
                for material_id, material in materials_store.items()
                if material.category.lower() == category.lower()
            }
        else:
            # ëª¨ë“  ìë£Œ ë‚´ë³´ë‚´ê¸°
            export_data = {
                material_id: material.dict()
                for material_id, material in materials_store.items()
            }

        if not export_data:
            return f"ë‚´ë³´ë‚¼ ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤. ì¹´í…Œê³ ë¦¬: {category or 'ì „ì²´'}"

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"research_export_{timestamp}.json"

        export_path = DATA_DIR / filename
        with open(export_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2, default=custom_json_serializer)

        logger.info(f"ìë£Œ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {filename}, {len(export_data)}ê°œ")
        return f"ì—°êµ¬ ìë£Œê°€ ì„±ê³µì ìœ¼ë¡œ ë‚´ë³´ë‚´ì¡ŒìŠµë‹ˆë‹¤. íŒŒì¼: {filename}, ìë£Œ ìˆ˜: {len(export_data)}"
    except Exception as e:
        logger.error(f"ìë£Œ ë‚´ë³´ë‚´ê¸° ì˜¤ë¥˜: {e}")
        return f"ìë£Œ ë‚´ë³´ë‚´ê¸° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

@research_server.tool(
    name="import_research_materials",
    description="JSON íŒŒì¼ì—ì„œ ì—°êµ¬ ìë£Œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."
)
async def import_research_materials(filename: str, merge_mode: str = "skip") -> str:
    """JSON íŒŒì¼ì—ì„œ ì—°êµ¬ ìë£Œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        filename: ê°€ì ¸ì˜¬ JSON íŒŒì¼ëª…
        merge_mode: 'skip' (ì¤‘ë³µ ì‹œ ê±´ë„ˆë›°ê¸°), 'overwrite' (ì¤‘ë³µ ì‹œ ë®ì–´ì“°ê¸°), 'new_id' (ìƒˆ IDë¡œ ìƒì„±)
    """
    try:
        import_path = DATA_DIR / filename
        if not import_path.exists():
            return f"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"

        with open(import_path, 'r', encoding='utf-8') as f:
            import_data = json.load(f)

        imported_count = 0
        skipped_count = 0
        error_count = 0

        for material_id, material_data in import_data.items():
            try:
                # datetime ë¬¸ìì—´ì„ ë‹¤ì‹œ datetime ê°ì²´ë¡œ ë³€í™˜
                if 'created_at' in material_data and isinstance(material_data['created_at'], str):
                    material_data['created_at'] = datetime.fromisoformat(material_data['created_at'])
                if 'updated_at' in material_data and isinstance(material_data['updated_at'], str):
                    material_data['updated_at'] = datetime.fromisoformat(material_data['updated_at'])

                material = ResearchMaterial(**material_data)

                if material_id in materials_store:
                    if merge_mode == "skip":
                        skipped_count += 1
                        continue
                    elif merge_mode == "overwrite":
                        materials_store[material_id] = material
                        imported_count += 1
                    elif merge_mode == "new_id":
                        new_id = str(uuid.uuid4())
                        materials_store[new_id] = material
                        imported_count += 1
                else:
                    materials_store[material_id] = material
                    imported_count += 1

            except Exception as e:
                logger.error(f"ìë£Œ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜ (ID: {material_id}): {e}")
                error_count += 1
                continue

        if imported_count > 0:
            save_materials(materials_store)

        logger.info(f"ìë£Œ ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ: {imported_count}ê°œ ê°€ì ¸ì˜´, {skipped_count}ê°œ ê±´ë„ˆëœ€, {error_count}ê°œ ì˜¤ë¥˜")
        return f"ìë£Œ ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ. ê°€ì ¸ì˜¨ ìë£Œ: {imported_count}ê°œ, ê±´ë„ˆë›´ ìë£Œ: {skipped_count}ê°œ, ì˜¤ë¥˜: {error_count}ê°œ"

    except Exception as e:
        logger.error(f"ìë£Œ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
        return f"ìë£Œ ê°€ì ¸ì˜¤ê¸° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

if __name__ == "__main__":
    # ì„œë²„ ì‹œì‘ ì‹œ ë°ì´í„° ë¡œë“œ í™•ì¸
    print(f"ğŸ“š ì—°êµ¬ ìë£Œ ì €ì¥ MCP ì„œë²„ ì‹œì‘")
    print(f"ğŸ“ ë°ì´í„° ì €ì¥ ê²½ë¡œ: {DATA_DIR}")
    print(f"ğŸ“ ë°±ì—… ì €ì¥ ê²½ë¡œ: {BACKUP_DIR}")
    print(f"ğŸ“Š í˜„ì¬ ì €ì¥ëœ ìë£Œ ìˆ˜: {len(materials_store)}")
    print(f"ğŸ“ ë¡œê·¸ íŒŒì¼: research_mcp.log")
    
    # ì„œë²„ ì‹¤í–‰
    research_server.run()